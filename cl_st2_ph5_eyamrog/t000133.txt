Title: Measuring Human Water Needs


Abstract: Abstract

Water connects the environment, culture, and biology, yet only recently has it emerged as a major focus for research in human biology. To facilitate such research, we describe methods to measure biological, environmental, and perceptual indicators of human water needs. This toolkit provides an overview of methods for assessing different dimensions of human water need, both well-established and newly-developed. These include: (a) markers of hydration (eg, urine specific gravity, doubly labeled water) important for measuring the impacts of water need on human biological functioning; (b) methods for measuring water quality (eg, digital colorimeter, membrane filtration) essential for understanding the health risks associated with exposure to microbiological, organic, metal, inorganic nonmental, and other contaminants; and (c) assessments of household water insecurity status that track aspects of unmet water needs (eg, inadequate water service, unaffordability, and experiences of water insecurity) that are directly relevant to human health and biology. Together, these methods can advance new research about the role of water in human biology and health, including the ways that insufficient, unsafe, or insecure water produces negative biological and health outcomes.

Section: 1 WHY ASSESS WATER NEEDS?

Globally, some 844 million people do not have any basic access to drinking water services and another two billion people are drinking water likely contaminated with feces (World Health Organization (WHO), 2018 ). Within 5 years, half of the world's population is expected to live in a water-stressed region (WHO, 2018 ). That said, the lack of clear, consistent, and universally-accepted measures of water needs makes it difficult to clearly characterize the scale of the problem (Jepson, Wutich, Colllins, Boateng, & Young, 2017 ). The field of water insecurity research, which is quite new, is advancing a range of emerging methods to better assess and quantify human water needs (Wutich et al., 2017 ).
The field of human biology has only recently engaged directly with fully-integrated research questions that address human water needs from environmental and biological perspectives (Houck, 2017 ; Rosinger, 2015a ; Rosinger, 2015b ; Rosinger, 2018 ; Rosinger & Tanner, 2015 ). In this article, we focus on identifying and explaining methods and measures most relevant to advancing scholarship in human biology and health. Specifically, we describe methods to measure biological, environmental, and perceptual indicators of human water needs (including the need for acceptable water quality). We conclude with some comments on how further advancement of these methods could facilitate future human biological research on human water needs.
Water security, broadly defined, is “the ability to access and benefit from affordable, adequate, reliable, and safe water for wellbeing and a healthy life” (Jepson, Wutich, et al., 2017 ). When people lack adequate water flows, water quality, and water services, water insecurity undermines health and wellbeing through several basic pathways. Severe dehydration can produce serious health consequences, including delirium, renal failure, seizures, and death (Popkin, D'Anci, & Rosenberg, 2010 ; Thomas et al., 2008 ). Ingestion of contaminated water produces a range of viral (eg, hepatitis A), bacterial (eg, cholera), and protozoal (eg, amoebiasis) diseases; 2.4 million deaths annually are believed to be preventable with improved water provision, sanitation, and hygiene (Bartram & Cairncross, 2010 ). The act of carrying water can result in pain and disability related to excess loads, falls, assaults, and other traumas (Geere et al., 2018 ; Sorensen, Morssink, & Campos, 2011 ). Water insecurity also has indirect effects on growth and development. Examples of such pathways include exposure to infectious diarrheas or other diseases either through consumption of unsafe water or insufficient sanitation, limits on child feeding options that require preparation with water, and switching to sugar-sweetened beverages when water is unsafe (Bartram & Cairncross, 2010 ; Javidi & Pierce, 2018 ).
Coping with water insecurity, whether acute or chronic, is also highly stressful in ways relevant to human biological impacts (Wutich, 2020 , in this issue). That stress can be related to uncertainty as to when water will become available, stigma or shame due to unmet social expectations (eg, for hygiene), or perceived injustices and inequities in how safe water is distributed (Wutich & Brewis, 2014 ). Water-related distress appears to worsen common mental health disorders, such as anxiety and depression (Aihara, Shrestha, & Sharma, 2016 ; Cooper-Vince et al., 2018 ; Stevenson et al., 2012 ; Subbaraman et al., 2015 ; Workman & Ureksoy, 2017 ; Wutich, Brewis, Chavez, & Jaiswal, 2016 ). Low water access has also recently been linked to physical manifestations of stress as well, such as heightened blood pressure (Brewis, Choudhary, & Wutich, 2019a , 2019b ). Women, who shoulder the responsibilities of household water management in most societies, appear most vulnerable to the consequences of water insecurity; but under conditions of extreme water insecurity, when the household is in crisis-mode, men can exhibit strong distress responses to household water conditions as well (Wutich, 2009b ).
An important note is that even when people have enough water quantity and quality to meet basic physical needs, this water may still be insufficient for adhering to culturally-acceptable livelihoods or meeting social and spiritual obligations (Hadley & Wutich, 2009 ). This reality is encapsulated by a broader conceptualization of water insecurity that includes nonphysical dimensions of insufficiency (Jepson et al., 2017 ; Jepson, Wutich, et al., 2017 ; Wutich et al., 2017 ).

Section: 2 ASSESSING INDIVIDUAL-LEVEL HYDRATION AND WATER NEEDS

Hydration status is the internal state of water balance within an organism. Hydration is an important factor for human biologists to consider in research because it is tightly linked with cognitive and physical performance (Popkin et al., 2010 ), as well as having important implications for metabolic and cardiovascular health (Watso & Farquhar, 2019 ). Despite ample research in kinesiology, nutrition, and physiology, a gold standard measure for hydration status has been difficult to find (Armstrong, 2007 ). The most basic way to assess hydration status in a static state is by measuring weight changes. The general marker of 2% loss in body weight over a short time period (during which no food and water have been consumed and thus changes are due to water loss) is used as a cutoff for acute dehydration (Shirreffs, 2003 ). Several other reviews have gone in depth on biomarkers of hydration for diagnostic purposes, under conditions of experimental dehydration (Cheuvront, Ely, Kenefick, & Sawka, 2010 ; Cheuvront, Kenefick, Charkoudian, & Sawka, 2013 ; Oppliger, Magnes, Popowski, & Gisolfi, 2005 ), as well as in normal daily conditions (Armstrong et al., 2010 ; Armstrong et al., 2012 ). Biological and behavioral, or bio-behavioral, responses to different environments likely influence human water needs and variation in hydration biomarkers (Rosinger, 2020 in this issue). Table 1 summarizes the various hydration biomarkers used and covered in this section.
Urinary biomarkers of hydration status change throughout the day as they reflect water intake and outputs. When human water intake is experimentally manipulated either through extra water provision or water restriction, urinary biomarkers reflect those changes either through more dilute or more concentrated urine (Enhörning et al., 2019 ; Perrier et al., 2013 ). These biomarkers stabilize within 24 hours to reflect the current water intake state. The mechanism behind changes in urine concentration and urine volume are due to changes in vasopressin, or anti-diuretic hormone vasopressin, secretion which is released in response to body water excesses or deficits (Armstrong & Johnson, 2018 ). Biomarkers of urine concentration respond acutely to changes in water intake, unlike plasma osmolality, which is less responsive to acute changes in water intake (Cheuvront et al., 2010 ; Perrier et al., 2013 ).
While measuring hydration biomarkers in 24-hour urine samples provides the most accurate measure of daily hydration status, this method is often hard to implement in field settings where only spot sampling is possible. First morning samples are often used as well as early afternoon samples, which according to Perrier et al. ( 2013 ) most accurately reflect 24-hour sample averages. Another method is to statistically adjust for time of day and run sensitivity analyses between the different times of day to make sure that the relationship being tested is consistent across the day (Rosinger et al., 2019 ; Rosinger, Lawman, Akinbami, & Ogden, 2016 ).
There are four primary urinary biomarkers of hydration status. First, urine osmolality (Uosm) is the most reliable urinary biomarker of daily hydration status. It is the total concentration of dissolved particles per kg of water in urine. Similar to plasma osmolality, Uosm is measured on an osmometer. Values above 800 mOsm/kg are used as the most common cutoff, indicating inadequate hydration (Cheuvront et al., 2010 ) although different cutoffs have been proposed (500, 700, and 831 mOsm/kg) (Armstrong et al., 2012 ; Perrier et al., 2015 ; Stookey, 2019 ). Such cutoffs may be dependent on age if the study population includes a wide age range of adults, since age is negatively associated with urine osmolality at an average decline of 3.1 mOsm/kg per year above 20 (Rosinger et al., 2016 ).
An osmometer can range between $4000 and $20000. (eg, the Osmo1 Osmometer is $14000). The analysis takes on average 3 minutes per sample including time to clean the chamber in between samples. The other advantage of osmometers is that they are able to analyze multiple specimen types for hydration markers (ie, plasma/serum, urine, saliva). However, osmometers require an electrical outlet for power and need a stable, room temperature environment, and flat surface to be placed upon.
Second, urine specific gravity (Usg) provides the density of urine relative to water with a value of 1.000 (g/ml) equivalent to water. Usg ranges from 1.000 to 1.040 g/mL, with values above 1.020 g/mL used as a well-established cutoff for inadequate hydration (Armstrong, Johnson, et al., 2012 ; Popowski et al., 2001 ). Values of 1.002 g/mL and below may reflect inability to concentrate urine due to kidney problems, excessive water intake, or diabetes insipidus (Cook, Caplan, LoDico, & Bush, 2000 ).
This measure is the best field-friendly, noninvasive hydration biomarker because Usg is much easier to measure in the field (Armstrong, 2007 ). It has been shown to work well in human biology research in remote settings (Rosinger, 2015a , 2015b , 2018 ). This makes Usg particularly useful for field-based research on the impact of unmet human water needs on human health and biology.
One handheld battery-powered refractometer costs on average ~$450 from Atago, which is significantly cheaper than osmometers. The urine samples are measured in the field immediately after collection from participants. The researcher simply places the refractometer directly in the urine sample after collection and the refractometer provides the digital reading after a couple seconds of analysis.
While urine osmolality is the most precise urinary hydration marker, it is highly correlated with Usg. For example, using data from the Water, Health and Nutrition Lab at Pennsylvania State University, we compared paired samples analyzed for Uosm and Usg and found a correlation of R = .977, P < .0001 (Figure 1 ). Moreover, if a researcher wants to transport urine samples on ice to measure them for additional biomarkers back in a lab, Usg is highly stable. We measured Usg 30 minutes after sample collection, froze samples within 2 hours of sample collection for up to 6 months, then thawed and retested Usg. Results for 121 paired samples were nearly identical (Pearson's R = .985, P < .0001, data not shown).
Urine specific gravity can also be measured on reagent strips. While this method of measuring Usg is highly correlated to Usg using refractometers, it is more likely to yield false negatives when applying a euhydration cutoff (Usg < 1.020 g/mL). Therefore, refractometry is recommended (Abbey, Heelan, Brown, & Bartee, 2014 ).
The digital refractometers have temperature compensation but the calibration water used should be similar to the sample temperature. To address this, it is recommended to cover urine samples and allow them to cool slightly to room temperature. Otherwise, the Usg values may be slightly underestimated—that is, the warmer the urine samples are, the lower the Usg values will be if the calibrated sample was at room temperature.
Third, urine color (Ucol) has been used as a biomarker of hydration status, often coupled with other biomarkers. Urine samples are compared to a urine color strip with scores of 1 to 8. Higher numbers represent more concentrated urine. The general cutoff for inadequate hydration is 5 (Armstrong et al., 2010 ), and this biomarker has been used and validated among pregnant and lactating women (McKenzie et al., 2017 ).
Urine color is a more subjective rating, as the researcher has to hold the urine sample up to the urine color chart. However, is fairly well correlated with Usg and Uosm. Urine color is the lowest cost (and least technologically complicated) method, as the urine color chart costs just ~$4 for a small size. In the absence of a refractometer, urine color can provide an indicator of hydration status.
Finally, 24-hour urine volume can also be assessed as a marker of hydration with higher urine production signifying a more hydrated state (Armstrong et al., 2010 ; Armstrong, Johnson, et al., 2012 ). This measure is more laborious in field settings. It requires researchers to transport large urine collection containers (3-4 L) to the fieldsite, which cost ~$190 to 250 for 40 of these containers. Additionally, it is more challenging for compliance since it requires participants to capture all of their urine for a 24-hour period, with all the associated difficulties, including refrigerating the urine specimen over the 24-hour time period.
This urine volume is then measured in a graduated cylinder in milliliters. In hot environments, urine production can often be low because more body water is lost through respiration and sweating. In such settings, urine volume will not be a good indicator of water intake, but can provide relative hydration levels.
Two primary blood biomarkers can provide an acute look into a person's hydration status and their body water homeostasis.
First, plasma or serum osmolality (Sosm) is viewed by some as the best marker of bodily hydration status, which is buffered against acute changes in water intake (Cheuvront & Sawka, 2005 ). Serum osmolality is not as influenced by recent food and water intake or physical exertion as Uosm. It can therefore serve as a more stable indicator of longer-term hydration status.
Osmolality is determined by freezing point depression osmometry, calculated as milliosmoles per kilogram (mOsm/kg) on an osmometer. The normal range of a euhydrated or well-hydrated state is serum osmolality between 285 and 295 mOsm/kg. Values of 295 mOsm/kg indicate impending dehydration. Values above 300 mOsm/kg are used as a cutoff for dehydration. Values below 285 mOsm/kg may reflect overhydration or hyponatremia, which can be similarly dangerous and should be examined for serum sodium levels to further examine electrolyte imbalances (Cheuvront et al., 2010 ). However, the correlation between Sosm and urinary biomarkers of hydration are not very strong ( r = ~.2). Without an osmometer in the lab, generally the cost is ~$25 per sample.
In terms of measurement, the size of the plasma sample used in osmometers varies. Normally there are two standard sizes: 20 or 250 μL. The smaller specimen size of 20 μL may artificially elevate the Posm value by 7 mOsm/kg (Sollanek, Kenefick, & Cheuvront, 2019 ). This is likely due to plasma proteins in the blood, which means that this is not an issue for urine osmolality.
Second, vasopressin is the hormone most responsible for water conservation. Vasopressin can be measured through plasma as well as serum samples. Concentrations below 2.0 pg/mL represent a baseline hydrated state (ie, the brain is not attempting to conserve water), while concentrations above 2.0 indicate dehydration because the brain is acting to conserve water.
Vasopressin is measured through an ELISA kit (which can range from $580 to 795 for a single 96 well plate). Like other ELISA kits, it necessitates a full wet lab with microplate readers, an incubator, microtiter plate, graduated cylinders, distilled water, pipettes, and test tubes. While vasopressin is a newer biomarker of hydration, it is well correlated with urinary biomarkers of hydration and parlays into the neural circuitry designed to keep body water homeostasis (Armstrong & Johnson, 2018 ). However, vasopressin can be quite unstable in lab analyses, attaches to platelets and is cleared rapidly (Morgenthaler, Struck, Alonso, & Bergmann, 2006 ).
To address this, many researchers are measuring the peptide copeptin, which is a precursor to vasopressin and is highly correlated ( r = .78) with vasopressin (Morgenthaler, Struck, Jochberger, & Dünser, 2008 ). Copeptin is seen as a vasopressin surrogate. It can also be measured using an ELISA kit (which can range from $400 to $900 for a single 96 well plate) in serum and plasma samples as small as 50 μL. The analyte is stable for up to 7 days at room temperature, with the most stability seen in EDTA plasma samples (Morgenthaler et al., 2006 ).
Full methods on measuring vasopressin and copeptin can be found in Morgenthaler et al. ( 2006 ). In sum, copeptin is a more useful and practical biomarker to measure vasopressin for human biology research than directly measuring vasopressin. Nonetheless, copeptin and vasopressin are more reflective of changes in water intake than plasma osmolality (Enhörning et al., 2019 ).
Blood biomarker methods, when compared with urine biomarkers, are not easier to implement in clinical or lab settings. Further, they are not field-friendly because they require venous blood draws and freezers. Such methods and equipment needs can be challenging in remote fieldsites.
Two primary methods exist to estimate total water intake. Both are time intensive, but they differ markedly in cost and time.
Doubly labeled water (DLW) is the gold standard for calculating total body water usage in a 24-hour period (liters/day). DLW can be calculated with deuterium from stable isotope analysis. This method is relatively field-friendly and has been conducted in small-scale populations in remote areas (Christopher et al., 2019 ; Pontzer et al., 2015 ).
DLW requires isotopic analyses for the determination of both energy expenditure and water throughput. This is because both rely on the rate of isotope depletion in the body as the enriched water is flushed from the body (Raman et al., 2004 ; Schoeller et al., 1986 ). First, the subject provides a urine sample. Once they return with the urine sample, they drink a dose (120 mL, or 4 oz) of DLW enriched with safe, nonradioactive oxygen and hydrogen isotopes (deuterium and oxygen-18; IAEA, 2009 ). The dose amount should be tailored to the subject's body weight following IAEA ( 2009 ) recommendations of 1.08 g of DLW per kg of body mass. Then, four urine samples are collected over 14 days. While the isotopic composition in the urine samples are stable, they should not be subjected to extreme heat. It is recommended they be refrigerated or frozen after collection as they are transported to the mass spectrometer for analysis. Finally, the urine samples are then analyzed on a mass spectrometer.
Tracking the enrichment of these uncommon isotopes in the urine samples allows the calculation of the subject's average rate of carbon dioxide production, and thus a measurement of water used by the subject's body (liters/day) during the measurement period. The cost per participant for the isotope dosing (which is dependent on weight of the participant) along with lab fees to run the mass spectrometer typically run ~$800 per participant. While DLW is the gold standard, the high cost and the time commitment of participants is often restrictive of large sample collection.
While DLW allows for an assessment of water throughput, it does not distinguish what people are actually consuming to meet their water needs. As many studies in human biology show, diet is intimately connected to environmental resources, market resources, and globalization (Dufour, Bender, & Reina, 2015 ; Piperata et al., 2011 ; Sorensen et al., 2005 ). Dietary recall analyses reveal many important facets of inquiry to human biology research, including insight into population level snapshots and trends of water intake, dietary exposures and adaptations, and health outcomes (Leonard, 2012 ; Popkin et al., 2010 ).
Researchers can use 24-hour multiple-pass dietary recalls to estimate total water intake from all food and water/fluid sources consumed. While there are known limitations of dietary recalls, including differential bias dependent on weight status of adults (Johansson, Wikman, Åhrén, Hallmans, & Johansson, 2001 ), the multiple pass method reduces recall bias (Conway, Ingwersen, & Moshfegh, 2004 ). In the 24-hour dietary recall method, participants are guided through all periods of the previous day from midnight-to-midnight by trained interviewers to list all foods and beverages consumed, aided with plates, cups, and bowls to assist with size estimation. The interviewer then goes back over the day to allow for any missed dietary consumption.
Analyzing dietary recalls for total water intake requires the time-consuming task of converting all items reported in the dietary recall to grams or milliliters of water based on food composition tables and databases. For dietary data collected in diverse settings, challenges can emerge when local foods are not included in national food composition tables. In such cases, analogous foods must be used as replacements. This raises an issue of potential bias if local varietals of tubers, for example, are more water-rich than a varietal from the US that can be found in the USDA's nutrient database. Despite these potential biases, the conversion from diet intake to food moisture allows for an examination of hydration strategies (Rosinger & Tanner, 2015 ).
Hydration strategies provide a way to examine how much water is coming from different dietary components, like plain water, alcohol, sugar-sweetened beverages, foods, and so forth (Rosinger & Herrick, 2016 ). Additionally, conducting multiple 24-hour dietary recalls over different seasons enables an examination of how total water intake shifts over the year as the weather changes and dietary availability of some items change (Tani et al., 2015 ). In terms of cost, the time it takes to conduct the interviews and convert dietary data into water is the greatest limitation to this method.

Section: 3 ASSESSING WATER QUALITY

Water quality refers to a suite of physical, chemical, microbiological, and other characteristics that are generally referenced against some national standard for assuring human and/or ecosystem health. A given characteristic may be assessed by directly measuring the presence or concentration of a property, substance, or organism, or indirectly via a proxy indicator (WHO, 2017a ). Indirect measurement techniques, such as detecting fecal indicator bacteria, are generally faster or less expensive. Such approaches often serve as a proxy for the effectiveness of a particular component of water treatment and the likely presence of related substances or organisms. Water quality may also describe indicators for industrial and other uses, but here we focus on the implications of quality for human health. The implications of water quality are broadly relevant to human health and biology, from mitigating household waterborne diseases (Gundry, Wright, & Conroy, 2004 ) to assessing global achievement of the Millennium Development Goals water target (Onda, LoBuglio, & Bartram, 2012 ).
Table 2 presents a simple classification of common water quality characteristics that may influence human health directly, or indirectly by affecting the acceptability of drinking water. Each of these has a gold standard laboratory procedure that is detailed in the Standard methods for the examination of water and wastewater , 23rd ed. (American Public Health Association & American Water Works Association, 2017 ). Reproducing these procedures is well beyond the scope of this tool kit, and modern water systems now integrate a broad set of water screening technologies that include sensors and biosensors, whole-organism assays and bioassays, and biological early warning systems (Allan et al., 2006 ). Instead we highlight the options for quantifying turbidity, free chlorine, and coliforms and Escherichia coli , which are the most common indicators used in low-resource, water-insecure communities. We also highlight recent trends in the measurement of newer emerging contaminants.
The most commonly measured of the physical properties with human biological ramifications is turbidity, which refers to the cloudiness of a water sample due to suspended particles. Additional properties, such as color, odor, taste, pH, hardness, and total dissolved solids, do not have established guidelines by WHO because the levels found in drinking water generally do not present health concerns (WHO, 2017a ).
Water organoleptics (ie, properties we experience through taste, sight, smell, and touch) are known to shape water acceptability (Dietrich, 2006 ; Jardine, Gibson, & Hrudey, 1999 ). Poor aesthetics of safe water may even undermine adherence or sustainability of drinking water interventions (Francis et al., 2015 ). Turbidity itself presents no health effects, but can impair disinfection and signal the presence of waterborne pathogens. High turbidity in filtered water could mean poor removal of pathogens by filtration media, and an increase in turbidity in distribution systems can indicate sloughing of biofilms and oxide scales or introduction of contaminants through pipe cracks or breaks (WHO, 2017a ).
Low cost options for measuring turbidity include the Jackson candle and the turbidity tube, both of which have limited sensitivity and are not appropriate for field testing (WHO, 2017a ). The gold standard for measuring turbidity is a battery-operated turbidimeter, which can cost from $350 to $1000 plus the cost of glass vials, and works by electronically measuring the refraction of light through a water sample via a photodiode. The turbidimeter usually comes precalibrated; the operator fills a turbidity vial, wipes it clean of any fingerprints or other marks with a lint-free cloth, then inserts the vial into the turbidimeter's cell compartment, closes the compartment cover, selects any desired signal averaging procedures, and the final measurement is displayed in nephelometric turbidity units (NTU). The WHO standard for turbidity is ≤5 NTU, and ideally <1 NTU (WHO, 2017b ). The speed, accuracy, and portability of the turbidimeter are important tradeoffs to device cost.
Turbid water is not necessarily risky, and clear water is not necessarily healthy. Context is extremely important, and many household point-of-use treatment products mimic modern water treatment processes by using a flocculant that causes particles to fall to the bottom (and are ultimately strained out) during disinfection (Sobsey, Stauber, Casanova, Brown, & Elliott, 2008 ). Water with turbidity above 4 NTU is usually visible and will often affect the acceptability to consumers (WHO, 2017b ).
The most frequently measured inorganic, chemical component of treated water quality is free (or residual) chlorine. Free chlorine is the amount available for disinfection after added chlorine reacts to various organic and inorganic compounds in the water. The WHO standard for free chlorine residual in drinking water is ≥0.5 mg/L after at least 30 minutes contact time at pH < 8.0, with a minimum residual concentration of 0.2 mg/L at point of delivery (WHO, 2017a ).
The gold standard is to use a digital colorimeter. These are handheld, battery-operated devices used to measure free chlorine in a variety of field settings and are an effective proxy for the efficacy of chlorine-based disinfection, whether by a water distribution network or household point-of-use disinfection. The colorimeter works by beaming light through a water sample, some of which is absorbed by the solution, and electronically measuring the decrease in light intensity that strikes a photodiode. The device first needs to be configured with a blank reading; once configured, the operator simply fills a small vial with a water sample, adds a reagent powder pillow, inserts the vial into the colorimeter's cell, closes the cell chamber, and results are displayed in mg/L (Hach, 2014 ). A digital colorimeter costs ~$400 to $500, and the 10 ml reagent pillows are available in bulk (100 or 1000 units) for ~$0.20/pillow with a 3 to 5 year shelf life.
In low-resource settings, pool kits and color wheels are very low-cost methods that can be used to determine free chlorine levels in drinking water. However, these methods suffer from lack of calibration and standardization. As a result, they generally offer unreliable quantitative results (Center for Disease Control (CDC), 2014 ).
The most common microbiological water quality indicators are total coliforms, fecal coliforms, and E. coli (Bain et al., 2012 ). These three indicators, along with others such as heterotrophic bacteria and Pseudomonas counts, are often used as a proxy for water treatment efficiency and risk of more virulent organisms. The simplest tests indicate presence or absence of one or more indicators.
The Colilert test by IDEXX, for example, is available in tubes with predispensed reagent to which one adds a 10 mL water sample and incubates at 35 ± 0.5°C for 24 hours. After incubation, colorless tubes are negative, yellow tubes are positive for total coliforms, and yellow tubes that fluoresce under ultraviolet light are positive for total coliforms and E. coli . These tubes take 1 minute to fill, are stable at room temperature, and last about a year (IDEXX, 2019 ). When used in combination with IDEXX's Quanti-Tray, the Colilert tests can quantify coliform and E. coli levels using most probable number (MPN) tables and special software. Although the field-based Colilert tests do not require refrigeration, they do require stable ambient temperatures and must be read promptly after the 24-hour incubation period. The predispensed tubes cost approximately $1.50 per test when purchased in 100-tube increments.
Other microbiological tests use plate and film methods, where a small water sample is dispensed onto a growth medium on a plate, such as 3 M's Petrifilm, which are then incubated for 24 to 48 hours before enumerating bacterial colonies though a grid. Chuang, Trottier, and Murcott ( 2011 ) evaluated four low-cost field-based microbiological test kits for fecal indicator bacteria—the H 2 S bacteria test, Coliscan Easygel media (by Micrology Laboratories), Colilert, and Petrifilm—against the Quanti-Tray. They found promising results when these inexpensive kits are used in combination.
The multiple tube method is another traditional indicator in which a measured water sample is diluted with sterile growth medium, then a portion is decanted into multiple tubes, and the process repeated to yield a range of dilutions. After incubation, the resulting number of positive-growth tubes at each dilution level is used to calculate the concentration of the original sample. This technique is time- and labor-intensive, requires a lot of tubes and cleaning, and costs ~$2 to 5 per test. While more precise than plate methods, it is ultimately less precise than membrane filtration.
The gold standard for assessing microbiological quality is membrane filtration. A water sample is vacuumed through a membrane filter, which is then transported into a Petri dish prepared with growth medium to culture the bacteria of interest. The dish is incubated and colonies enumerated by direct microscopic count using a millimeter grid. Membrane filtration allows the testing of large volumes of water, and use of different media to differentiate organisms. It also tends to be the most expensive and labor-intensive method, requires trained staff, and uses less portable equipment.
Aquagenx's Compartment Bag Test (CBT), for example, quantifies bacterial MPN in a 100 mL water sample. It is a more recent option for testing E. coli in low-resource field conditions, and compares favorably to results from membrane filtration on m1 agar, a common laboratory growth medium (Stauber, Miller, Cantrell, & Kroell, 2014 ). To conduct a CBT, one collects a water sample in a sterile 100 mL plastic bottle or Whirl-Pak Thio Bag, adds a packet of bacterial growth medium, pours the sample in the compartment bag (and clips and seals it), incubates the bag for 24 to 48 hours, scores the MPN test results, and finally uses a chlorine tablet to disinfect the sample (Aquagenx, 2019 ). This technique has the advantages of not requiring electricity, bulky equipment, controlled incubation, or highly-trained technicians, but cannot be easily sterilized and reused like many other methods.
Cost often drives the selection of microbiological indicators. Tests for fecal coliforms and E. coli are often the least expensive given their broad use. Bain et al. ( 2012 ) catalog 44 different tests for fecal bacteria suitable for low- and middle-income settings, with material costs ranging from US$0.50 to 7.50 per test.
Scientists have identified a growing category of emerging contaminants in water due to modern anthropogenic processes and increased laboratory detection capabilities. Emerging contaminants are substances with a perceived impact on human or environmental health, but without any published health criteria. These substances include perfluorinated compounds, water disinfection byproducts, gasoline additives, pharmaceutical metabolites, and nanoparticles (Lei et al., 2015 ), as well as microplastics (Richardson & Kimura, 2016 ), plasticizers, pesticides, whitening agents, hormones, x-ray contrast media, artificial sweeteners, and flame retardants (Pal, He, Jekel, Reinhard, & Gin, 2014 ). The number of regulated chemical pollutants, for example, is now but a tiny fraction of all the anthropogenic chemicals present in the environment (Daughton, 2004 ).
Measurement of emerging contaminants requires advanced laboratory capabilities. Solid phase extraction with Oasis HLB cartridges has recently been the most common method of extraction and concentration for emerging contaminants in water. Solventless extraction techniques, such as solid phase microextraction, and dispersive liquid-liquid microextraction (which uses very little water, ie, ~10 mL) are increasingly being used (Richardson & Kimura, 2016 ). Liquid chromatography and high-resolution-mass spectrometry, as well as complementary analytical techniques such as nuclear magnetic resonance spectroscopy, have also been growing in use for identifying unknown contaminants (Richardson & Kimura, 2016 ).
These procedures are very far from being deployable in field test kits. They are generally only available at significant cost by leading laboratories in high-income settings. This constrains modern characterization of water quality, for most of the world, to basic physico-chemical and microbiological parameters.
There is an urgent need to conduct routine, cost-effective testing of emerging contaminants. Many of these agents—particularly pesticides, pharmaceuticals, industrial chemicals, and surfactants—have entered the water cycle with suspected toxicity for humans (and ecosystems), but are rarely, if ever, tested in municipal water systems (Rosenfeld & Feng, 2011 , p. 215). We know even less about how emerging contaminants may interact with each other or other human chemical exposures currently considered to be benign.

Section: 4 ESTIMATING HOUSEHOLD WATER INSECURITY STATUS

Since water is generally collected, stored, and used within households, the household is an important unit of analysis for water-related studies. As the field of water security exploded in recent years (Bakker, 2012 ), the definition of household water insecurity evolved quickly. The field defines household water insecurity experiences as broadly encompassing people's ability to “engage with and benefit from” water and the institutions that manage “water flows, water quality, and water services” (Jepson, Budds, et al., 2017 , p. 47). This emerging scholarly literature provides several key parameters that, taken together, can provide effective estimates of household water insecurity (see Wutich et al., 2017 ; Jepson, Wutich, et al., 2017 for reviews and Fam et al. 2015 for an introduction); these are summarized in Table 3 . New research indicates that household water insecurity is associated with a range of health outcomes including breastfeeding, water-carrying injuries, and mental ill-health (Boateng et al., 2018 ; Geere et al., 2018 ; Wutich, 2020 , in this issue).
One of the most common and simplistic measures of household water insecurity is the water source. The adequacy of water sources is commonly assessed using a tool developed by the Joint Monitoring Programme (JMP) for Water Supply and Sanitation, which is managed by WHO and UNICEF ( 2006 , 2017 , 2019 ). The JMP ladder for drinking water classifies water sources into five levels: (a) safely managed (a contamination-free improved water source, located on the premises), (b) basic (an improved source where water fetching takes <30 minutes), (c) limited (an improved source where water fetching takes >30 minutes), (d) unimproved (an unprotected spring or well), or (e) surface water (canal, river, lake, pond, dam, and so on). The JMP collects data on water sources from water regulators, water administrators, and household surveys, but the protocol can also be implemented in household surveys alone. While the JMP approach is used for assessing major global initiatives like the Sustainable Development Goals, it has been widely critiqued as failing to measure important determinants of water insecurity like water quality, cost, accessibility, availability, adequacy, or cultural dimensions (Bain et al., 2014 ; Mehta & Movik, 2014 ; Satterthwaite, 2003 ).
In piped water systems, household water use is typically tracked using a meter. In informal, hybrid, or unpiped systems, water diaries can be used to collect data on total household water use for all lifestyle activities including cooking and cleaning, storage, expenditure, and treatment (Apoorva, Biswas, & Srinivasan, 2018 ; Harriden, 2013a ; Harriden, 2013b ; Hoque & Hope, 2018 ; Bishop, 2015 ; Wutich, 2009a ). Diaries typically include respondent self-measurement and self-report on household water practices. They may also include structured or semi-structured reflections on the experiences of water in the household. Compared with household surveys using recall methods, water diaries are the more accurate method for collecting household water use data; diaries have the advantage of minimizing forgetting and response bias (Apoorva et al., 2018 ; Wutich, 2009a ). However, water diaries also have potential disadvantages including: training requirements, respondent burden, and errors related to innumeracy, event omission, or inaccurate measurement (Wutich, 2009a ).
Water diaries can be used to assess the number of liters of water consumed per capita per day (lpcd), but there is no universally-accepted cutoff for meeting minimum water needs. In order to determine whether or not household water use is adequate to meet basic consumption and sanitation needs, it is useful to consult WHO guidelines (WHO, 2005 ; World Health Organization, & Water, Engineering and Development Centre, 2011 ) on the number of liters needed per person per day for a range of activities and contexts. The United Nations Special Rapporteur on the Human Right to Safe Drinking Water and Sanitation states that 20 lpcd is needed to reach minimum levels of a human right to water, but that 50 to 100 lpcd is a more appropriate target to fulfill the human right to water (United Nations, 2019 ).
Affordability is another key dimension of water insecurity. Common metrics for estimating household-level affordability include: (a) water expenditures as a share of household income, and in low-income countries the affordability threshold is between 3% and 6%; (b) cost of equipment needed to access water; (c) financial and economic costs as a percentage of annual income, and in low-income countries, it is calculated as 5% or less of the community's median household income (Hutton, 2012 ). Where water bills are unavailable, water affordability data are often collected through self-report surveys and are vulnerable to recall errors and other reporting biases.
New approaches attempt to capture the relative cost of water as it trades off against other household resources and needs (eg, labor time and effort) (Wutich et al., 2017 , p. 3). Alternative measures would use affordability calculations that address, for instance, consumption levels and the affordability benchmarks (Gawel, Sigel, & Bretschneider, 2013 ; Mack & Wrase, 2017 ; Smets, 2009 ), as well as the direct costs and labor time for water treatment and storage management (Vandewalle & Jepson, 2015 ). Such alternative measures are being developed to address the wider range of costs and tradeoffs involved in water affordability, but these are not yet widely used and may be particularly difficult to apply in informal economies and field settings (Davis & Teodoro, 2014 ; Hutton, 2012 ).
Water storage is an important dimension of water insecurity in unpiped households or those with intermittent water service (Bain et al., 2014 ; Kumpel et al., 2017 ). Storage of water increases risk of water (re)contamination, an important factor for the documented diarrheal disease burden (Clasen & Bastable, 2003 ; Clasen & Cairncross, 2004 ; Copeland et al., 2009 ), and may support mosquito-borne disease transmission by inadvertently providing mosquito breeding sites. Water storage is generally measured in liters, and may be observationally collected by assessing the household's barrels, rainwater collection equipment, underground tanks, or other storage containers (Hadley & Wutich, 2009 ).
Intermittency refers to nonconstant availability of water service; Intermittency can be particularly problematic for households when water service availability is unpredictable (Galaitsi et al., 2016 ; Hamer et al., 2018 ). Intermittent water supplies in piped or centralized systems affect chlorination and increase the risk of microbial contamination of drinking water supplies (Bain, Cronk, Hossain, et al., 2014 ; Kumpel & Nelson, 2013 ). In addition, intermittency increases the need for water storage, which, as previously discussed, increases risk of water recontamination (Elala, Labhasetwar, & Tyrrel, 2011 ; Levy, Nelson, Hubbard, & Eisenberg, 2008 ). Intermittency can be assessed observationally (eg, through participant-observation), using key informant interviews (eg, with water providers), or with survey and interview data. While the existence of intermittency and its predictability is fairly easy to document, detailed data on intermittency would require the collection of costly and potentially less accurate observational, self-report, or recall data.
Water treatment is a common water management strategy to support health outcomes at the household level (Lilje & Mosler, 2017 ). Typically, data collection focuses on conventional forms of household water treatment (eg, boiling, use of filters, chlorination, and solar UV disinfection) (Hunter, 2009 ). Since water treatment approaches vary widely across communities, it may be important to explore local practices (eg, bleach, settling, straining through cheesecloth). Developing an inventory of all household water treatment strategies for a community may require formative research using methods like participant-observation, focus groups, or interviews. For household assessments, appropriate methods include observational or survey protocols with checklists assessing water treatment usage and frequency. Since water treatment can be costly (in terms of time and resources), the costs of water treatment should be included in assessments of household water insecurity (McLennan, 2000 ; Shrestha, Thapa, et al., 2018 ; Shrestha, Aihara, et al., 2018 ).
The spatial dynamics of household water acquisition are a recently-explored dimension of household water insecurity. A novel GPS-based system on jerry cans, developed by Pearson ( 2016 ), yields an estimate of minutes per roundtrip, distance traveled, and validates data for self-reported water journeys. Spatial video is another tool to collect accurate water acquisition data—such as time, path, and hazards—to assess the risk and burden to household members in urban and peri-urban environments (Curtis et al., 2019 ; Smiley, Curtis, & Kiwango, 2017 ). Such methods combine video with embedded GPS coordinates, which are then digitized to a geographic information system. Spatial video provides a visual mechanism to correct for GPS bounce, verify walk purpose, and document the water fetching path in terms of health and environmental hazards, such as standing water, open drains, trash, and potential practices where water could be (re)contaminated in the conveyance process. These methods have all been deployed successfully in field research settings, but spatial video can require data coding, which is (at present) costly and time-intensive.
The development of household water insecurity scales and indices, based on data collected from household surveys, has advanced significantly in the last decade (Lester & Rhiney, 2018 ; Shrestha, Aihara, et al., 2018 ; Shrestha, Thapa, et al., 2018 ). A wide range of locally-grounded scales have emerged to measure household water insecurity following Wutich's first household water insecurity scale in Cochabamba, Bolivia (Hadley & Wutich, 2009 ; Wutich, 2006 ; Wutich & Ragsdale, 2008 ). Locally-grounded water insecurity scales have been developed in sites in many low- and middle-income countries including: Ethiopia (Stevenson et al., 2012 ), Nepal (Aihara, Shrestha, Kazama, & Nishida, 2015 ) Palestine's West Bank (Galaitsi et al., 2016 ), Uganda (Tsai et al., 2015 ), South Africa (Bulled, 2017 ), Jamaica (Lester & Rhiney, 2018 ), Bolivia's Amazon region (Rosinger, 2018 ), Kenya (Boateng et al., 2018 ), and Cameroon (Nounkeu & Dharod, 2018 ). For use in high-income countries, the only known household water insecurity scale is Jepson's scale, developed for colonias in Texas, US (Jepson, 2014 ; Jepson & Vandewalle, 2016 ).
An important recent advance was the development of the Household Water Insecurity Experiences (HWISE) scale (Young et al., 2019 ; Young et al., 2019 ), a scale derived from data in 28 sites in low- and middle-income countries. This 12-item scale includes questions that capture household experiences such as feeling thirst, the inability to wash hands or bodies due to water problems, and worry over water. An advantage of such a scale is that it potentially enables direct comparison of data collected across sites or through time (Jepson, Wutich, et al., 2017 ) for applications such as monitoring and evaluation. However, since water insecurity is often a locally, culturally, and geographically unique phenomenon, sometimes locally specific scales or indices are more appropriate, as some scholars have demonstrated (Wutich, 2020 , in this issue). For example, the HWISE scale does not include items about water quality, water for agriculture, or household food production, which may be essential components of local water insecurity in some communities.
Before adopting or revising any household water insecurity scale, it is important to note three major unresolved points to be considered alongside the goals of the research. First, there are important trade-offs to make between the validity of locally-adapted scales vs the comparability of cross-site scales like Young and colleagues' HWISE scale. The second challenge is the question of dimensionality; there is no consensus on whether uni-dimensional or multi-dimensional scales better capture household water insecurity in ways relevant to modeling with human biological (or other) data. Recent literature points to the need for more subscale development so as to differentiate water quality and quantity, thus, suggesting the rising need for capturing water phenomena multidimensionally (Jepson, Wutich, et al., 2017 ; Subbaraman et al., 2015 ). Third, there is no current agreement on recall periods nor the appropriate recall intervals for scalar questioning on experiences of water insecurity. Recall periods vary across scales from 1 week to 12 months (by comparison, food insecurity scales tend to apply a more standard 2- to 4-week window).
Brief “screener” items can provide alternatives to multiple-item scales as an effective and efficient means to capture a summary assessment of household water insecurity. In some research, a one- or two-question screener might be sufficient to summarize basic issues of low water quantity and quality. For example, in Wutich and Brewis' ( 2019 ) research, in which water insecurity is a minor rather than focal variable, they simply ask: (a) In the last month, was there a time when your household did not have enough water for drinking, cooking, bathing, handwashing, and other household tasks? [quantity] (b) In the last month, would you describe the water your household used as acceptable quality? [quality] While a water insecurity screener provides less precise measurement than scales, it is easily deployed cross-culturally and has a much lower respondent burden than a scale if detailed water insecurity information is not required (eg, is included in a study as a covariate).

Section: 5 FUTURE DIRECTIONS

The science of measuring water needs has developed unevenly. Measurements of hydration and water quality have been well-established for decades, though there is a significant need for new methods of water quality measurement for contaminants of emerging concern. In contrast, the measurement of household water insecurity is a largely new field, with only a few metrics and little overall agreement over which are best to apply under what conditions. The expanding range of measures currently available allows new areas of research to begin opening up. Table 4 contains useful resources for further review.
One of the emerging issues in hydration research is that hydration biomarkers are often used without knowing the participant's current hydration state. Additionally, they may not be measured under experimental conditions where it is possible to confirm exactly what percentage of body water has been lost. Therefore, recent advances in the field point to an evolution of how we should think about hydration changing from a state to a process (Perrier et al., 2014 ), as well as changing the terminology away from “dehydration” when not in an experimental setting to that of “inadequate hydration” or “underhydration” (Kavouras, 2019 ). Future research should aim to better assess baseline hydration status and changes or individual-level variation from the baseline. Furthermore, dried blood spot analysis is a methodological development needed to assess blood biomarkers of hydration status in remote settings.
Laboratory-based capabilities for assessing water quality have raced ahead. We can now measure certain trace contaminants in water in ever smaller concentrations, such as parts per trillion. Yet as the number of anthropogenic chemicals found in drinking water increased exponentially in the 20th century globally, water quality metrics have continued to focus on organoleptics and traditional microbiological indicators. Technology for point-of-use methods has lagged far behind. As a result, most of the world's water supplies are now poorly characterized with residents unable to assess potential exposures, particularly in contexts where residents regularly choose among multiple water sources. Significant advances are needed for portable, low-cost, multi-agent test media that can allow communities to play a more meaningful role in monitoring their water, and ultimately improve local environmental stewardship and water security.
Water adequacy has long been monitored using well-established metrics to assess water sources and affordability. However, these metrics are widely understood to be inaccurate and to contribute to the underestimation of water insecurity (Mehta & Movik, 2014 ; Satterthwaite, 2003 ). As a result, there has been an enormous amount of effort invested in advancing the field of water insecurity measurement in the last 10 years (Wutich et al., 2017 ), including for water diaries (Hoque & Hope, 2018 ). Two areas of rapid methodological development are around spatial approaches and water insecurity scales. First, scholars are developing new ways to incorporate GPS in collecting water insecurity data (eg, spatial video). Second, scale development for measuring water insecurity in low- and middle-income countries is an area of recent rapid progress; more research is needed to develop scales suitable for use in high-income countries.

Section: 6 CONCLUSION

Unmet human water needs can produce a variety of negative biological and health outcomes that are of interest to human biologists. Measures of hydration (and underhydration) are important for measuring the impacts of water need on human biological functioning. Measures of water quality are essential for understanding the health risks associated with exposure to microbiological, organic, metal, inorganic nonmental, and other contaminants. Finally, the emerging field of household water insecurity measurement provides a range of methods for understanding how inadequate water service, unaffordability, and the experiential dynamics of water need are associated with negative health outcomes. Together, these methods open up the possibility of exploring many new research questions about the impacts of water needs on human biology and health.

Section: AUTHOR CONTRIBUTIONS

A.W., A.R., and A.B. conceptualized this manuscript. A.W., A.R., J.S., W.J., and A.B. all contributed to the writing, editing, and revision of this draft.

Section: ACKNOWLEDGEMENTS

Our work was supported by NSF grant BCS-1759972 HWISE RCN: Building A Community of Practice for Household Water Insecurity Research (HWISE). Wutich and Brewis acknowledge support for the Global Ethnohydrology Study, funded as part of the ASU Center for Global Health. Wutich received support from NSF grants SES-1462086 and DEB-1637590. We thank our close colleagues Barbara Piperata and Amanda L. Thompson for generously providing feedback on earlier drafts and helping us to improve the clarity of our thinking and writing.
