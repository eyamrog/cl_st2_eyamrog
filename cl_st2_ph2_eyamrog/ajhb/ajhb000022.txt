Title: Medieval menarche: Changes in pubertal timing before and after the Black Death


Abstract: Abstract


Abstract_Section: Objectives

Bioarcheological evidence suggests stature increased in males but decreased in females after the Black Death (1348-1350 CE). Because tradeoffs between growth and reproduction can result in earlier ages at menarche and lower limb length, we assess menarcheal age between 1120 and 1540 CE to better understand the health of medieval adolescent females before and after the plague.

Abstract_Section: Materials and Methods

Our sample comprises 74 adolescent females from St. Mary Spital, London (1120-1540 CE) within the age range during which menarche occurs (10-25 years). They were assessed as being pre- or post-menarcheal and divided into three groups: Early Pre-Black Death (n = 13), Late Pre-Black Death (n = 38), and Post-Black Death (n = 23). Changes in the ages of pre- and post-menarcheal females were assessed using Mann-Whitney tests.

Abstract_Section: Results

The average age of post-menarcheal females increased from the Early- to Late Pre-Black Death periods and declined after the Black Death.

Abstract_Section: Conclusions

Short stature can reflect unfavorable growth environments, while younger menarcheal age indicates improved living conditions. The paradoxical pattern of female, but not male, stature reduction after the Black Death might reflect the association of early menarche with lower limb length and signal that adolescent females experienced improved health conditions after the epidemic. Our focus on pre- and post-menarche within a limited age span provides a novel approach for inferring average ages of menarche over time. Pathways to skeletal development and reproductive investment are part of an integrated system, providing a bridge between life history research in bioarchaeology and human biology.

Section: 1 INTRODUCTION

Recent paleodemographic research indicates that survivorship (which reflects underlying population health) declined before the 14th-century Black Death (1348-1349 CE) but improved in its aftermath in London (DeWitte, 2014a , 2014b , 2015 , 2018 ). Analyses of adult tibia length (a proxy for adult stature) indicate significant declines in stature before the Black Death but significant increases afterward for males. In contrast, female stature increased (but not significantly) before the epidemic but decreased significantly afterward (DeWitte, 2018 ). Parallels between the trends in survivorship and stature for males might reflect a relatively straightforward relationship between developmental stress and adult survivorship for males. However, the results for females are more perplexing. The decrease in female stature in the context of improved survivorship after the Black Death appears contradictory, as the first suggests, at worst, increased physiological stress (and perhaps poor health) whereas the latter indicates improved general health. One possible explanation for these disparate findings for the sexes is that after the Black Death, females were better buffered (eg, in terms of immunocompetence) against dying from stressors that disrupt growth. Sex differences in biological buffering that favor females have been demonstrated by studies of living and past populations (see, eg, reviews by Guatelli-Steinberg & Lukacs, 1999 ; Stinson, 1985 ). If females were better buffered during development in medieval London, this might have produced more heterogenous cohorts of adult females (with respect to stature) compared to males—that is, a greater number of short females survived to adulthood after the Black Death than was true beforehand or relative to males. An alternative interpretation is that reduced female stature after the Black Death actually reflects improvements in nutritional status or disease burden.
This latter interpretation hinges on energy tradeoffs made between reproduction and growth and the associations observed in living populations between nutrition, disease, age at menarche, growth, and final adult stature. Puberty occurs as the result of stimulation of the hypothalamic-pituitary-gonadal (HPG) axis and the hypothalamic-pituitary-adrenal (HPA) axis. The HPG axis is latent during childhood and reactivated in adolescence through the secretion of gonadotropin-releasing hormone (GnRH) by the hypothalamus, ultimately leading to stimulation of the gonads (Ellison et al., 2012 ). The adolescent growth spurt, while stimulated by gonadal steroid hormones, is also reliant on the production of insulin-like growth factors by the pituitary gland (or the GH/IGF-1 axis) (Marshall & Tanner, 1986 ; Reiches, 2019 ). Within a year of this peak height velocity (PHV) being reached (ie, during the deceleration phase), females menstruate (Zacharias & Wurtman, 1969 ). The timing of menarche is the result of a complex interaction between genetic signaling, fetal programming (or preconditioning), and short-term adaptation to severe stress or under nutrition (Dorn, Hostinar, Susman, & Pervanidou, 2019 ; Wojtyla, Wojtyla-Buciora, & Marcinkowski, 2012 ; Worthman, Dockray, & Marceau, 2019 ). Approximately 50% of the variation in the timing of menarche is genetically influenced (Towne et al., 2005 ). Environmental factors such as nutritional status, body mass, exercise, socioeconomic status, education, psychological stress, and urban vs rural residence may delay or accelerate menarche (or be associated with factors that do so) within each generation (Ellis, 2004 ; Šaffa, Kubicka, Hromada, & Kramer, 2019 ). However, the timing of menarche may also be affected by in utero environments and exposures. Adverse circumstances in the womb may program the fetus to expect a shorter life span, thereby resulting in early menarche to allow for earlier reproductive success (Wojtyla et al., 2012 ). Life history theory dictates that there is a trade-off between the energy and resources required for growth vs that required for reproduction, and during adolescence there is a shift in energy allocation from development toward reproduction (Ellison et al., 2012 ; Jazwiec & Sloboda, 2019 ). Hence, a female only achieves an additional 2 to 4 in. of growth after menarche, which thus marks the point at which she has attained most of her final adult height (Nakamoto, 2000 ; Tanner, 1978 ).
Menarcheal age is viewed as a reflection of population health (Cho et al., 2010 ) and may be an appropriate estimator for socioeconomic conditions in historical populations (Lehmann, Scheffler, & Hermanussen, 2010 ). Mean age at menarche is more strongly related to the growth environment (as measured by gross domestic product, or GDP, per capita) than height, and thus it better reflects the living standards of a population than average height (Sohn, 2017a ). As detailed below, age at menarche declines in some contemporary populations with improvements in nutritional status and disease burden, and further, a change in the age at menarche is associated with changes to adult stature within some contexts. While most studies focus on industrialized populations, there is evidence from modern nonindustrial populations that an earlier age of menarche is associated with shorter leg length in adult females (Schooling et al., 2010 ). If health conditions improved substantially following the Black Death, adolescent girls may have experienced an earlier age at menarche and earlier skeletal maturation. This cessation of long bone growth reflects the overlap in simultaneous investments in growth and may have resulted in a shorter stature for these healthier adult females after the Black Death compared to pre-epidemic conditions.
Although reliable information about pubertal timing is generally lacking from historical documents (see, for example, Green's, 2001 : p. 215] cautionary note regarding reported ages of menarche and menopause in historical documents) there is some intriguing historical evidence that a decline in the age at menarche occurred after the Black Death: a cleric in London wrote in the late 14th-century that “In ancient times, the menses did not begin to flow until the fifteenth or fourteenth year, or certainly not before age twelve. But now they begin in certain girls in the eleventh or in the tenth year” (Green, 2005 ; p. 55). This observation can be confirmed using bioarchaeological data with precise chronological control; these data can also be used to assess trends in menarcheal age in the time period before and after the Black Death to clarify the effects that environmental and health conditions might have had on pubertal timing during the medieval period.
This study tests the hypothesis that average age at menarche declined from the pre- to the post-Black Death periods by assessing skeletal indicators of pubertal events using a sample from medieval London. Most of the existing research on changes in pubertal timing, including age at menarche, has been done to clarify the timing of and variation in secular trends in growth and development within modern industrialized countries. This research has, in particular, revealed general trends of earlier puberty and increases in stature, at the population level, that seem to be a consequence of improved living conditions (eg, better nutritional status and reduced infectious disease burdens) that can accompany industrialization. The secular trend of earlier age at menarche has been documented since the mid-19th century (Lehmann et al., 2010 ; Wyshak & Frisch, 1982 ). However, although patterns of and trends in pubertal timing that predate industrialization and modernization have received attention recently (Arthur, Gowland, & Redfern, 2016 ; Doe, Pérez, Cambra-Moo, Martín, & Martín, 2019 ; Lewis, Shapland, & Watts, 2016 ), they remain largely understudied and thus unresolved. Confirmation of an earlier age at menarche (relative to pre-Black Death conditions), with the possibility of a positive association between age at menarche and female stature, in post-Black Death medieval England will tell us more about the effects of the Black Death, specifically, and importantly, perhaps will be informative about how conditions of growth affect puberty in contexts beyond the modern, industrialized societies that predominate studies of pubertal timing.

Section: 2 MATERIALS AND METHODS

Our study sample comprises 74 adolescent females aged between 10 and 25 years from the cemetery of St. Mary Spital (SRP98), London. This was the main cemetery associated with the hospital and priory of St. Mary Spital; it was in use from 1120 to 1540 CE and is widely representative of the medieval population of London, as monks, lay sisters, and wealthy benefactors of St. Mary Spital were also interred in the cemetery (Connell, Gray Jones, Redfern, & Walker, 2012 ). There were four periods of use within the cemetery: Period 14 (c. 1120-1200), Period 15 (c. 1200-1250), Period 16 (c. 1250-1400), and Period 17 (c. 1400-1540) (Sidell, Thomas, & Bayliss, 2007 ). There are four different types of burials in St. Mary Spital, all of which were used in each Period (Connell et al., 2012 ). Single interments (Type A burials) and two types of relatively small group burials (Types B and C) are considered to be associated with normal “attritional” mortality (Connell et al., 2012 ). The fourth burial type, relatively large group burials (Type D), is associated with catastrophic mortality events, most likely famine (Connell et al., 2012 ). This study uses adolescent females from all burial types across Periods 14, 15, and 17. We exclude individuals buried in Period 16 to enable comparison of pre- vs post-Black Death trends, as we are primarily interested in the possible effects of the Black Death on pubertal timing. Our sample includes individuals who were sufficiently preserved to provide necessary data on age, sex (and specifically those who were determined to be female or probable female), and for whom we could determine whether they had passed menarche, using the methods described below.
As our focus was on the health of adolescent females around the time they would have experienced menarche, the sample only includes individuals from age 10 up to the age of 25 years, encompassing the World Health Organization's ( 1993 ) definition of this life stage and the age range during which menarche is expected to typically occur. The temporal resolution in our samples is not sufficient to examine intragenerational- and short-term intergenerational variation in growth conditions and menarcheal age; however, one of the benefits of the relatively long timeframes represented by these samples is that we can potentially circumvent the limitation of intergenerational inertia with respect to development and reproductive investment. The sample sizes used from each Period within St. Mary Spital are shown in Table 1 . Given the small size of our samples, particularly for the Early Pre-Black Death period, the conclusions that we can draw here are necessarily tentative.
Age at death was estimated using the development of the mandibular permanent dentition, with the exception of the canine. A mean dental age was calculated using the average of the ages for all observed teeth (Moorrees, Fanning, & Hunt Jr., 1963 ; and for the third molar: Liversidge & Marsden, 2010 ). Individuals with a mean dental age were then placed into one-year age categories (ie, 10.0-10.9, etc) to allow for a more detailed assessment of age at menarche (Shapland & Lewis, 2013 ). When dental development was complete, age was determined using skeletal maturation of the pelvis (excluding the iliac crest), sacrum, medial clavicle, and vertebral annular rings (Albert & McCallister, 2004 ; Schaefer, Black, & Scheuer, 2009 ; Scheuer & Black, 2000 ). Following Lewis et al. ( 2016 ), those assigned to the 22-25-year category demonstrated complete fusion of the vertebral annular rings, ischial epiphysis and sacrum, and a fusing epiphysis at the medial clavicle (c. 21-25 years). Anyone with a fused S1-S2 junction and fused medial clavicle epiphysis was considered to be over 25 years of age and was excluded from the study.
Skeletons aged 10.0 to 13.9 years were sexed based on features of the pelvis and humerus, and “female” or “?female” were assigned when observations in both areas agreed. Only traits that have been reported to achieve over 70% accuracy in non-adult individuals were selected (ie, ilium: sciatic notch angle (72%), sciatic notch depth (81%), auricular elevation (72%-85%); humerus: trochlear symmetry (81.5%), olecranon fossa shape (85%), medial epicondyle angle (78%); mandible: chin prominence (73%) (Falys, Schutkowski, & Weston, 2005 ; Sutter, 2003 ). After the age of 15 years, standard methods based on the pelvis and mandible were used (Buikstra & Ubelaker, 1994 ). Features of the cranium that are often used to assess sex, based on the robusticity thereof, were avoided as they are often unpronounced in young males and could give a false “female” result (Walker, Johnson, & Lambert, 1988 ).
An assessment of pubertal stage was carried out using the osteological methods developed by Shapland and Lewis ( 2013 , 2014 ) and Lewis et al. ( 2016 ). This included assessment of the mineralization of the canine; morphology of the cervical vertebrae (CVM); development of the hamate hook; epiphyseal fusion of the distal humerus, proximal ulna and distal radius; fusion of the metacarpals and hand phalanges; and ossification and fusion of the iliac crest. A pubertal stage was assigned where three or more features could be observed and at least three agreed. Females tend to achieve menarche after peak height velocity of growth (PHV) and during the deceleration phase toward the end of the growth spurt (Tanner, 1978 ). Hence, adolescent females were placed within the “menarche passed” group (post-menarcheal) where the epiphyses of the hand phalanges were capped or partially fused, there was at least partial fusion of the capitate of the distal humerus and distal ulna, the hamate hook was complete (stage I), and CVM was stage 3 or above. Ossification of the superior iliac crest was also taken as an indicator that PHV had been passed, and that menarche had occurred (Lewis et al., 2016 ). The iliac crest is known to “appear” within approximately six months of menarche being achieved (Buehl & Pyle, 1942 ; Palma, Cavina, Giusti, & Borghi, 1967 ), but as an unfused ossified iliac crest rarely survives in the archaeological record, its absence was not used as an indicator that a female was pre-menarcheal. Pre-menarche (ie, the initiating or acceleration phase of the growth spurt) was only assigned when the individuals displayed an incomplete hamate hook, developing canine root, and unfused elbow and wrist.
To assess temporal trends in the timing of menarche, for each of the three time periods (Early Pre-Black Death c . 1120-1200, Late Pre-Black Death c . 1200-1250, and Post-Black Death c . 1400-1540), we estimated the average ages at death of the adolescent females who had not yet passed menarche at the time of their death (pre-menarcheal) and of those who had passed menarche at the time of their death (post-menarcheal). The average ages at death of pre- and post-menarcheal females are both viewed here as potentially informative about the average age at menarche for the corresponding sample; for example, we would interpret relatively young average ages at death for pre-menarcheal or post-menarcheal females (or both) as indicating a relatively young average age at menarche. Previous research indicates that London females achieved menarche round 17 years, and the majority had completed their skeletal maturation by 25 years (Lewis, 2016 ). Only one of these females had an ossified but unfused iliac crest, making it impossible to identify a more specific age of menarche for each individual and between each time period. By assessing the age at death of females who died within a 15-year period when menarche would have been achieved, and using these broad divisions of pre- and post-menarcheal, we have devised a novel approach by which general fluctuations over time can be compared in the largest possible sample.
Paleodemographic research must carefully consider the effects that deviations from population stability (eg, because of migration or changes in age-specific fertility or mortality rates) can have on age-at-death distributions observed in skeletal samples. Previous research using human skeletal remains from medieval London has indicated that age-at-death distributions differed significantly between the Early Pre-Black Death and Late Pre-Black Death periods (DeWitte, 2015 ) and between the Pre- and Post-Black Death periods (DeWitte, 2014b ). In light of these findings, and because variation in the age-at-death composition of our samples in general might influence our findings with respect to the ages of pre- and post-menarcheal adolescent females, we analyzed differences in the adolescent female age-at-death distributions among the Early Pre-Black Death, Late Pre-Black Death, and Post-Black Death periods using Kolmogorov-Smirnov tests. We initially assessed variation in ages at death of pre-menarcheal and post-menarcheal females (separately) among the three time periods using one-way ANOVA; as the distributions of mean ages at death for some of the subsamples deviated from normality, we also assessed differences in ages at death among all three time periods using the Kruskal-Wallis H test. Ultimately, however, given the previous findings reported in DeWitte ( 2018 ) and described in the Introduction, we are specifically interested in between-pair differences rather than whether there are differences in general among the three time periods. Thus, we compared the ages at death of pre-menarcheal and post-menarcheal females across all possible pairs of time periods used in this study (ie, Early Pre-Black Death vs Late Pre-Black Death, Late Pre-Black Death vs Post-Black Death, and Early Pre-Black Death vs Post-Black Death patterns) using the Mann-Whitney test. The Mann-Whitney test is applicable when data are not normally distributed. All analyses were done in SPSS (version 26) using a sample that included individuals scored as “female” and as “probable female” (n = 74), as well as using a smaller sample (n = 51) that excluded the probable females in order to control for the potential inclusion of males or intersex individuals in the sample.
Although we are wary of relying too heavily on conventional hypothesis testing with an arbitrary P -value (see, for example, Greenland et al., 2016 ), particularly given our relatively small sample sizes, following previous work (eg, DeWitte, 2018 ), we view P -values of less than .1 indicating a trend worthy of consideration and further study. We chose this alpha level in light of the fact that, as is typical in bioarchaeology, we are working with small sample sizes, and our primary motivation is to determine whether there is a general trend that might be both informative about conditions in the context of medieval London and that might stimulate further study, ideally with larger sample sizes. Ultimately, we also want to demonstrate the potential utility of focusing on pubertal changes as a reflection of standards of living in past populations. Given the recognized arbitrary nature of a conventional alpha level of .05, it is our view that adherence to such an alpha level might hinder dissemination of results that, though not significant at relatively conservative alpha levels, are still worthy of consideration as potentially reflecting larger population patterns.

Section: 3 RESULTS

The results of the Kolmogorov-Smirnov tests indicate no significant differences in the age-at-death distributions among the samples of adolescent females used in our study (Early Pre-Black Death vs Late Pre-Black Death P = .961; Early Pre-Black Death vs Post-Black Death P = .999; Late Pre-Black Death vs Post-Black Death P = .998). The percentages of females and probable females who were pre- and post-menarcheal in each time period are shown in Figure 1 . The mean ages at death for individuals who were pre-menarcheal and post-menarcheal in each time period are shown in Table 2 , as are the results of the one-way ANOVA (the results of the Kruskal Wallis test are consistent with those of the ANOVA, and thus we report only the latter for the sake of simplicity). The results of the Mann-Whitney tests are shown in Table 3 (note that the results of the Mann-Whitney tests report mean ranks, not mean ages at death, for each subsample for each pair-wise comparison). These tables include results for the larger pooled sample of females and probable females, and the more restrictive sample excluding probable females. The ANOVA results indicate a significant difference among all three time periods with respect only to the ages of post-menarcheal females (using the “female only” sample). Further, the results of the Mann-Whitney tests using the larger “female”/”probable female” sample indicate no significant changes over time in the observed ages at death for adolescents were pre-menarcheal females nor for those who were post-menarcheal. Analyses of the more restrictive “female”-only sample similarly revealed no significant changes over time in the ages-at-death of pre-menarcheal adolescents. However, analysis of the “female” sample indicates there is a significant increase from the Early Pre-Black Death to the Late Pre-Black Death period in the ages at death of adolescents who were post-menarcheal ( P = .088), and also a significant decrease from the Late Pre-Black Death to the Post-Black Death period in the observed ages at death of adolescent females who were post-menarcheal ( P = .094). Box-plots of mean ages at death for individuals who were pre-menarcheal and post-menarcheal are shown in Figures 2 and 3 .

Section: 4 DISCUSSION

These results suggest that the average age at death of post-menarcheal adolescent females increased from approximately 19 years in the Early Pre-Black Death period to 22 years in the Late Pre-Black Death period and then declined from 22 years to 19 years after the Black Death. These findings would indicate that the average age at menarche also increased before the Black Death and then decreased after the epidemic. The results indicate no significant differences between the Early Pre-Black Death and Post-Black Death periods, which might suggest similar conditions of growth for the two periods. We found no significant changes over time with respect to average ages at death of pre-menarcheal females. The lack of significant differences, over time, in the adolescent age-at-death distributions, as indicated by the Kolmogorov-Smirnov tests, indicates that the observed trends in post-menarcheal age are not an artifact of changes in the underlying age-at-death distributions. Even if (as indicated by previous research by the first author) the age composition of the once-living population did vary over time, perhaps because of migration or changes in age-specific mortality or fertility rates, such possible changes do not appear to have affected the composition of the samples used for this study in ways that would affect our evaluation of temporal changes in the timing of menarche.
As detailed above, previous analyses of a larger sample of human skeletal remains from medieval London revealed that adult female (n = 135) tibial length increased (although not significantly) by 1.88 mm from the Early Pre-Black Death to Late Pre-Black Death period and then decreased significantly, by 5.58 mm, in the Post-Black Death period (DeWitte, 2018 ). These trends in tibial length, and by inference, adult female stature, occurred at the same time that survivorship changed substantially in ways that suggest declines in health before the Black Death and improvements in its aftermath. The results of the current study suggest that there were also important changes in the average age at menarche across the medieval period, which in turn might be reflected in patterns of female stature. These findings further suggest substantial changes in nutritional status, disease burden, physiological, and perhaps psychological health before and after the Black Death.
Changes in the timing of menarche have been used by various scholars as a biological indicator of changes in standards of living. For example, Sohn ( 2016 ) found that an improved living environment (as measured by GDP) was accompanied by a decrease in the age at menarche (and increased life expectancy at birth) in Korea from 1940-2010; early menarche may therefore signal better living conditions (Sohn, 2017b ). Many studies have shown earlier pubertal development in girls of higher socioeconomic status compared to girls from lower status, reflecting status-mediated differences in health and nutrition (Ellis, 2004 , p 200). Examination of data from various populations reveals that declines in age at menarche, specifically, have tended to occur earlier in higher status women compared to working class women since at least the mid-19th century (Lehmann et al., 2010 ). Other studies indicate that earlier menarche is associated with improved nutritional status and more dietary energy (eg, Cho et al., 2010 ; Chowdhury et al., 2000 ; Padez, 2003 ; Rah et al., 2009 ). There is some evidence, for example, that diets high in calcium, animal protein, and animal fat are associated with an earlier age at menarche (Wiley, 2011 ). Ellis ( 2004 ) points out that nutritional deprivation delays the onset of puberty, but that within populations with adequate nutrition, variation in the quality and quantity of diets has little effect on pubertal timing. In living populations, declines in age at menarche have also been shown to accompany reductions in disease burdens (particularly life-threatening childhood infections), improvements in healthcare, and reduced workloads during childhood (Sohn, 2017b ; Wiley, 2011 ). Further, the negative effect of disease on pubertal timing has been demonstrated in adolescents from medieval England; Lewis, Shapland, and Watts ( 2015 ) found that people with skeletal pathologies attributable to chronic disease (eg, tuberculosis) experienced significantly delayed puberty compared to their peers. Šaffa et al.'s ( 2019 ) meta-analysis of modern women indicates that fertility and adult female mortality are significant predictors of mean age at menarche, such that menarche is delayed in countries with high fertility and high female mortality, and they argue that fertility and mortality are proxies for assessing overall environmental quality and resource availability. Another variable, which is invisible to the bioarcheologist, is the effect of psychological stress on the onset of menarche. Accumulated childhood hardships such as poverty, exposure to violence, physical neglect, and the absence or death of a parent are known to delay or accelerate menarche as part of a stress response in industrialized societies (Boynton-Jarrett & Harville, 2012 ; Dossus et al., 2012 ; Karapanou & Papadimitriou, 2010 ; Sear, Sheppard, & Coall, 2019 ), and may have been factors experienced by adolescent females throughout the medieval period.
These findings raise the question of why menarcheal age generally declines when conditions for growth are good? Ellis ( 2004 ) reviews several theories regarding the timing of pubertal development, such as the energetics theory and the stress suppression theory, concluding that “there can be little doubt that energetics play a key role in determining timing of pubertal maturation” (p. 948), as chronic poor nutrition during childhood is associated with relatively late puberty. Hochberg and Belsky ( 2013 ) concur, arguing that females are programmed to delay sexual maturation until they have reached the required body weight providing for later fecundity, fertility, and greater longevity. Under conditions of chronically low resource availability during childhood, it is presumably beneficial to reserve energy for maintenance and survival and to postpone maturation and reproduction to a time when resources are more abundant. Conversely, relatively abundant resources during childhood (ie, a healthy body weight) signal that earlier maturation and reproduction are sustainable.
Interestingly, although earlier age at menarche can reflect high standards of living and generally good health during growth (at least up to the point of menarche) in females and the population in general, early menarche (<12 years of age) is associated with a variety of poor health outcomes in adulthood, such as obesity, diabetes, cardiovascular disease, breast cancer, and all-cause mortality (Karapanou & Papadimitriou, 2010 ). Some of these associations exist because over nutrition and obesity in childhood are associated both with early onset of puberty in girls and poor health in adulthood (Barros, Kuschnir, Bloch, & Silva, 2019 ; Reinehr & Roth, 2019 ; Villamor & Jansen, 2016 ). Lee et al. (this issue) find that early pubertal timing is possibly associated with reduced bone mineral density later in life. At the other end of the spectrum, late menarche is associated with osteoporosis and elevated risk of fracture (Karapanou & Papadimitriou, 2010 ). These variable outcomes of menarcheal timing highlight the complex nature of the association between and tradeoffs among pubertal timing, growth conditions, and health outcomes later in life as well as the difficulty of defining health in general.
Age at menarche is tightly synchronized with skeletal development, more so than it is with weight or accumulation of fat (Ellison, 1982 ). Numerous studies of living populations, or using clinical or historical data from the recent past, have tested the association between age at menarche and final adult height or components of adult height, such as leg length. Some studies reveal negative associations, that is, an earlier age at menarche is associated with taller stature in females (Chun & Shin, 2018 ; do Lago, Faerstein, Sichieri, Lopes, & Werneck, 2007 ; Malina, Peña Reyes, & Little, 2008 ); in some populations, no significant association between the two is observed (Ersoy, Balkan, Gunay, Onag, & Egemen, 2004 ; Jaruratanasirikul, Chanpong, Tassanakijpanich, & Sriplung, 2014 ; Komlos, 1989 ); while in others, the observed association is positive, that is, an earlier age at menarche is associated with shorter stature in females (Gharravi, Gharravi, Marjani, Moradi, & Golalipour, 2008 ; Kang, Kim, Lee, Kim, & Lim, 2019 ; Okasha, McCarron, McEwen, & Smith, 2001 ; Onland-Moret et al., 2005 ; Petersohn, Zarate-Ortiz, Cepeda-Lopez, & Melse-Boonstra, 2019 ; Schooling et al., 2010 ). The latter pattern is observed in the current study.
These variable findings have been interpreted as reflecting changing growth conditions during the process of industrialization and modernization. When comparisons are made at the population level, the secular trend that accompanies industrialization is, as mentioned above, characterized by an earlier age of puberty and larger body size in adulthood. However, the underlying associations between pubertal timing and height within populations appear to vary based on the degree to which a population has industrialized. McIntyre and Kacerosky ( 2011 ) compared age at menarche and stature in small-scale and agrarian societies with industrialized societies; in the former, there was a negative association, and in the latter the association is positive. The finding that in small-scale and agrarian societies, earlier menarche and taller adult stature occur together is consistent with a general finding, across a wide range of species, that improved living conditions allow for optimal growth and reproduction, resulting in earlier maturation and a larger body size (McIntyre & Kacerosky, 2011 ).
Bioarchaeologists are used to the idea that delayed menarche and short stature are related, in that they both reflect poor living conditions where access to nutritional resources restricts childhood growth and maturation, ultimately resulting in short stature. The positive association between early sexual maturation and a smaller body size might seem unusual; however, it makes sense when viewed through the lens of life history theory. The relationship between stature and age at menarche is complicated by the fact that both have a genetic component. In times of rapid environmental improvement, secular changes, resulting in greater optimal growth and body weight, trigger earlier menarche at a time when greater height has been achieved (McIntyre, 2011 ; McIntyre & Kacerosky, 2011 ). Once improvements to living conditions have stabilized, advances in stature attainment are no longer discernible, while the effect of earlier menarche becomes more evident when compared to females from the same cohort with a later age at menarche. Hence, the relationship changes from a negative one to a positive one with sustained improved living conditions (McIntyre, 2011 : p. 718). This was further demonstrated by Sohn ( 2014 ) who explored this pattern in an Indonesia, a “mid-level industrial” country; Indonesia began rapidly industrializing in the 1960s, but improvements in its standard of living still fall below that of fully industrialized countries. As predicted by McIntyre and Kacerosky ( 2011 ), Sohn ( 2014 ) found that the relationship between age at menarche and height was weak.
Several studies indicate that the positive association between age of menarche and adult height is determined by lower leg length, that is, earlier puberty results in shorter final stature by reducing leg length (McIntyre, 2011 ; Onland-Moret et al., 2005 ; Said-Mohamed et al., 2018 ; Schooling et al., 2010 ). This occurs due to the cephalocaudal pattern of skeletal growth, whereby the majority of lower leg growth occurs during later childhood (Bogin & Varela-Silva, 2010 ), and it is this growth that is truncated by early puberty (McIntyre, 2011 ). We note that the direction of the association between earlier puberty and adult height can differ between the sexes within populations, and this may be related to the fact that fusion of the growth plates and cessation of long bone growth is controlled by estrogen, or more specifically estradiol (Chagin & Sávendahl, 2007 ; Juul, 2001 ). For example, Schooling et al. ( 2010 ) found that while earlier puberty was associated with taller sitting height in both sexes in China, earlier puberty was associated with longer legs in males but shorter legs in females. Studies into the role of high leptin levels on skeletal maturation associated with adolescent obesity are inconclusive, but may indicate the role of fat and body mass, energy expenditure, and nutrition in final adult stature (Klein et al., 1998 ; Rogol, Roemmich, & Clark, 2002 ; see also Tobolsky, Hollander, Capellini, Zeng, and Lieberman ( 2019 ) regarding the possible effects of insulin in regulating linear growth).
Returning to the findings of our study, the observed increase in average post-menarcheal age from the Early Pre-Black Death to Late Pre-Black Death periods may reflect the negative effects of famine in the early 13th century; that is, famines, which occurred repeatedly from the 11th to early 13th centuries (Farr, 1846 ), would have produced generally poor growth conditions, and therefore, delayed age at menarche. The observed decrease in post-menarcheal age after the Black Death may reflect improved growth conditions that resulted in earlier puberty in the post-epidemic population of London. As mentioned above, there is historical evidence that following the Black Death in England, depopulation and resulting wage increases resulted in improvements in diet for people of all status levels. In particular, people were eating more high-quality wheat bread and greater quantities of fresh meat and fish after the Black Death than they had beforehand (Dyer, 2005 ). Increased consumption of animal protein and animal fat might have resulted in an earlier age at menarche (see, eg, Wiley, 2011 ). If these, and other dietary changes improved nutritional status, this might have led to a general improvement in immunocompetence and subsequent reductions in disease burdens, which also might have resulted in earlier menarche. Economic, dietary, and other conditions following the Black Death might also have lessened psychosocial stress, which could have influenced menarcheal timing. Our study is the first, to our knowledge, to document a decline in menarcheal age within the context of improved standards of living in a population that pre-dates the 19th century.
Given that previous research (DeWitte, 2018 ) did not reveal significant changes in female stature before the Black Death, but did find significant decreases thereof after the epidemic, we focus here on the possible association between menarche and stature just in the Post Black Death population. We suggest that a decline in the average age at menarche is associated with truncated growth of the tibia and hence reduced height in females, relative to males, in the post-Black Death period. However, our data can only provide indirect evidence for this relationship, as we are unable to pinpoint the age at which menarche actually occurred in the sample. Conversely, several studies on modern long-term temporal trends in stature and menarche show a decrease in age at menarche concurrent with an increase in height over a long time period and across consecutive birth cohorts (Chun & Shin, 2018 ; Malina et al., 2008 ; Onland-Moret et al., 2005 ). However, within a shorter time frame, Okasha et al. ( 2001 ) observed a significant decline in both age at menarche and female height over time during the first half of the 20th century in Glasgow when nutritional conditions were improving; although this decline in height was not significant when the authors controlled for social status. Our data, and that of DeWitte ( 2018 ), indicate similar short-term patterns were at play in post-plague London in response to improved living environment. But, given that we cannot directly assess the exact age at menarche and stature in each of our London females, nor explore changes across birth cohorts or control for possible confounding variables such as status or psychological stress, inferences about the possible effects of changes in pubertal timing across the medieval period on achieved stature remain tentative.
There is evidence from modern nonindustrial populations similar to ours, that a positive relationship between menarche and linear growth exists, although such studies are rare compared to the much richer body of evidence from industrialized groups (eg, see McIntyre & Kacerosky, 2011 ). This bias in the literature toward modern industrialized populations is unsurprising given that pubertal timing is easier to assess in living people using recall data or via longitudinal studies. Nevertheless, Schooling et al.'s ( 2010 ) study of pubertal timing in China includes older individuals who would have developed under preindustrial conditions, and they found that an earlier age of puberty was associated with shorter leg length in adult females (but not males).
Given the myriad factors that can affect achieved adult height, it is possible that our findings do not, in fact, reflect changes in menarcheal age in medieval London as a result of improved nutritional and health conditions produced by the Black Death. It is possible that female stature declined after the Black Death because women started reproducing at earlier ages following the epidemic. Several studies have shown that maternal age at first birth is positively associated with height (eg, Helle, 2008 ; Silventoinen, Helle, Nisén, Martikainen, & Kaprio, 2013 ; Stearns, Govindaraju, Ewbank, & Byars, 2012 ; Stulp, Verhulst, Pollet, & Buunk, 2012 ). Vercellotti and Piperata ( 2012 ), for example, found among rural Amazonians that age at menarche and height were not significantly associated, but that age at first birth was significantly, positively associated with height (and, specifically, total leg length), which they view as suggesting the negative effects of adolescent pregnancy on female growth. This raises the question of whether there is evidence that average age at first birth declined after the Black Death.
One way to assess this is via trends in fertility rates and age at marriage, as age at first birth is negatively associated with the former and positively associated with the latter in many populations (Fagbamigbe & Idemudia, 2016 ; Rao & Balakrishnan, 1988 ). Some historians (as summarized by Bailey, 1996 ) have argued that fertility rates declined after the Black Death. Previous estimates, using skeletal data from medieval London, of a fertility proxy (the ratio of individuals above the age of 30 to those above the age of 5 years, which Buikstra, Konigsberg, & Bullington, 1986 found to be negatively associated with birth rates) indicated that fertility did not change significantly from early- to late Pre-Black Death period (DeWitte, 2015 ), nor from the pre- to the post-Black Death period (DeWitte, 2014b ). These findings suggest, at the very least, that women did not start reproducing at later ages before the Black Death or at earlier ages after the Black Death, as this would presumably have led to decreases and then increases in fertility before and after the epidemic, respectively. There is also evidence that in north-western Europe, the Black Death ushered in the widespread adoption of the European marriage pattern, which is characterized by a later average age at marriage and high rates of female celibacy; for example, in England in the 15th century, age at marriage for women ranged from about 18 to 23 years, and in Yorkshire, urban women did not marry until their early to mid-20s (Goldberg, 1992 ; de Moor & Van Zanden, 2010 p. 17). Further, there is some evidence that the age at marriage in England was later following the Black Death than it had been beforehand (Gilchrist, 2012 ; Kowaleski, 1998 ). Altogether, the existing evidence suggests that a decline in stature following the Black Death was not the result of a reduction in average age at first birth.
We should also consider the possible effects on our findings of migration from rural areas after the Black Death. That is, it is possible that our results are an artifact of the influx of rural women into London. Migration likely increased after Black Death (Dyer, 2005 ). In medieval England, it was common for adolescents to travel to urban centers seeking economic opportunities (Dyer, 2002 ; Hanawalt, 1995 ; Lewis, 2016 ), and there is evidence that rural-urban migrants were disproportionately female (Goldberg, 1986 ; Kowaleski, 2013 ). Several studies in modern populations have produced evidence that migrants in some contexts are healthier on average than people in their receiving populations (eg, Ng, 2011 ; Norredam et al., 2014 ). If this was also the case during the medieval period in England, it is possible that an influx of young, healthy women from rural areas, who themselves might have reached menarche at younger ages on average, produced daughters who also reached menarche relatively early. Studies in modern populations have noted precocious puberty (including early age at menarche) in migrating children, which might, among other things, reflect improved nutrition or escape from stressors (including psychological stressors) (Parent et al., 2003 ) for formerly deprived children. While this so-called “healthy migrant” effect is short-lived and tends not to persist across several generations (Norredam et al., 2014 ), increased rural migration to London as a result of changing economic circumstances after the Black Death appears to have continued at least into the early fifteenth century, which would have maintained this influx of “healthy” migrant women (McClure, 1979 ). It is possible that during the medieval period, rural females reached puberty at earlier ages and were shorter than their urban peers. If these shorter females migrated into London at higher rates after the Black Death, our findings might actually reflect medieval urban vs rural conditions rather than a more direct effect of the Black Death on pubertal timing within the context of London alone. It may also be the case that fewer of these potentially younger post-menarcheal females migrated to London just prior to the Black Death (ie, during the Late-Pre Black Death period) than they did in the Early Pre-Black Death period or after the epidemic, which would account for an increase in post-menarcheal age in the Late Pre-Black Death sample. Findings from more recent 19-20th century populations have shown that the average age at menarche is earlier in urban populations compared to their rural peers (Ellis, 2004 ; Hesketh, Ding, & Tomkins, 2002 ; Hossain, Islam, Aik, Zaman, & Lestrel, 2010 ; Kozlov & Vershubsky, 2015 ; Lehmann et al., 2010 ; Said-Mohamed et al., 2018 ; Torres-Mejía et al., 2005 ), but that daughters of poor migrant families have an earlier age at menarche than girls of similar socioeconomic status in nonmigrant groups (Gomula & Koziel, 2015 ). Urban vs rural conditions were likely very different in medieval England compared to those in modern populations, and the possible effect of rural-to-urban migration on our findings deserves further study by gathering puberty data from contemporaneous rural sites.

Section: 5 CONCLUSION

This research expands the investigation of life history tradeoffs in past populations to include pubertal timing as a variable of interest, expanding our arsenal of tools to assess this phenomenon beyond those already productively employed by bioarchaeologists (eg, LEH and stature, see Holder et al. and Ham et al.'s contributions to this issue). Though we are limited by small sample sizes, particularly for the Early Pre-Black Death period, the fact that we observed a trend suggests that the use of our approach might be informative about conditions of growth in other past contexts and with larger sample sizes. As noted above, Šaffa et al. ( 2019 ) found, across modern populations, delayed menarche in countries with high fertility and high female mortality, and therefore argue that fertility and mortality are proxies for assessing overall environmental quality and resource availability. We hope our study encourages other bioarchaeologists to incorporate analyses of variation in pubertal timing, and of the drivers and potential health outcomes of that variation, in other contexts in the past. It is possible that by incorporating the reproductive system into bioarchaeological research, our field can accomplish more robust testing of energetic tradeoffs in past populations in ways that put us into fruitful dialog with human biologists focusing on living people. In addition, as bioarchaeology has access to long periods of time, it has the potential to circumvent, or even test the problem of intergenerational inertia in life history studies.
After the Black Death in England (1348-1350 CE) bioarcheological evidence indicates that survivors and their descendants experienced a lower risk of death and better health conditions than people did prior to the epidemic. Several possible reasons for these trends have been suggested, such as greater numbers of healthy young migrants, a decline in social inequality, fewer frail individuals, and improved access to resources (DeWitte, 2014a , 2014b , 2015 , 2018 ). However, the greatest gains appear to have been in the males who show a significant increase in stature that is not reflected in the females. Shorter stature in females seems at odds with other evidence for improved living conditions in this London parish and thus begs further examination. Our findings indicate that the age of menarche, mapped indirectly through the ages of pre- and post-menarcheal adolescent females, increased in the Late Pre-Black Death period and declined after the epidemic, a trend that supports the idea of improved living conditions after the Black Death. The association between earlier menarche and reduced tibial length has been related to the cephalocaudal pattern of skeletal growth, whereby the majority of lower leg growth occurs during later childhood and becomes truncated by early puberty. In the short term, improvements in living conditions have been shown to produce a positive relationship between menarche and stature in modern studies, but this pattern seems to disappear in the longer term. While this study only provides indirect evidence for the age of menarche, and confounding variables such as social status, migration, and psychosocial stress are unaccounted for, it provides the first skeletal evidence for reduced menarche in the context of improving conditions prior the nineteenth century. This finding suggests that the improved conditions of growth that are demonstrated (or presumed) to accompany industrialization and the second epidemiological transition might not be unique to those phenomena and might exist in other contexts and earlier than previously thought. For our particular context, we note that future work integrating more data on pubertal ages for rural adolescent females who may have migrated into London is required to explore the impact of the urban-rural divide on health during this crucial period of European history.

Section: CONFLICT OF INTEREST

The authors have no conflicts of interest to declare.

Section: ACKNOWLEDGMENTS

We thank Dr Monica Green for bringing to our attention details about historical descriptions of changes in timing of menses during the medieval period. Puberty assessment of the St Mary Spital females was collected by Dr Fiona Shapland as part of the Leverhulme Trust funded Project on medieval adolescents (University of Reading). Both authors are grateful to the Museum of London for granting access to this collection for our research. We also thank Dr. Holly Dunsworth and the three anonymous reviewers whose constructive comments enhanced this article. Puberty data collection was funded by a Leverhulme Trust Large Grant for the “Medieval Adolescence, Migration and Health” Project (2011–14) Grant number: F/0 0239/AM.
