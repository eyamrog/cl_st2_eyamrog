Title: Public Health and Online Misinformation: Challenges and Recommendations


Section: 

Copyright © 2020 by Annual Reviews. This work is licensed under a Creative Commons Attribution 4.0 International License , which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. See credit lines of images or other third party material in this article for license information.


Section: ABSTRACT

The internet has become a popular resource to learn about health and to investigate one's own health condition. However, given the large amount of inaccurate information online, people can easily become misinformed. Individuals have always obtained information from outside the formal health care system, so how has the internet changed people's engagement with health information? This review explores how individuals interact with health misinformation online, whether it be through search, user-generated content, or mobile apps. We discuss whether personal access to information is helping or hindering health outcomes and how the perceived trustworthiness of the institutions communicating health has changed over time. To conclude, we propose several constructive strategies for improving the online information ecosystem. Misinformation concerning health has particularly severe consequences with regard to people's quality of life and even their risk of mortality; therefore, understanding it within today's modern context is an extremely important task.

Section: INTRODUCTION

The internet has become a popular resource to learn about health and to investigate one's own health condition. However, given the large amount of inaccurate information online, people can easily become misinformed. For example, the notion that eating apricot seeds will cure cancer is a misconception that can be found online ( 16 ). There is no scientific evidence to support the claim; in fact, it is well established that eating apricot seeds may even cause cyanide poisoning ( 124 ). Individuals have always obtained information from outside the formal health care system, and health misinformation and disinformation are not new. For instance, between 1921 and 1974, Listerine advertised that their mouthwash could cure colds and sore throats, resulting in a corrective advertising order from the Federal Trade Commission ( 4 ). Understanding how the internet has changed our engagement with health (mis)information, and whether individuals can successfully evaluate veracity, is an important task. This is because misinformation concerning health has particularly severe consequences with regard to people's quality of life and even their risk of mortality.
In recent years, the quintessential example of misinformation in public health is the misconception that the measles, mumps, rubella (MMR) vaccine causes autism, a concept popularized by a 1998 study published in The Lancet ( 127 ). The link was immediately refuted by the scientific community [for example, Taylor et al. ( 117 )], and eventually the publication itself was retracted, with the lead author being barred from practicing medicine. However, the misconception has gained substantial currency with predictable negative societal impact. In 2019, the United States saw multiple declarations of public health emergencies due to measles outbreaks ( 19 , 20 ). In Europe, the World Health Organization revoked the measles eradication status of four countries: Albania, Czechia, Greece, and the United Kingdom ( 131 ).
Poland & Spier ( 89 ) argued that this tragedy has occurred for a number of reasons: a “too little, too late” response from public health authorities, corrective information filled with scientific jargon and low-quality content, and lack of reasoning by both the press and the public, resulting in a retreat from evidence-based medicine and a step toward media- and celebrity-based medicine. The blame is not on one institution alone, and we can all make improvements to decrease the likelihood of such crises occurring and to reduce the prevalence of public health misinformation in general. This review aims to explore ( a ) how health seekers are engaging with misinformation online, ( b ) whether personal access to information is helping or hurting health outcomes, ( c ) how trustworthiness for institutions communicating health has changed over time, and ( d ) constructive strategies for improving the information ecosystem.

Section: DEFINING MISINFORMATION AND DISINFORMATION

Defining misinformation can be a complex task ( 128 ). For the purpose of this article, we define science and health misinformation as information that is contrary to the epistemic consensus of the scientific community regarding a phenomenon. By this definition, what is considered true and false is constantly changing as new evidence comes to light and as techniques and methods are advanced. To illustrate, presently we would consider the claim that thalidomide is not harmful during pregnancy to be misinformation. However, in the late 1950s, the apparent consensus was that treating morning sickness in pregnant women with thalidomide was safe ( 121 ). Although thalidomide was distributed in 46 countries and rapidly became one of the best-selling drugs in the world, it has now been described as one of the biggest man-made medical disasters of all time, where more than 10,000 children were born with severe malformations ( 122 ). Understanding consensus and taking a scientific approach to determine when a field reaches one are therefore important tasks ( 104 ).
Disinformation is a coordinated or deliberate effort to knowingly circulate misinformation in order to gain money, power, or reputation. Because public health is a field in which there are obvious winners and losers, and the losers have significant financial loss at stake, it is a venue where disinformation can thrive. While the usage of the term disinformation is somewhat mixed in the literature, we eschew the use of the term to refer to a subtype of misinformation because disinformation incorporates the notion of intentionality, which is an attribute of the people spreading the information rather than of the information itself. An example of disinformation would be when the sugar industry funded research that successfully cast doubt on the health risks of sugar (and fat was blamed as the culprit instead; 57 , 72 ). By contrast, the popular misconception that sugar causes hyperactivity in children does not have an apparent vested interest behind it, and so it can be considered misinformation ( 61 ). Unfortunately, teasing apart disinformation from misinformation can be extremely difficult, given that intent behind a message is not always transparent or constant from messenger to messenger.

Section: HOW ARE INDIVIDUALS ENGAGING WITH HEALTH (MIS)INFORMATION ONLINE?

There is an extensive literature on health information–seeking behavior, and the ways that people learn about their various illnesses, risks, and protective behaviors ( 63 ). We get our information from a multitude of places: Knowledge regarding health and well-being is cobbled together from health care professionals, family, friends, books, newspapers, magazines, educational pamphlets, radio, television, and pharmaceutical advertisements ( 129 ). However, we are increasingly heading online for answers rather than pursuing information through these other avenues ( 54 ). In 2013, 72% of US adults looked online for health information ( 34 ). Although some individuals are less likely to get health information from the internet, such as older adults and those with less education and income ( 23 ), there is no doubt that the internet has democratized medicine. The internet is often viewed as a singular entity for content, but it contains a myriad of very different platforms and functionalities. We now turn to the various pathways of misinformation online.
Some health seekers bypass search engines altogether and go straight to online domains to read information regarding health, for instance, if a person were to go directly to the online version of The New York Times . The quality of direct sources depends on whether people choose to go to reputable sites such as the Centers for Disease Control and Prevention (CDC) website or, for example, to a disreputable blog. We discuss the general quality of online sources below.
In using the term search, we include all general search engines, Google being by far the most prominent example. When Google Search started in 1998, it simply identified which pages on the internet contained words that were being searched for and presented a top list of matching websites based on PageRank ( 66 ). Search is vastly more complex today and becoming ever more so, including specialized functionality that identifies health information ( 93 ). Approximately 5% of all internet searches are health related ( 93 ), with the number of health-related searches doubling the week prior to an emergency department visit ( 5 ). Although most individuals report that search empowers their decision making regarding health issues ( 95 ), the first challenge to finding online information is often choosing the correct symptoms or diagnosis to search for in the first place .
Keselman et al. ( 59 ) investigated online health information-seeking by asking laypeople to read a hypothetical scenario regarding a relative who was experiencing stable angina symptoms (chest pain) and subsequently search the internet for information. The authors found that initially incorrect knowledge often led individuals to search for information on irrelevant websites and to seek out data that would confirm their initial incorrect hypothesis. This phenomenon is otherwise known as confirmation bias, where individuals selectively expose themselves to evidence that supports prior beliefs ( 79 ). Confirmation bias is problematic because online one can find evidence to support many different hypotheses, particularly in fields such as nutrition ( 99 ). The vast amount of information that is possible to be retrieved makes it difficult to separate fact from fiction and interpret the findings, even for highly motivated individuals.
There are many different platforms that provide an ecosystem for coproduction and consumption of content by users. These include content-rating sites such as Yelp, content-editing sites such as Wikipedia, and social media platforms such as Facebook and Twitter. Furthermore, user content is frequently embedded in many other sites, often in the form of comments. For example, WebMD and Amazon permit comments on products, and news media sites house discussion threads. Some of these platforms appear to be more permeable to misinformation than others. Although Wikipedia provides wide access to editing, individuals are required to follow a strict set of norms about what constitutes information worthy of inclusion, and a hierarchical authority structure gives various editors, who have earned trust, more authority in the editing system than others. As such, scientific articles in Wikipedia have a similar, if not lower, rate of errors as that of the Encyclopedia Britannica ( 39 , 65 ).
By contrast, Twitter's framework, including the capacity to post content, to reshare, and to reply to posts, and WhatsApp's framework, consisting of group chats, have left these platforms less resilient to misinformation threats ( 96 , 125 ). Even more concerning, sites that are generally considered credible sources of health information are vulnerable to misinformation. For example, as of September 2019, WebMD presents numerous unverified testimonials to the effectiveness of apricot seeds (otherwise known as apricot kernels) for cancer treatment, with an effectiveness rating of 4.60 on a 5-point scale ( 130 ; see Figure 1 ). Despite WebMD accurately describing apricot kernels as “likely unsafe” in the side effects tab and that it “could cause serious harm, including death” in the overview tab, the addition of an uncurated and unsupervised comments section creates a vector for misinformation testimonials. Similarly, as of September 2019, one could find positive testimonials regarding apricot kernels on Amazon.com ( https://www.amazon.com/s?k=apricot+kernels ). Perhaps greater oversight of health information on content rater systems is warranted, particularly for pages where we know health misinformation is likely to thrive.

Figure 1 Click to view


Unfortunately, misinformation on many of these platforms is not well understood, given that the data are not publicly available for researchers to analyze. Most of the current social media research relies on Twitter owing to more open data sharing by the platform, even though Facebook remains the most popular with more than 2 billion users, and YouTube and WhatsApp are not far behind ( 21 ). Furthermore, the visual nature of Instagram, YouTube, TikTok, and Pinterest creates additional difficulties for researchers. To highlight the importance of studying these platforms, Guidry et al. ( 45 ) analyzed 800 vaccine-related Pinterest posts and found that 74% were antivaccine in sentiment. Furthermore, Hawke et al. ( 48 ) found that videos marketing unproven stem cell treatments on YouTube consisted primarily of patients discussing health improvements (91%), praising providers (54%), and recommending the treatment (29%). Thus, further attention needs to be given to these platforms to know where corrective information efforts should be focused.
The proliferation of mobile health apps has largely been without oversight or regulation, and the quality of these apps is highly variable. For example, smoking cessation apps were found to rarely adhere to established medical guidelines ( 1 ). In addition, while 95% of cancer information apps aimed at health care workers contained scientifically valid information, this was true of only 32% of apps aimed at the general public ( 85 ). One example of such an app was The Whole Pantry. The app was created by Belle Gibson, a popular Australian wellness blogger who publicly revealed that she had terminal cancer. Although she reportedly tried radiation and chemotherapy, she gave up on traditional treatment and was successfully managing her cancer with diet, exercise, and alternative therapies. The app was downloaded 200,000 times within the first month, voted Apple's Best Food and Drink App of 2013 and ranked #1 in the App store ( 76 ). However, in April 2015, Belle admitted that she in fact did not have cancer and never did ( 70 ). Personal anecdotes like Belle Gibson's can have powerful sway. Below, we discuss the ramifications of the misconception that cancer can be managed by using diet and alternative therapies alone.
Some health seekers bypass search engines altogether and go straight to online domains to read information regarding health, for instance, if a person were to go directly to the online version of The New York Times . The quality of direct sources depends on whether people choose to go to reputable sites such as the Centers for Disease Control and Prevention (CDC) website or, for example, to a disreputable blog. We discuss the general quality of online sources below.
In using the term search, we include all general search engines, Google being by far the most prominent example. When Google Search started in 1998, it simply identified which pages on the internet contained words that were being searched for and presented a top list of matching websites based on PageRank ( 66 ). Search is vastly more complex today and becoming ever more so, including specialized functionality that identifies health information ( 93 ). Approximately 5% of all internet searches are health related ( 93 ), with the number of health-related searches doubling the week prior to an emergency department visit ( 5 ). Although most individuals report that search empowers their decision making regarding health issues ( 95 ), the first challenge to finding online information is often choosing the correct symptoms or diagnosis to search for in the first place .
Keselman et al. ( 59 ) investigated online health information-seeking by asking laypeople to read a hypothetical scenario regarding a relative who was experiencing stable angina symptoms (chest pain) and subsequently search the internet for information. The authors found that initially incorrect knowledge often led individuals to search for information on irrelevant websites and to seek out data that would confirm their initial incorrect hypothesis. This phenomenon is otherwise known as confirmation bias, where individuals selectively expose themselves to evidence that supports prior beliefs ( 79 ). Confirmation bias is problematic because online one can find evidence to support many different hypotheses, particularly in fields such as nutrition ( 99 ). The vast amount of information that is possible to be retrieved makes it difficult to separate fact from fiction and interpret the findings, even for highly motivated individuals.
There are many different platforms that provide an ecosystem for coproduction and consumption of content by users. These include content-rating sites such as Yelp, content-editing sites such as Wikipedia, and social media platforms such as Facebook and Twitter. Furthermore, user content is frequently embedded in many other sites, often in the form of comments. For example, WebMD and Amazon permit comments on products, and news media sites house discussion threads. Some of these platforms appear to be more permeable to misinformation than others. Although Wikipedia provides wide access to editing, individuals are required to follow a strict set of norms about what constitutes information worthy of inclusion, and a hierarchical authority structure gives various editors, who have earned trust, more authority in the editing system than others. As such, scientific articles in Wikipedia have a similar, if not lower, rate of errors as that of the Encyclopedia Britannica ( 39 , 65 ).
By contrast, Twitter's framework, including the capacity to post content, to reshare, and to reply to posts, and WhatsApp's framework, consisting of group chats, have left these platforms less resilient to misinformation threats ( 96 , 125 ). Even more concerning, sites that are generally considered credible sources of health information are vulnerable to misinformation. For example, as of September 2019, WebMD presents numerous unverified testimonials to the effectiveness of apricot seeds (otherwise known as apricot kernels) for cancer treatment, with an effectiveness rating of 4.60 on a 5-point scale ( 130 ; see Figure 1 ). Despite WebMD accurately describing apricot kernels as “likely unsafe” in the side effects tab and that it “could cause serious harm, including death” in the overview tab, the addition of an uncurated and unsupervised comments section creates a vector for misinformation testimonials. Similarly, as of September 2019, one could find positive testimonials regarding apricot kernels on Amazon.com ( https://www.amazon.com/s?k=apricot+kernels ). Perhaps greater oversight of health information on content rater systems is warranted, particularly for pages where we know health misinformation is likely to thrive.

Figure 1 Click to view


Unfortunately, misinformation on many of these platforms is not well understood, given that the data are not publicly available for researchers to analyze. Most of the current social media research relies on Twitter owing to more open data sharing by the platform, even though Facebook remains the most popular with more than 2 billion users, and YouTube and WhatsApp are not far behind ( 21 ). Furthermore, the visual nature of Instagram, YouTube, TikTok, and Pinterest creates additional difficulties for researchers. To highlight the importance of studying these platforms, Guidry et al. ( 45 ) analyzed 800 vaccine-related Pinterest posts and found that 74% were antivaccine in sentiment. Furthermore, Hawke et al. ( 48 ) found that videos marketing unproven stem cell treatments on YouTube consisted primarily of patients discussing health improvements (91%), praising providers (54%), and recommending the treatment (29%). Thus, further attention needs to be given to these platforms to know where corrective information efforts should be focused.
The proliferation of mobile health apps has largely been without oversight or regulation, and the quality of these apps is highly variable. For example, smoking cessation apps were found to rarely adhere to established medical guidelines ( 1 ). In addition, while 95% of cancer information apps aimed at health care workers contained scientifically valid information, this was true of only 32% of apps aimed at the general public ( 85 ). One example of such an app was The Whole Pantry. The app was created by Belle Gibson, a popular Australian wellness blogger who publicly revealed that she had terminal cancer. Although she reportedly tried radiation and chemotherapy, she gave up on traditional treatment and was successfully managing her cancer with diet, exercise, and alternative therapies. The app was downloaded 200,000 times within the first month, voted Apple's Best Food and Drink App of 2013 and ranked #1 in the App store ( 76 ). However, in April 2015, Belle admitted that she in fact did not have cancer and never did ( 70 ). Personal anecdotes like Belle Gibson's can have powerful sway. Below, we discuss the ramifications of the misconception that cancer can be managed by using diet and alternative therapies alone.

Section: THE SPREAD OF MISINFORMATION ON SOCIAL MEDIA

Misinformation and disinformation are introduced online by many different sources: vested interests, politicians ( 120 ), news media ( 12 ), gossip, and works of fiction ( 71 ). For a comprehensive report on origins of misinformation, see Lewandowsky et al. ( 67 ). Whereas models of contagion are becoming increasingly effective in explaining how disease spreads, we are just beginning to understand the epidemiology of misinformation. Vosoughi et al. ( 125 ) tracked 126,000 rumors spread by more than 3 million individuals on Twitter. The authors found that false information diffused significantly farther, faster, deeper, and more broadly than did the true information. Studies focusing on health misinformation have found similar outcomes. For example, misinformation about Zika was three times more likely to be shared than were verified stories on multiple social media sites, with half of the top 10 news stories regarding Zika considered to be misinformation ( 106 ; see also 102 ). Vosoughi et al. ( 125 ) posited that the reason that false information diffused farther, faster, and deeper than true information was because the content was more novel and elicited more disgust, fear, and surprise. Although this finding is consistent with psychological literature suggesting that content eliciting high-arousal emotions is more likely to be shared ( 7 , 49 , 87 ), this relationship is associative and further experimental research is required to determine causality.
It is also important to note that large-scale virality—where information rapidly spreads from person to person—is a fairly rare occurrence. Goel et al. ( 40 ) investigated the structure of how content spread on Twitter and the likelihood that it was to spread by either virality (i.e., person-to-person diffusion) versus being broadcast. A broadcast is where many people receive the information directly from the same source, and the information becomes popular simply because influential accounts share it with their audiences (for example, Taylor Swift or CNN). The authors found that popularity of the information was predicted primarily by the largest broadcast, and viral cascades were a relatively uncommon occurrence. Even internet memes that are described as spreading virally also often receive substantial media coverage. This finding suggests that individuals and corporations with large social media audiences have a greater responsibility to check that the health information they are sharing is correct. It also suggests that encouraging individuals with high follower rates to share corrective or high-quality information could be an effective strategy to reduce the spread of misinformation.
Many entities spread misinformation and disinformation online, whether it be corporations and multinationals attempting to shape the public debate owing to economic interests ( 84 ) or social media bots that amplify low-credibility sources ( 101 ). It is also important to understand who is sharing disproportionately more misinformation at an individual human level. Much of what we know about who engages with false information online currently comes from studying politics or news. Grinberg et al. ( 42 ) found that political fake news engagement was extremely concentrated on Twitter. Approximately 1% of individuals saw 80% of the fake news sources, and just 0.1% of individuals shared 80% of the fake news sources. Aside from the 0.1% “supersharers,” individuals most likely to engage with fake news were conservative leaning, highly engaged with political news, and older adults. In fact, Guess et al. ( 43 ) found that adults over the age of 65 were seven times more likely to share political fake news on Facebook than were those between 18 and 29. Future research must further investigate why certain demographics are sharing disproportionately more misinformation [see Brashier & Schacter ( 14 ) for a review regarding older adults]. Additionally, it is important to investigate whether these findings are replicated for health misinformation because the extent to which these political findings generalize is uncertain.
Another recent concern has been over misinformation echo chambers where individuals have an information diet that reinforces their worldview and extremism is exacerbated ( 108 ). Although there is a growing consensus that fears over political echo chambers have been overblown ( 44 ), health echo chambers still require further exploration. To illustrate, Getman et al. ( 37 ) found that although antivaccine content was uncommon, there was a clear separation between the vaccine-hesitant and mainstream media community, potentially indicating that these communities rarely interact with one another. Furthermore, Seymour et al. ( 100 ) investigated the interconnectedness of antifluoride activists on Facebook who lobby against fluoride. The authors found that antifluoride networks were highly interconnected, significantly more so than the social networking site overall (in line with 42 ).
Misinformation and disinformation are introduced online by many different sources: vested interests, politicians ( 120 ), news media ( 12 ), gossip, and works of fiction ( 71 ). For a comprehensive report on origins of misinformation, see Lewandowsky et al. ( 67 ). Whereas models of contagion are becoming increasingly effective in explaining how disease spreads, we are just beginning to understand the epidemiology of misinformation. Vosoughi et al. ( 125 ) tracked 126,000 rumors spread by more than 3 million individuals on Twitter. The authors found that false information diffused significantly farther, faster, deeper, and more broadly than did the true information. Studies focusing on health misinformation have found similar outcomes. For example, misinformation about Zika was three times more likely to be shared than were verified stories on multiple social media sites, with half of the top 10 news stories regarding Zika considered to be misinformation ( 106 ; see also 102 ). Vosoughi et al. ( 125 ) posited that the reason that false information diffused farther, faster, and deeper than true information was because the content was more novel and elicited more disgust, fear, and surprise. Although this finding is consistent with psychological literature suggesting that content eliciting high-arousal emotions is more likely to be shared ( 7 , 49 , 87 ), this relationship is associative and further experimental research is required to determine causality.
It is also important to note that large-scale virality—where information rapidly spreads from person to person—is a fairly rare occurrence. Goel et al. ( 40 ) investigated the structure of how content spread on Twitter and the likelihood that it was to spread by either virality (i.e., person-to-person diffusion) versus being broadcast. A broadcast is where many people receive the information directly from the same source, and the information becomes popular simply because influential accounts share it with their audiences (for example, Taylor Swift or CNN). The authors found that popularity of the information was predicted primarily by the largest broadcast, and viral cascades were a relatively uncommon occurrence. Even internet memes that are described as spreading virally also often receive substantial media coverage. This finding suggests that individuals and corporations with large social media audiences have a greater responsibility to check that the health information they are sharing is correct. It also suggests that encouraging individuals with high follower rates to share corrective or high-quality information could be an effective strategy to reduce the spread of misinformation.
Many entities spread misinformation and disinformation online, whether it be corporations and multinationals attempting to shape the public debate owing to economic interests ( 84 ) or social media bots that amplify low-credibility sources ( 101 ). It is also important to understand who is sharing disproportionately more misinformation at an individual human level. Much of what we know about who engages with false information online currently comes from studying politics or news. Grinberg et al. ( 42 ) found that political fake news engagement was extremely concentrated on Twitter. Approximately 1% of individuals saw 80% of the fake news sources, and just 0.1% of individuals shared 80% of the fake news sources. Aside from the 0.1% “supersharers,” individuals most likely to engage with fake news were conservative leaning, highly engaged with political news, and older adults. In fact, Guess et al. ( 43 ) found that adults over the age of 65 were seven times more likely to share political fake news on Facebook than were those between 18 and 29. Future research must further investigate why certain demographics are sharing disproportionately more misinformation [see Brashier & Schacter ( 14 ) for a review regarding older adults]. Additionally, it is important to investigate whether these findings are replicated for health misinformation because the extent to which these political findings generalize is uncertain.
Another recent concern has been over misinformation echo chambers where individuals have an information diet that reinforces their worldview and extremism is exacerbated ( 108 ). Although there is a growing consensus that fears over political echo chambers have been overblown ( 44 ), health echo chambers still require further exploration. To illustrate, Getman et al. ( 37 ) found that although antivaccine content was uncommon, there was a clear separation between the vaccine-hesitant and mainstream media community, potentially indicating that these communities rarely interact with one another. Furthermore, Seymour et al. ( 100 ) investigated the interconnectedness of antifluoride activists on Facebook who lobby against fluoride. The authors found that antifluoride networks were highly interconnected, significantly more so than the social networking site overall (in line with 42 ).

Section: IS PERSONAL ACCESS TO INFORMATION HELPING OR HURTING HEALTH OUTCOMES?

No longer is a patient a passive recipient of health advice but they can have an active role in consuming and evaluating health information. However, laypeople are not health experts, and there may be a cost to people having the freedom to research their own ailments. Is the ability to access one's own health information helping or hindering? Answering this question feasibly depends on three factors: ( a ) the general quality of health information online, ( b ) whether people are able to come to the correct health conclusions themselves, and ( c ) if people do not come to the correct conclusions, how much harm is it causing them?
Several meta-analyses of studies have investigated the quality of online health information. In 2002, Eysenbach et al. ( 31 ) performed a meta-analysis of 79 studies evaluating the quality of information online. Grouping together quality criteria into the categories of accuracy and completeness/comprehensiveness, the authors reported that 70% of the studies concluded that quality was a problem on the internet. Zhang et al. ( 134 ) continued this line of research, subsequently reviewing 165 articles published between 2002 and 2013. Although many studies noted an improvement in quality over time, 55% of the articles reviewed concluded that the quality of online health information was problematic. Given that quality of general health information online cannot be guaranteed, it is currently up to individuals to be discerning and critically evaluate information they read.
eHealth literacy is the ability to seek, find, and understand health information from electronic sources in order to make appropriate health decisions ( 83 ). Tools for measuring health literacy include Rapid Estimate of Adult Literacy in Medicine (REALM; 78 ), Test of Functional Health Literacy in Adults (TOFHLA; 86 ), and the eHealth Literacy Scale (eHEALS; 82 ). The most comprehensive estimate of health literacy was conducted in 2003 by the Department of National Assessment. The report stated that 36% of US adults had basic or below basic health literacy levels ( 62 ). The estimated economic drain of this low health literacy could be up to $238 billion annually ( 123 ). Individuals with low health literacy are more likely to delay or not receive health care, have more hospitalizations, have poorer overall health status, and have higher mortality rates ( 9 ). However, it seems that the vast majority of people, not only those with basic health literacy, use low-quality websites when looking for health information. Quinn et al. ( 91 ) asked participants to search for six common health questions and monitored whether the participants went to accredited sites or unaccredited sites such as blogs. They found that 96% of individuals used an unaccredited source for at least one question.
Even if health seekers are able to tease apart reputable from disreputable sources, they may not engage with high-quality information if the low-quality information is easier to understand or more engaging. For example, Loeb et al. ( 69 ) found a negative correlation between scientific quality and viewer engagement for information regarding prostate cancer on YouTube. In other words, as scientific quality decreased, engagement (such as views and likes) increased. Perhaps this is an indication that creators of high-scientific-quality content also need to consider how to make their educational information easier to understand and more captivating. One example of an engaging public health campaign that was highly effective at changing attitude and behavior was the Australian SunSmart “Slip! Slop! Slap!” campaign, which began in the 1980s. An animated seagull that provided the simple message to “slip” on protective clothing, “slop” on sunscreen, and “slap” on a hat ultimately helped to reverse the trends of increasing skin cancer incidence and morbidity ( 53 , 77 ). The challenge will be to bring similar campaigns into the social media age.
Crocco et al. ( 24 ) conducted a systematic review to evaluate the number of reported cases of harm associated with the use of health information on the internet. Of the 1,512 abstracts and 186 papers fully reviewed, only three articles reported cases of actual harm: a case where three dogs were accidentally poisoned, an individual who had kidney and liver failure after self-medicating for cancer, and an individual who experienced emotional distress after reading misinformation regarding fetal irregularities. In addition, a Pew Research Center report found that just 3% of people reported being harmed, or reported knowing someone who has been harmed, by information found online ( 34 ). On the one hand, it could be that individuals are not being noticeably harmed by information they find online. However, it could also be that people do not remember where they learned the information or do not consider the information to be inaccurate or causing harm.
The true proportion of harm is likely to be higher simply due to the reported rates of people adhering to unofficial medical advice. For example, take the misconception that alternative medicines alone can cure cancer. Approximately 39% of the population will be diagnosed with cancer during their lifetime ( 80 ). Furthermore, 39% of people in the United States believe that alternative medicine such as dieting, herbs, and vitamins can cure cancer without the use of standard cancer treatments ( 6 ). This percentage is extremely problematic given that there is an increased risk of mortality for people who use alternative cancer therapies in lieu of traditional treatment, even when controlling for cancer severity. Johnson et al. ( 56 ) found that the overall hazard ratio after a 5-year period was 2.50. In other words, on average, choosing alternative medicines alone was associated with more than double the risk of death. In subgroups with lung, colorectal, and breast cancers, the hazard ratio was 2.17, 4.57, and 5.68, respectively. See Figure 2 for the overall survival of colorectal cancer patients receiving alternative medicine versus conventional cancer treatment. The people who were more likely to use alternative medicines were younger, female, more educated, and had a higher income ( 56 ). It is therefore clear that we need better ways of measuring the real impact of misinformation online that do not rely on self-report alone.

Figure 2 Click to view


Several meta-analyses of studies have investigated the quality of online health information. In 2002, Eysenbach et al. ( 31 ) performed a meta-analysis of 79 studies evaluating the quality of information online. Grouping together quality criteria into the categories of accuracy and completeness/comprehensiveness, the authors reported that 70% of the studies concluded that quality was a problem on the internet. Zhang et al. ( 134 ) continued this line of research, subsequently reviewing 165 articles published between 2002 and 2013. Although many studies noted an improvement in quality over time, 55% of the articles reviewed concluded that the quality of online health information was problematic. Given that quality of general health information online cannot be guaranteed, it is currently up to individuals to be discerning and critically evaluate information they read.
eHealth literacy is the ability to seek, find, and understand health information from electronic sources in order to make appropriate health decisions ( 83 ). Tools for measuring health literacy include Rapid Estimate of Adult Literacy in Medicine (REALM; 78 ), Test of Functional Health Literacy in Adults (TOFHLA; 86 ), and the eHealth Literacy Scale (eHEALS; 82 ). The most comprehensive estimate of health literacy was conducted in 2003 by the Department of National Assessment. The report stated that 36% of US adults had basic or below basic health literacy levels ( 62 ). The estimated economic drain of this low health literacy could be up to $238 billion annually ( 123 ). Individuals with low health literacy are more likely to delay or not receive health care, have more hospitalizations, have poorer overall health status, and have higher mortality rates ( 9 ). However, it seems that the vast majority of people, not only those with basic health literacy, use low-quality websites when looking for health information. Quinn et al. ( 91 ) asked participants to search for six common health questions and monitored whether the participants went to accredited sites or unaccredited sites such as blogs. They found that 96% of individuals used an unaccredited source for at least one question.
Even if health seekers are able to tease apart reputable from disreputable sources, they may not engage with high-quality information if the low-quality information is easier to understand or more engaging. For example, Loeb et al. ( 69 ) found a negative correlation between scientific quality and viewer engagement for information regarding prostate cancer on YouTube. In other words, as scientific quality decreased, engagement (such as views and likes) increased. Perhaps this is an indication that creators of high-scientific-quality content also need to consider how to make their educational information easier to understand and more captivating. One example of an engaging public health campaign that was highly effective at changing attitude and behavior was the Australian SunSmart “Slip! Slop! Slap!” campaign, which began in the 1980s. An animated seagull that provided the simple message to “slip” on protective clothing, “slop” on sunscreen, and “slap” on a hat ultimately helped to reverse the trends of increasing skin cancer incidence and morbidity ( 53 , 77 ). The challenge will be to bring similar campaigns into the social media age.
Crocco et al. ( 24 ) conducted a systematic review to evaluate the number of reported cases of harm associated with the use of health information on the internet. Of the 1,512 abstracts and 186 papers fully reviewed, only three articles reported cases of actual harm: a case where three dogs were accidentally poisoned, an individual who had kidney and liver failure after self-medicating for cancer, and an individual who experienced emotional distress after reading misinformation regarding fetal irregularities. In addition, a Pew Research Center report found that just 3% of people reported being harmed, or reported knowing someone who has been harmed, by information found online ( 34 ). On the one hand, it could be that individuals are not being noticeably harmed by information they find online. However, it could also be that people do not remember where they learned the information or do not consider the information to be inaccurate or causing harm.
The true proportion of harm is likely to be higher simply due to the reported rates of people adhering to unofficial medical advice. For example, take the misconception that alternative medicines alone can cure cancer. Approximately 39% of the population will be diagnosed with cancer during their lifetime ( 80 ). Furthermore, 39% of people in the United States believe that alternative medicine such as dieting, herbs, and vitamins can cure cancer without the use of standard cancer treatments ( 6 ). This percentage is extremely problematic given that there is an increased risk of mortality for people who use alternative cancer therapies in lieu of traditional treatment, even when controlling for cancer severity. Johnson et al. ( 56 ) found that the overall hazard ratio after a 5-year period was 2.50. In other words, on average, choosing alternative medicines alone was associated with more than double the risk of death. In subgroups with lung, colorectal, and breast cancers, the hazard ratio was 2.17, 4.57, and 5.68, respectively. See Figure 2 for the overall survival of colorectal cancer patients receiving alternative medicine versus conventional cancer treatment. The people who were more likely to use alternative medicines were younger, female, more educated, and had a higher income ( 56 ). It is therefore clear that we need better ways of measuring the real impact of misinformation online that do not rely on self-report alone.

Figure 2 Click to view



Section: TRUSTWORTHINESS IN INSTITUTIONS THAT PROVIDE HEALTH INFORMATION

Source credibility is often considered to be made up of two components: expertise and trustworthiness. Whereas expertise is the extent to which the source is able to give accurate information, trustworthiness reflects the extent that one is willing to provide accurate information ( 90 ). When it comes to persuasion and the correction of misinformation, perceived trustworthiness is more important than expertise ( 25 , 73 ). Thus, the pervasive loss of trust in the institutions that provide health information has long been a growing topic of concern. We have focused on the institutions most relevant to the communication of health information: the media, science, governmental bodies, and health professionals.
The media continues to be an integral source of information on health ( 32 ). However, trust in the media has dramatically decreased over time. In 1972, when Gallup started its poll, 68% of people reported that they had either a “great deal” or a “fair amount” of trust that the US mass media was reporting the news fully, accurately, and fairly ( 110 ). In 2016, this percentage sank to a new low of 35% ( 110 ). Worldwide, media is now one of the least trusted institutions, on par only with government ( 31 ). Needless to say, the media ecosystem has changed significantly; whereas in 1972 the typical individual had access to only a handful of media sources, today the media represents an eclectic array of outlets.
While the decrease in public trust in the media should not be taken lightly, how much trust does the institution of media deserve as a whole? If the perception of quality overall has decreased, so should perceived trust. Ideally, trust should remain high for quality media and decrease for low-quality media. A cause for concern is when public trust in both reputable and disreputable media sources decreases simultaneously and individuals struggle to distinguish between the two. Indeed, in 2018, 59% of people reported that it was becoming harder to tell if a piece of news was produced by a respected media organization ( 29 ). Perhaps we should be less concerned with the overall decrease in trust in the media and more concerned about the inability to discern and place trust in sources that provide evidence-based health information.
When discerning between high- and low-quality sources, it is tempting to idealize the media ecosystem prior to the internet and assume that traditional news media is always more accurate. However, this is not necessarily the case. Cooper et al. ( 22 ) examined dietary advice from the top ten selling newspapers in the United Kingdom over the course of a week. They found that misreporting of advice was widespread, and up to 72% had insufficient evidence to justify the health claim being made. Ideally, there would be more sources that consistently provide high-quality reporting on health issues, regardless of the medium.
Public confidence in science has remained more or less stable since the 1970s ( 36 ). Figure 3 demonstrates how US adults rate their confidence in the scientific community as a whole. Trust in medical scientists appears to be even greater than in scientists in general; 84% of individuals report having confidence that medical scientists will act in the best interest of the public, compared with 76% reporting having confidence in scientists in general ( 58 ). Furthermore, people report trusting scientists to provide scientific information far more than other institutions. When asked how much people trusted medical scientists to give full and accurate information on the health benefits of the MMR vaccine, 55% said “a lot.” This percentage was substantially greater than for pharmaceutical industry leaders, holistic health groups, news media, and elected officials at 13%, 9%, 8%, and 6%, respectively ( 36 ). Science generally has broad public support, although there is a partisan divide. The majority of Democrats trust science “a lot,” whereas the majority of Republicans trust science “a little” ( 36 ).

Figure 3 Click to view


Trust in the government is important because people with high governmental trust are more likely to be vaccinated ( 92 ), use health care services, adhere to medication instructions ( 60 ), and take disease precautions during epidemics ( 11 ). Although trust in government and political leaders is low worldwide ( 29 ), this is not necessarily mirrored in the bodies that provide health information, such as the CDC. For example, in 2015, only 19% of Americans reported that they trusted the federal government, yet 70% reported that they viewed the CDC favorably ( 88 ). Furthermore, experimental evidence indicates that official bodies such as the CDC can be extremely effective in reducing health misconceptions when they provide corrective information on social media ( 126 ). This experimental finding aligns with real-world examples; for instance, one tweet from Tokyo city hall significantly reduced the rumor that there would be chemical rain after an earthquake ( 115 ; see also 103 ).
While trust in the institution of medicine seems to have slowly declined since the 1970s ( 36 ), health professionals as individuals seem to be at the top of nearly all scales for public trustworthiness. For example, in 2018, the top three professions in the Gallup poll for honesty and ethics were nurses, medical doctors, and pharmacists ( 15 ). In fact, nurses were rated the highest for a seventeenth consecutive year, where 84% of people rated nurses’ honesty and ethical standards as high or very high ( 15 ). Despite some health practitioners feeling that they have suffered a blow to respect and social status ( 68 ), trust in physicians remains high even though the internet has allowed patients to take their health care into their own hands ( 47 ).
The media continues to be an integral source of information on health ( 32 ). However, trust in the media has dramatically decreased over time. In 1972, when Gallup started its poll, 68% of people reported that they had either a “great deal” or a “fair amount” of trust that the US mass media was reporting the news fully, accurately, and fairly ( 110 ). In 2016, this percentage sank to a new low of 35% ( 110 ). Worldwide, media is now one of the least trusted institutions, on par only with government ( 31 ). Needless to say, the media ecosystem has changed significantly; whereas in 1972 the typical individual had access to only a handful of media sources, today the media represents an eclectic array of outlets.
While the decrease in public trust in the media should not be taken lightly, how much trust does the institution of media deserve as a whole? If the perception of quality overall has decreased, so should perceived trust. Ideally, trust should remain high for quality media and decrease for low-quality media. A cause for concern is when public trust in both reputable and disreputable media sources decreases simultaneously and individuals struggle to distinguish between the two. Indeed, in 2018, 59% of people reported that it was becoming harder to tell if a piece of news was produced by a respected media organization ( 29 ). Perhaps we should be less concerned with the overall decrease in trust in the media and more concerned about the inability to discern and place trust in sources that provide evidence-based health information.
When discerning between high- and low-quality sources, it is tempting to idealize the media ecosystem prior to the internet and assume that traditional news media is always more accurate. However, this is not necessarily the case. Cooper et al. ( 22 ) examined dietary advice from the top ten selling newspapers in the United Kingdom over the course of a week. They found that misreporting of advice was widespread, and up to 72% had insufficient evidence to justify the health claim being made. Ideally, there would be more sources that consistently provide high-quality reporting on health issues, regardless of the medium.
Public confidence in science has remained more or less stable since the 1970s ( 36 ). Figure 3 demonstrates how US adults rate their confidence in the scientific community as a whole. Trust in medical scientists appears to be even greater than in scientists in general; 84% of individuals report having confidence that medical scientists will act in the best interest of the public, compared with 76% reporting having confidence in scientists in general ( 58 ). Furthermore, people report trusting scientists to provide scientific information far more than other institutions. When asked how much people trusted medical scientists to give full and accurate information on the health benefits of the MMR vaccine, 55% said “a lot.” This percentage was substantially greater than for pharmaceutical industry leaders, holistic health groups, news media, and elected officials at 13%, 9%, 8%, and 6%, respectively ( 36 ). Science generally has broad public support, although there is a partisan divide. The majority of Democrats trust science “a lot,” whereas the majority of Republicans trust science “a little” ( 36 ).

Figure 3 Click to view


Trust in the government is important because people with high governmental trust are more likely to be vaccinated ( 92 ), use health care services, adhere to medication instructions ( 60 ), and take disease precautions during epidemics ( 11 ). Although trust in government and political leaders is low worldwide ( 29 ), this is not necessarily mirrored in the bodies that provide health information, such as the CDC. For example, in 2015, only 19% of Americans reported that they trusted the federal government, yet 70% reported that they viewed the CDC favorably ( 88 ). Furthermore, experimental evidence indicates that official bodies such as the CDC can be extremely effective in reducing health misconceptions when they provide corrective information on social media ( 126 ). This experimental finding aligns with real-world examples; for instance, one tweet from Tokyo city hall significantly reduced the rumor that there would be chemical rain after an earthquake ( 115 ; see also 103 ).
While trust in the institution of medicine seems to have slowly declined since the 1970s ( 36 ), health professionals as individuals seem to be at the top of nearly all scales for public trustworthiness. For example, in 2018, the top three professions in the Gallup poll for honesty and ethics were nurses, medical doctors, and pharmacists ( 15 ). In fact, nurses were rated the highest for a seventeenth consecutive year, where 84% of people rated nurses’ honesty and ethical standards as high or very high ( 15 ). Despite some health practitioners feeling that they have suffered a blow to respect and social status ( 68 ), trust in physicians remains high even though the internet has allowed patients to take their health care into their own hands ( 47 ).

Section: TACKLING HEALTH MISINFORMATION

Science on the effectiveness of interventions regarding health misinformation is sparse. Here we discuss several approaches based on the available research, though they require further examination prior to making broader policy recommendations. We propose ( a ) improving ehealth literacy, ( b ) using the internet as a collaborative tool with physicians, ( c ) strengthening the signal of source quality online, ( d ) increasing accuracy of information from health communicators, ( e ) increasing the frequency of corrections, and ( f ) taking advantage of technological advances.
Evidence suggests that critical thinking is a skill that can be taught ( 3 , 75 , 98 ), and new resources to teach ehealth and media literacy are becoming increasingly available [for example, the News Literacy Project ( 105 ) and the Center for Media Literacy's MediaLit Kit ( 119 )]. However, gauging the efficacy of health literacy programs is extremely difficult, and findings have been mixed ( 8 , 17 , 97 ). One meta-analysis evaluated the efficacy of enhancing students’ skills to critically appraise health claims ( 81 ). The authors found that while there were beneficial short-term effects on appraisal abilities, none of the studies evaluated any long-term effects of interventions. Furthermore, if it is older adults who are spreading most of the online misinformation ( 42 , 43 ), then health literacy classes in schools will have limited efficacy to improve the online information ecosystem in the near future. It may be necessary to study the efficacy of public health campaigns for the general public or that specifically target older adults.
Although laypeople may not always have the expertise to separate health myths from facts, the internet can be an extremely powerful tool when individuals collaborate with their physicians. A meta-analysis showed that online information seeking had the potential to help patients be more actively involved in decision making, prepare for their doctor's visit, aid communication, and improve the patient–doctor relationship ( 116 ). Doctors and nurses themselves suffer from a whole host of biases that impact decision making ( 13 ), are extremely busy, and thus can feasibly be assisted by an individual who is motivated to learn about their own health. Although health practitioners have the potential to feel threatened, online health information seeking is generally seen as a way to have a more collaborative relationship with patients ( 74 , 107 ).
Although there is evidence that trustworthiness is more influential than expertise when correcting misinformation, expertise is still an important heuristic when evaluating veracity ( 111 ). If an individual finds the source credible, they are more likely to believe that the information is true. Because of this, people with medical credentials who stoke unfounded fears are among the most dangerous for spreading misinformation ( 64 ) [for example, the lead author of the study suggesting the MMR–autism link ( 129 )]. Similarly, those who claim to be experts by either fabricating a degree or buying one online can be particularly impactful when spreading misinformation ( 41 ). Anyone can assert that they have a doctoral degree or claim to be a medical expert on the internet, and so it would be beneficial to explore how often this takes place. Henle et al. ( 51 ) found that even on job resumes, 72% of job seekers embellished or exaggerated information, 61% omit information such as being fired, and 31% outright fabricated information such as listing credentials or degrees that were never earned. Developing online systems hosted by universities to allow for easy checks of earned credentials could help solve this problem. In addition, health mobile apps could require the creator's credentials such as university affiliations or previous training, as well as the literature or data that support their recommendations. At a time when people are confused about who can provide quality information, it would be helpful to give users a clearer signal for who has earned expertise.
Of course, some individuals will trust nonexperts over experts. For example, Jenny McCarthy, one of the faces of the antivaxx movement, is known as an actress and has never claimed to be a medical expert. Some comfort is that these individuals appear to be in the minority; only 2% of parents reportedly trust celebrities “a lot” for vaccine safety information, compared with 76% who do “not trust at all” ( 35 ). In addition, a 2019 study showed that factual tweets regarding cervical cancer were shared more frequently than personal anecdote tweets ( 133 ). Nonetheless, it is important to further our understanding of the mechanisms behind trust in celebrity health advice and the power of the personal anecdote to be able to better educate individuals using evidence-based methods (see 52 ).
Ideally, scientists would create quality information, and the media would communicate it accurately to the public. Unfortunately, the peer-review process does not always guarantee high-quality science. The MMR vaccine misconception is a salient reminder of the high stakes and potential consequences in a field such as public health. There have been some movements toward change, and the replication crisis has been a positive jolt to the life sciences ( 83 ). However, even when quality research is produced, health communicators should be careful not to overstate causal inference between an intervention and a health outcome. Haber et al. ( 46 ) found that 34% of academic studies and 48% of media articles used language that was too strong for their strength of causal inference.
Scientists can also have an impact by publishing in open access journals, being more involved on social media platforms to communicate with the public, and directly contributing to information online. For example, Wikipedia is often at the top of health online searches, and there have long been calls to action for scientists to edit Wikipedia articles [for example, Heilman et al. ( 50 )]. Furthermore, scientists and the media can collaborate more closely. It is often reported that the relationship between scientists and the media is somewhat fraught, where scientists believe media reports are inaccurate and journalists believe scientists lack the communication skills to relay information to the public ( 10 ). It is important for journalists to both assist scientists in presenting information in ways that are accessible for laypeople and also allow scientists to review articles prior to publication to minimize errors. For advice on how to clearly communicate statistics of health risk, see Gigerenzer et al. ( 38 ).
We are still learning how to minimize the continued influence effect of misinformation, where misinformation continues to influence reasoning even after a correction has been presented ( 67 ). However, on the whole, people are actually quite good at reducing their belief in misinformation when faced with a clear evidence-based correction ( 111 , 113 ). Where once it was a common concern that retractions may backfire and people may believe even more in the misinformation after the correction is presented, recent research has found this phenomenon to be rare ( 111 , 113 , 132 ). Therefore, all health communicators—the media, scientists, governmental bodies, and health practitioners—should be eliciting corrective information. Particularly during breaking news and disasters, governmental agencies can successfully use social media to spread truthful information and dispel misinformation ( 28 ).
We highlight several practical recommendations for effectively correcting misinformation, given our understanding of cognitive psychology. For instance, providing factual alternatives helps to switch out the incorrect information with correct information [i.e., “Gas cylinders did not start the fire; it was arson” ( 55 )]. Furthermore, repetition of corrections also appears to be helpful for reducing the continued influence effect ( 26 , 27 ). For further information, see Swire & Ecker ( 112 ).
Advances in technology can also be part of the solution. For instance, aids can help individuals sort reputable from disreputable websites, such as NewsGuard, a browser extension that provides a green–red signal to indicate whether a website adheres to basic standards of credibility and transparency ( 33 ). In addition, other tools can communicate health advice in real time. In response to a rapid decline in human papillomavirus (HPV) vaccine uptake, Danish public health officials created a Facebook page where professionals answered parents’ questions in a timely manner ( 109 ). These technologies can be particularly useful for rural communities. For example, GiftedMom, a text-messaging app, gives women in communities across Cameroon free health advice from doctors ( 118 ). Avenues where fast, affordable health advice is readily available from experts can only be beneficial for reducing misinformation.
Misinformation in public health is still an emerging field, and many unanswered questions remain. For example, are people more or less misinformed than prior to the internet? We must also be cognizant that much of the research has been performed with political misinformation rather than health misinformation, and most of the research was conducted within the United States. Given that belief in misinformation and the way it is processed depend on the sociocultural context (see 2 , 114 ), it is important to study misconceptions outside of the United States. One destructive example of a misconception that prevails in sub-Saharan Africa is that albinos’ body parts bring good luck and wealth ( 94 ), which has led to an estimated 75 deaths in Tanzania alone between 2000 and 2016 ( 30 ). Studying misinformation internationally would provide more generalizable insights into public health misinformation. Finally, a large step forward would be if platforms such as Google conducted randomized controlled trials on interventions. For instance, if they were to experiment with how information is presented or how expertise and trust are signaled to the public, they could develop better systems to help individuals tease apart reputable from disreputable health sources.
Evidence suggests that critical thinking is a skill that can be taught ( 3 , 75 , 98 ), and new resources to teach ehealth and media literacy are becoming increasingly available [for example, the News Literacy Project ( 105 ) and the Center for Media Literacy's MediaLit Kit ( 119 )]. However, gauging the efficacy of health literacy programs is extremely difficult, and findings have been mixed ( 8 , 17 , 97 ). One meta-analysis evaluated the efficacy of enhancing students’ skills to critically appraise health claims ( 81 ). The authors found that while there were beneficial short-term effects on appraisal abilities, none of the studies evaluated any long-term effects of interventions. Furthermore, if it is older adults who are spreading most of the online misinformation ( 42 , 43 ), then health literacy classes in schools will have limited efficacy to improve the online information ecosystem in the near future. It may be necessary to study the efficacy of public health campaigns for the general public or that specifically target older adults.
Although laypeople may not always have the expertise to separate health myths from facts, the internet can be an extremely powerful tool when individuals collaborate with their physicians. A meta-analysis showed that online information seeking had the potential to help patients be more actively involved in decision making, prepare for their doctor's visit, aid communication, and improve the patient–doctor relationship ( 116 ). Doctors and nurses themselves suffer from a whole host of biases that impact decision making ( 13 ), are extremely busy, and thus can feasibly be assisted by an individual who is motivated to learn about their own health. Although health practitioners have the potential to feel threatened, online health information seeking is generally seen as a way to have a more collaborative relationship with patients ( 74 , 107 ).
Although there is evidence that trustworthiness is more influential than expertise when correcting misinformation, expertise is still an important heuristic when evaluating veracity ( 111 ). If an individual finds the source credible, they are more likely to believe that the information is true. Because of this, people with medical credentials who stoke unfounded fears are among the most dangerous for spreading misinformation ( 64 ) [for example, the lead author of the study suggesting the MMR–autism link ( 129 )]. Similarly, those who claim to be experts by either fabricating a degree or buying one online can be particularly impactful when spreading misinformation ( 41 ). Anyone can assert that they have a doctoral degree or claim to be a medical expert on the internet, and so it would be beneficial to explore how often this takes place. Henle et al. ( 51 ) found that even on job resumes, 72% of job seekers embellished or exaggerated information, 61% omit information such as being fired, and 31% outright fabricated information such as listing credentials or degrees that were never earned. Developing online systems hosted by universities to allow for easy checks of earned credentials could help solve this problem. In addition, health mobile apps could require the creator's credentials such as university affiliations or previous training, as well as the literature or data that support their recommendations. At a time when people are confused about who can provide quality information, it would be helpful to give users a clearer signal for who has earned expertise.
Of course, some individuals will trust nonexperts over experts. For example, Jenny McCarthy, one of the faces of the antivaxx movement, is known as an actress and has never claimed to be a medical expert. Some comfort is that these individuals appear to be in the minority; only 2% of parents reportedly trust celebrities “a lot” for vaccine safety information, compared with 76% who do “not trust at all” ( 35 ). In addition, a 2019 study showed that factual tweets regarding cervical cancer were shared more frequently than personal anecdote tweets ( 133 ). Nonetheless, it is important to further our understanding of the mechanisms behind trust in celebrity health advice and the power of the personal anecdote to be able to better educate individuals using evidence-based methods (see 52 ).
Ideally, scientists would create quality information, and the media would communicate it accurately to the public. Unfortunately, the peer-review process does not always guarantee high-quality science. The MMR vaccine misconception is a salient reminder of the high stakes and potential consequences in a field such as public health. There have been some movements toward change, and the replication crisis has been a positive jolt to the life sciences ( 83 ). However, even when quality research is produced, health communicators should be careful not to overstate causal inference between an intervention and a health outcome. Haber et al. ( 46 ) found that 34% of academic studies and 48% of media articles used language that was too strong for their strength of causal inference.
Scientists can also have an impact by publishing in open access journals, being more involved on social media platforms to communicate with the public, and directly contributing to information online. For example, Wikipedia is often at the top of health online searches, and there have long been calls to action for scientists to edit Wikipedia articles [for example, Heilman et al. ( 50 )]. Furthermore, scientists and the media can collaborate more closely. It is often reported that the relationship between scientists and the media is somewhat fraught, where scientists believe media reports are inaccurate and journalists believe scientists lack the communication skills to relay information to the public ( 10 ). It is important for journalists to both assist scientists in presenting information in ways that are accessible for laypeople and also allow scientists to review articles prior to publication to minimize errors. For advice on how to clearly communicate statistics of health risk, see Gigerenzer et al. ( 38 ).
We are still learning how to minimize the continued influence effect of misinformation, where misinformation continues to influence reasoning even after a correction has been presented ( 67 ). However, on the whole, people are actually quite good at reducing their belief in misinformation when faced with a clear evidence-based correction ( 111 , 113 ). Where once it was a common concern that retractions may backfire and people may believe even more in the misinformation after the correction is presented, recent research has found this phenomenon to be rare ( 111 , 113 , 132 ). Therefore, all health communicators—the media, scientists, governmental bodies, and health practitioners—should be eliciting corrective information. Particularly during breaking news and disasters, governmental agencies can successfully use social media to spread truthful information and dispel misinformation ( 28 ).
We highlight several practical recommendations for effectively correcting misinformation, given our understanding of cognitive psychology. For instance, providing factual alternatives helps to switch out the incorrect information with correct information [i.e., “Gas cylinders did not start the fire; it was arson” ( 55 )]. Furthermore, repetition of corrections also appears to be helpful for reducing the continued influence effect ( 26 , 27 ). For further information, see Swire & Ecker ( 112 ).
Advances in technology can also be part of the solution. For instance, aids can help individuals sort reputable from disreputable websites, such as NewsGuard, a browser extension that provides a green–red signal to indicate whether a website adheres to basic standards of credibility and transparency ( 33 ). In addition, other tools can communicate health advice in real time. In response to a rapid decline in human papillomavirus (HPV) vaccine uptake, Danish public health officials created a Facebook page where professionals answered parents’ questions in a timely manner ( 109 ). These technologies can be particularly useful for rural communities. For example, GiftedMom, a text-messaging app, gives women in communities across Cameroon free health advice from doctors ( 118 ). Avenues where fast, affordable health advice is readily available from experts can only be beneficial for reducing misinformation.
Misinformation in public health is still an emerging field, and many unanswered questions remain. For example, are people more or less misinformed than prior to the internet? We must also be cognizant that much of the research has been performed with political misinformation rather than health misinformation, and most of the research was conducted within the United States. Given that belief in misinformation and the way it is processed depend on the sociocultural context (see 2 , 114 ), it is important to study misconceptions outside of the United States. One destructive example of a misconception that prevails in sub-Saharan Africa is that albinos’ body parts bring good luck and wealth ( 94 ), which has led to an estimated 75 deaths in Tanzania alone between 2000 and 2016 ( 30 ). Studying misinformation internationally would provide more generalizable insights into public health misinformation. Finally, a large step forward would be if platforms such as Google conducted randomized controlled trials on interventions. For instance, if they were to experiment with how information is presented or how expertise and trust are signaled to the public, they could develop better systems to help individuals tease apart reputable from disreputable health sources.

Section: CONCLUSION

In general, we do not have the cognitive capacity, motivation, or time to evaluate all the information that we encounter online. However, motivation is increased when we are to research a topic regarding our own health condition or symptoms. Even under these circumstances, the assessment of source reputability and the veracity of information is an extremely difficult task. Additionally, the internet is a fluid, ever-changing system, making the study of health misinformation online even more complex. A limitation of our review is that this space can change rapidly . As researchers, we must attempt to find robust solutions that function even when the system is dynamic. The recommendations above can serve as guidelines, but further research that can inform the development of policy is desperately needed ( 18 ). All health communicators must work together to keep misinformation at bay, given that the ramifications of health misinformation can be particularly serious.

Section: disclosure statement

The authors are not aware of any affiliations, memberships, or financial holdings that might be perceived as affecting the objectivity of this review. Financial support was provided to D.L. from the Hewlett Packard Foundation.

Section: acknowledgments

We thank Stefan McCabe for comments on the review and for adapting Figure 3 .

Section: literature cited

