Title: The Googlization of Health: Invasiveness and corporate responsibility in media discourses on Facebook's algorithmic programme for suicide prevention


Abstract: Abstract

Big tech companies increasingly play a role in the domain of health. Also called the “Googlization of Health”, this phenomenon is often studied by drawing on the notion of ‘hostile worlds’, where market values and common goods are incommensurable. Yet, the ‘hostile worlds’ theory is not uncontested; scholars for instance argue that the justifications of big tech companies are important analytical considerations as well. Building on this literature, in this paper I report on a case study of Facebook employing AI for suicide prevention, moving beyond Facebook's justifications only to study the ways in which media commentators and their audiences discussed Facebook's programme and the values they saw as being at stake. In the results, I show how invasiveness was, in different ways and forms, a main theme in thinking about Facebook using AI to do suicide prevention. Commentators and readers alike discussed how: 1) Facebook takes corporate responsibility with this initiative, or alternatively Facebook only has commercial interests and uses the notion of ‘public good’ to transgress spheres and sectors even further, thus being invasive; 2) Facebook's AI suicide prevention programme is invasive in relation to privacy and privacy laws, or, instead, people give up their privacy willingly in exchange for entertainment; 3) The programme undermines, rather than enhances, safety; 4) Suicide prevention in itself is already invasive. These different forms of invasiveness, I argue in the conclusion, also imply responsibility for different actors, from AI itself to Facebook through to medical professionals. Moreover, they show what values are at stake in, and transformed through, Facebook's AI suicide prevention programme, going beyond the frames of privacy and surveillance capitalism .

Section: 1. Introduction

At the end of 2017, Facebook started to use AI to detect suicide risk among its users. Referred to by Facebook as a “proactive upgrade”, the use of algorithms for suicide prevention by Facebook was a scaling up of an already existing programme that used human moderators and relied on reporting by Facebook friends to detect potential suicidality. In part, the upgrade has been considered as a response to the livestreaming of suicides on Facebook's platform, although as of yet the algorithm cannot analyze videos themselves, only the comments posted in response to videos as well as general posts by users. The programme, which has been developed in collaboration with suicide prevention organizations, uses natural language processing to determine risk. Once a post is flagged by the AI system, it is then sent to a team of human moderators who decide whether the risk is real, and what constitutes the best response, from sending the person deemed at risk resources and ways they can reach out through to a so-called ‘wellness check’ by the police. In an internal evaluation of the programme, according to the NPR, Facebook mentioned that 3500 wellness checks had been done in the first year it ran ( Facebook Increasingly Reliant on A.I. To Predict Suicide Risk: NPR ). For Facebook these wellness checks constituted part of the effectiveness of the programme, although official numbers about how many people were prevented from actually taking their own lives are, to my knowledge, lacking.
Facebook's programme is presented as one way to address the complexity of suicide and predicting suicide risk. In particular, suicide is notoriously difficult to predict, with even psychiatrists described as being no better than a ‘coin toss’ ( Suominen et al., 2002 ; Roberts et al., 2019 ). Using big data is also in the medical field seen as one of the promising techniques to better predict suicide ( Luxton et al., 2015 ; Duarte et al., 2017 ). At the same time, Facebook engaging in suicide prevention fits in a more general trend of big tech companies offering health solutions, also recently referred to the Googlization of health ( Sharon, 2016 ). As Sharon (2021) has argued both in relation to research and (personalized) care, “[i]n recent years, the large consumer technology companies that have become the predominant architects of our digital environments have swiftly moved into the health and biomedical sector, positioning themselves as important facilitators of digital health and medicine” (p. 315). The Googlization of Health is a relatively recent term to describe a heterogenous set of activities and actors, ranging from health apps through to content moderation for health on platforms like Facebook and Twitter. Because of such activities, (big) tech companies and often the algorithms they develop increasingly contribute to how we define health and what constitutes an appropriate response to what is deemed unhealthy ( Gerrard and Thornham, 2020 ; Lupton, 2017 ; Sanders, 2017 ).
Often when scholars write about this phenomenon of private tech companies delivering a form of health care they see it as ‘sector creep’ or ‘sphere transgression’, where market values and market thinking gets intertwined with the public service of healthcare ( Sharon, 2021 ). Sector creep is the phenomenon where organizations generally serving one domain (e.g. social media companies) move into another domain (e.g. health). This “can lead to a reshaping of these sectors to align with the values and interests of non-specialist private actors, which may or may not be the interests and values of those groups and individuals who should immediately benefit from the distribution of goods in those spheres” ( Sharon, 2020 , 2021 ). A similar criticism is that of “surveillance capitalism”, which Zuboff defines as a “new form of information capitalism [that] aims to predict and modify human behavior as a means to produce revenue and market control.” (p. 75) In such systems, data is gathered, accumulated and analysed, in order to predict what people are going to do (and buy), to classify them in certain ways, and to nudge people in certain directions based on such data-sets, all in order to profit from the knowledge gained. It thus combines ‘surveillance’ – monitoring people continuously, measuring their performance, modifying their behaviour – with a capitalist logic of profiting from people's often invisible labour.
While there is much to say for such a sector creep and surveillance capitalism perspective, allowing us to critically study big tech practices for health and beyond, it also relies on a stark distinction between market logic and public values, which are deemed incommensurable. However, Sharon (2021) has recently countered this perspective of incommensurable or hostile spheres, by drawing on Bolthanski and Thévenot's orders of justifications, Viviane Zelizer's criticism of the idea of distinct spheres, and Michael Walzer's theory of justice . Sharon argues that rather than seeing the market and the public good as ‘hostile worlds’, it can be more helpful analytically to see them as ‘multiple spheres’, and to study the different values private actors draw upon to justify their activities. She for instance has examined different practices under the umbrella of the ‘Googlization of Health’ and the justifications that big tech companies give for their health practices, such as that of efficiency and public health .
In addition to studying how actors themselves justify their practices though, it is also crucial to examine how they are received by the public. For instance, how do people perceive the justifications given by big tech companies? And how do they see the Googlization of Health impacting on their own lives and the communities and societies in which they live? As big tech is increasingly governing the public sphere, including in relation to notions of democracy, normalcy, health, and sociality ( Cheney-Lippold, 2011 ; Birk et al., 2021 ), it is important to understand better how this is perceived in the public domain, by those directly or indirectly affected by the platformization of health ( Bucher, 2017 ; Poell et al., 2019 ).
In this paper, I will take Facebook's AI programme for suicide prevention as a case study of the Googlization of Health. In particular, I am interested in the media and public response to this programme, and the values that both media commentators and their public draw upon to make sense of and assess the programme. Of course, the media and public response is only one side of the coin, and it is important to also investigate how those Facebook users affected by the programme experience it. At the same time, previous studies have shown that (social) media analyses can shed light on the values that are deemed at stake, and the extent to which such Googlization of health technologies change these values ( Brownlie, 2018 ; Kudina and Verbeek, 2019 ). The Facebook suicide prevention programme has received criticisms in academic literature ( Marks, 2019 ), but hitherto no wider studies have been done in relation to public and societal perceptions, to my knowledge. This paper will investigate public perceptions with the programme through exploring media items and comments.
But let me start with a disclaimer. It is not my intention in this paper to make claims about how suicide prevention should be done, nor how current approaches are lacking. Indeed, authors in critical suicidology (one of the branches of literature I will draw upon, as described further below) who critically consider suicide prevention practices, still argue that a plurality of approaches including that of prevention are needed to address the complexity of suicide ( Marsh et al., 2022 ; White and Morris, 2019 ; Chandler et al., 2022 ; White, 2017 ). Neither am I interested in solely criticizing Facebook's programme. Rather, I am interested in the values that both Facebook itself and the media and their audience draw upon in talking about this programme, and how they see these values affected or enacted. To do so, I draw on the theory of technical mediation, as well as studies in healthcare looking at the multiplicity and enactment of values of good care, described in more detail in the next section.
My aim in this paper is not to answer questions like ‘should Facebook engage in suicide prevention? Should algorithms? How should either/both do this? And should suicide be prevented at all?’ Instead, my aim is to investigate what values are at stake in media discussions about this, and how they respond to Facebook's public justifications of the program. As such, my theoretical inspirations come from different strands of literature that all are connected through their focus on ‘values’.
Firstly, this paper builds on insights from the aforementioned Googlization of Health. For instance, Sharon's (2021) recent contribution on ‘multiple spheres’ explicitly draws upon actors' justification practices based on ‘orders of worth’ that include values, such as a civic order in which “the common good is conceptualized in terms of social value and collective benefit and values like solidarity, equality and participation are foregrounded” (p. 321). As Sharon then goes on to argue, a framework based on ‘multiple spheres’ “can thus account for a broad diversity of value orientations that seem to be at work in a phenomenon like the Googlization of health” ( Sharon, 2021 : 321). Values play a role both in justification of private actors' involvement in health care and research and in the specific technologies used and how they are used, and can in fact be seen to be constituted by these technologies ( Sharon, 2017 , 2021 ).
Indeed, in the process of developing, implementing, and justifying new technologies, values are not just drawn upon but are also (subtly) changed, as researchers of technological mediation have also shown ( Verbeek, 2006 ). Justifications then can be performative, constituting the value frameworks at the same time as appealing to them, but also in subtly changing (or translating) these values. For instance, Kudina and Verbeek (2019) show the added value of the theory of technological mediation for investigating the way values are transformed by exploring online comments discussing Google Glass (i.e. smart glasses that, for instance, allows people to access information hands-free). From a technological mediation perspective, one “studies the dynamics of technomoral change itself”, investigating the way in which technologies mediate “the relation between users and their environment” (p. 297). The authors explore the value of ‘privacy’ and how it is variously defined, or enacted ( Mol, 2003 ), in online discussions about Google Glass. My paper shares the concern for how technologies (in this case Facebook's AI suicide prevention programme) are interrelated with certain values, and in particular those of privacy, human dignity in matters of life and death, and public goods.
In addition, I draw on critical suicidology literature, which offers a critical perspective on how suicide and suicide prevention are conceptualized and approached, both in the literature and in practice. For instance, scholars of critical suicidology have pointed out that suicide is often medicalized, framed in terms of (mental) illnesses, and moved to the medical sphere and to responsibilities of medical professionals ( Hewitt, 2010 ; Petrov, 2013 ; van Wijngaarden et al., 2016 ; Marsh, 2020 ). In addition, they have argued how “neoliberal discourses of risk and responsibility” contribute to the individualization of the problem of suicide in a “de-politicized way, placing the responsibility (onus) for change on individuals through therapeutic techniques of rational problem-solving, self-management, coping, and skill-building” ( White and Morris, 2019 ). This is despite a growing acknowledgement that socio-economic determinants play a large role in suicide risk ( Chandler, 2020 ; Rogers and Pilgrim, 2014 ). Marsh has termed this tendency the “compulsory ontology of pathology” of suicide, where suicidal ideation is seen as pathological and placed in an individual ( Marsh et al., 2022 ), rather than paying attention to “psycho-political” regimes ( Marsh, 2020 ) and the “the social justice issues that make some lives more (un)livable than others” ( Marsh et al., 2022 : 12).
Moreover, some authors in critical suicidology literature argue that the mere notion and practice of suicide prevention often relies on a ‘logic of life’, where people are expected and disciplined to understand and acknowledge the intrinsic value of life, and where wanting to end one's life is seen as irrational and a sign of mental illness, rather than perhaps a logical response to the circumstances in which they find themselves ( Tack, 2019 ; van Wijngaarden et al., 2016 ; Améry, 1999 ). In that sense, suicide prevention itself – and who is responsible for it – is a value that is interrelated with, and can be changed by, technologies such as that of Facebook.
Finally, to more concretely analyze what values are at stake and what form they take, I take inspiration from the work of Mol and Pols, who both show how values like autonomy are ‘enacted’ in different ways in different situations ( Mol, 2003 ; Mol et al., 2010 ; Pols, 2006 ). Such a bottom-up approach means studying how values take shape in particular situations, where Pols for instance showed that autonomy can mean respecting an individual's wishes at all cost, actively helping individuals formulate goals, or as relational autonomy ( Pols, 2006 ). While Mols and Pols' studies are ethnographic in nature, discourse is practice too and so conducting a media analysis can be one way of studying the values at stake and the forms these values take.
To sum up, studying discourses around Facebook's algorithmic suicide prevention programme allows for an exploration of the hopes and fears around such initiatives, the value judgments surrounding them, and a better understanding of how they would be adopted in society and with what (moral) responsibilities and consequences. The aim in this paper is to go beyond justifications by Facebook itself, to understand better how the larger public discusses the phenomenon of Facebook doing suicide prevention, and what values they see as at stake and enacted in and through Facebook's programme.

Section: 2. Methods

This study presents an analysis of both media items (texts and videos) and, where that option existed and comments were posted, comments of readers/viewers below these media items. ‘Media’ here is a deliberately broad term, where I did not make a distinction between more traditional media outlets and newer, online sources. Rather, I included all sources as long as they were not owned by Facebook itself. Because the aim of this study is to examine how a range of commentators, including readers themselves, discuss Facebook's AI programme for suicide prevention, only traditional media outlets would not give the breadth of perspectives desired. That was also the reason why YouTube videos were included, as these open up further possibilities for studying a range of commentators.
To analyze media reporting and readers' comments, I conducted several searches using the neutral search engine ‘Startpage’, with various combinations of the search terms ‘facebook’, ‘AI’, ‘algorithm’, and ‘suicide prevention’. The first four searches were conducted mid-October 2018, with the first two looking for media reports and the second two searches looking for videos. A final, fifth search was conducted at the end of Marks, 2019 with a time limit of one year, from March 2018 till Marks, 2019 , to find more recent media articles so as to allow an examination of whether the reporting had changed compared to the earliest articles when Facebook's programme had just been made public.
For each search, I looked at the first 50 links, and, when they seemed relevant and not a duplicate, I copied them into an Excel master sheet. I excluded articles that discussed Facebook's suicide prevention programme before AI was used. In total, 75 media documents have been included for the analysis, and between them they produced 1176 comments (from 18 different sources, the other 57 either didn't have the possibility for leaving comments or they did but there were no comments). The number of comments between these 18 sources differed extensively, with an outlier attracting 602 comments, and quite a few others only a handful. A table providing an overview of all included sources and the number of comments per source can be requested by contacting the author.
I converted all media documents into a word document, resulting in 75 word documents. In case of a video, I transcribed the text in a word document. If the media item attracted any comments, I also copied and pasted these in the same word document. Using the qualitative analysis software Atlas. ti, I first performed a very detailed coding of the media items and comments, resulting in 433 unique codes, that range from broad codes like ‘ethical issues’ through to very specific codes such as ‘arrogant overlords’. After a few more rounds of grouping (group) codes, I considered how ‘invasiveness’ could tie together many of the group codes. Even though ‘invasive’ does not capture all the individual codes similarly, it is exhaustive enough to cover many of them and to provide a relevant overarching framework. To give a few examples: ‘invasive’ is linked to privacy and where (private) companies can intervene; to AI intervening in a sphere that may need to be reserved to human beings; to Facebook being invasive and untrustworthy; or to suicide prevention in itself being invasive. Thus, ‘invasive’ came up mostly inductively, but in different forms, and it is exactly these different forms (with different values attached) that will be explored in this paper.
Responding to the latest debates on ethics in online research ( Sugiura et al., 2017 ), I have opted to paraphrase but not quote verbatim comments from the audience. Along those lines, I have also not mentioned where a reader's comment was placed unless this was very relevant for the argumentation. This is because readers have not been able to consent to their quotes being used in research papers. This is different for media articles and videos, that can be referred to less problematically, and that generally have copyright protection. For published media items, then, I have used verbatim quotes.

Section: 3. Results

In what follows, I will describe the various forms of invasiveness plus counter-arguments that were mentioned in relation to the Facebook AI suicide prevention programme. In particular: section 1 deals with skepticism about Facebook serving the public good, and being (commercially) invasive instead; in section 2 I will describe how commentators suggested Facebook was (or was not) abiding privacy and data protection laws, thus invading people's private lives; in section 3 I will describe how the invasiveness of the programme means it was undermining rather than enhancing safety, thus invading people's safe spaces; and in section 4 I will analyze how some commentators found suicide prevention itself to be already invasive.
A dominant theme throughout the media items and readers' comments was that of ‘corporate responsibility’, where Facebook, despite being a private company, contributes to what was generally seen in the media items as a public good (or, alternatively, where they invade a space where they shouldn't be). In this section, I will analyze how Facebook was by some commentators described as being invasive because of its private/commercial interests masked by this supposedly humanitarian initiative. But first the floor is given to those who genuinely believed Facebook to have the public good in mind.
Unsurprisingly, Facebook itself ‘sold’ its programme as part of a trope of corporate responsibility: “With all the fear about how AI may be harmful in the future, it's good to remind ourselves how AI is actually helping save people's lives today," CEO Mark Zuckerberg wrote in a post on the social network. [Engadget, 27-11-2017]
Forefront, a suicide prevention agency that collaborated with Facebook in creating the AI suicide prevention programme, likewise argued: “For better and for worse, social media has become a place where people reach out for help and sometimes share their suicidal thoughts and plans. Social media companies like Facebook have a responsibility to be proactive in this space,” said Forefront faculty director Jennifer Stuber. “There will always be concern about the misuse of AI technology in this realm, but with over 800,000 lives lost per year to suicide around the world, careful application of AI can’t come quickly enough.” [Intheforefront, 28-11-2017]
Some commentators often went along with this frame of corporate responsibility. In his video blog on Facebook's suicide prevention programme (as part of a more general video blog about machine learning and psychology), Oscar Alsing (20-12-2017) said: “I feel quite proud that the world's biggest social networks actually take this move to prevent suicide, because they have such massive influence .”
In some of the articles, Facebook was said to have a “precious dataset” [Mashable, 28-11-2017], and, as such, being in “a unique position” (as TechCrunch quoted Rosen, an employee of Facebook; 27-11-2017) to use that dataset for the public good and to save lives. Indeed, the notion that they did something for ‘good’ was used as a counter-argument against the programme potentially being intrusive: This is helping us in public safety," Gerace, who's been in law enforcement for 39 years, told CNBC. “We're not intruding on people's personal lives. We're trying to intervene when there's a crisis. [CNBC, 21-02-2018]
The appeal to corporate responsibility and public good was further strengthened (although sometimes simultaneously weakened, as it showed Facebook's vested interests) through references to live suicides Facebook encountered on its platform. Indeed, many media sources pointed out this was one of the reasons suicide prevention is particularly pressing for Facebook, such as an article in Wired:
[E]arlier this month, the company began turning some of those AI tools [machine learning, deep neural nets] to a more noble goal: stopping people from taking their own lives. Admittedly, this isn't entirely altruistic. Having people broadcast their suicides from Facebook Live isn't good for the brand. [Wired, 17-03-2017].
In readers’ comments too, it was often expressed how this programme was serving the (public) good. One reader, for instance, described how a friend of theirs was suicidal, and while all of her friends were asleep, Facebook was not and sent help, with the implication that Facebook saved her life.
Other media posts as well as readers’ comments argued that Facebook has a hypocritical role in both causing and alleviating mental health problems, such as in a commentary in the Guardian by lawyer Mason Marks: We already have mounting evidence that social media causes mental health problems, and it now seems those mental health problems have the potential to interfere with FB's revenues. Their response is to use AI to minimise the fallout from this, rather than to minimise the problems their product causes, which would most likely interfere with their revenues even more seriously. [The Guardian, 30-01-2019]
Indeed, a reader said that they could see that Facebook has a good dataset for estimating suicide risk, maybe better than any other company at the moment, but that they would not trust Facebook to do suicide prevention. The only company they would trust even less was a store selling guns and other material that can be used for committing suicide, this reader said tongue-in-cheek. Across several of the media sources, readers said that Facebook's main motivation is and always will be profit, and that this programme is just another way of securing profit or of making sure they do not lose out on profit, for instance because of declining credibility.
In addition, even where some media sources and their readers/audience agreed that Facebook has a corporate responsibility to use their precious dataset, they argued that Facebook is currently not using that responsibility well. For instance, such commentators noted, Facebook has so far not been transparent about their algorithm, and has not released any data about effectiveness of the programme. This quote taken from an article in Fortune is illustrative:
In the same way a clinician might acknowledge the side effects of a medication, it would behoove Facebook to recognize how it might be negatively affecting suicide rates with this intervention. To be successful, Facebook will need to plan how to measure its actual helpfulness against sham interventions. [Fortune, 30-11-2017].
A similar point was made in Nature, albeit giving Facebook slightly more the benefit of the doubt:
The company's tight-lipped approach has left some researchers concerned. “They have a responsibility to base all their decisions on evidence,” Cash says. Yet the company is providing little information that outside experts can use to judge its programme.
Nevertheless, Insel is glad that Facebook is trying. “You have to put this in the context of what we do now,” he says, “which is not working.” [Nature, 12-12-2018].
This final comment indeed seemed to resonate with some of the readers across the media sources. Such readers for instance argued that they personally were no fan of Facebook but that did not mean they couldn't see the good intentions – and even good results – associated with the programme, or they ironically commented on how, of course, letting someone die is better than help from Facebook.
Finally, in readers' comments especially, the sentiment could be found ‘damned if they do, damned if they don't’, meaning that Facebook cannot do it right: intervening is bad but not intervening is too, especially given the livestreaming of suicide on their platform. Moreover, while for some people Facebook's programme can be seen as “sphere transgression” ( Sharon, 2021 ), practicing medicine yet without a license, others argued that in that case none of us without a medical license can ever report someone with suicidal thoughts or try helping them. Here, then, some media commentators and readers seemed to see suicide as a decidedly medical problem, requiring medical attention, and governed by medical regimes in relation to duties of care, confidentiality, and transparency, while others argued that suicide is a societal problem that should not be left solely to medical professionals to be dealt with.
To conclude this first results section, corporate responsibility and the notion of the public good were used to motivate and defend Facebook's AI suicide prevention programme by some media commentators and readers, and they argued that due to its precious dataset Facebook might have a duty and a responsibility to intervene. Others however were more skeptical, arguing that Facebook's prime motivation always is private interest, not public good, and that Facebook oversteps its mark by now also adopting a hitherto strongly regulated public health function, something for which Facebook should and cannot be responsible. These commentators, then, saw the programme as Facebook being invasive, which we could link to the notion of sector creep.
In this section, I will analyze how commentators and readers/audiences of these commentaries argued that Facebook's AI suicide prevention programme is not appropriately regulated and/or does not sufficiently pay heed to privacy regulations. A CNN-article, about suicide prevention by platforms more generally, discussed this lack of regulation as follows: “Facebook is not a covered entity, and Amazon is not a covered entity. Google is not a covered entity," he [Magnus, “a professor of medicine and biomedical ethics at Stanford University”] said. “Hence, they do not have to meet the confidentiality requirements that are in place for the way we address health care information." […] The only protections of privacy that social media users often have are whatever agreements are outlined in the company's policy paperwork that you sign or "click to agree" with when setting up your account, Magnus said. [CNN, 12-02-2019]
Some articles even said that Facebook's initiative has been ignored by regulators: Though Facebook’s data practices have come under scrutiny from governments around the world, its suicide prediction program has flown under the radar, escaping the notice of lawmakers and public health agencies such as the Food and Drug Administration (FDA). [The Guardian, 30-01-2019]
At the same time, regulation was seen as the reason why Facebook's programme is not implemented all over the world. In particular, Facebook's algorithmic suicide prevention programme is not implemented in the EU, which in various articles was attributed to the GDPR . Consider for instance the Telegraph's heading: “EU data laws block Facebook's suicide prevention tool” (emphasis added; 28-11-2017).
However, other articles suggested that the GDPR in itself does not block Facebook's efforts, but only places certain conditions on its implementation. For instance, an article from the EETimes reported: He [Bietz, a member of the organization ‘Pervasive Data Ethics for Computational Research’] acknowledged that the EU has “some of the strictest data regulations in the world” but added that “it is not entirely clear that what Facebook is doing” would be illegal under the GDPR standard. It might be more accurate to say that Facebook is venturing onto “an edge that hasn’t really been tested,” he said, “and my guess is they decided they don’t want to be the test case.” [EETimes, 13-12-2017]
Others however suspected that Facebook's suicide prevention programme and not implementing it in the EU is in part a strategic move against the GDPR: But if complying is possible, why isn't Facebook making the effort to do so here in the EU? "I have to wonder if this is a shot across the EU’s bows," said Turner [“a data protection consultant”]. “Facebook perhaps wants to undermine the GDPR (…) and they’re using this as a method to do so." Turner added: "Nobody could argue with wanting to save lives, and it could be a way of watering down [data protection] legislation that is a challenge to Facebook’s data hungry business model. Without details of what they think the legal problems are with this, I’m not sure they deserve the benefit of the doubt." [Wired, 28-11-2017]
This article in Wired argued that if Facebook wants to comply with the GDPR, one route they could take is that of ‘consent’, asking users more explicitly to consent to the programme and offering a possibility to opt out as well. That was indeed one of the concerns in many articles and also of readers: that explicit consent for scrutinizing their data and potentially intervening in case of deemed risk has not been asked, making it particularly invasive.
When it comes to using data from, and about, people's personal lives, the sentiment that Facebook is stealing data came up frequently, resonating with Zuboff's points about surveillance capitalism ( Zuboff, 2015 ). One of the points raised was that this data can then be sold to, for instance, insurance companies and other companies that can add to the stigma that people with suicidal ideation may already experience: But it is easy to think of dozens of ways that pattern recognition software could be misused. For example, suicide is sometimes associated with mental illness which introduces the possibility that medical records might be compromised or mentally fragile users might be outed and then subjected to online bullying. [Diginomica, 30-11-2017]
Other commenters went against this reasoning and blaming of Facebook, however. They argued that people do in fact consent to such practices by Facebook in the act of using Facebook. Yet other commentators and readers said that people should have no expectation of privacy on the internet at all, or that people deserve this invasion of privacy by using Facebook. Indeed, some people suggested that people share their ‘boring lives freely online’ so there is no need to steal it – it is already out there. One person who themselves suffered from suicidal thoughts said they were not enthusiastic about Facebook's programme but, they argued, the data is not private but belongs to Facebook.
To conclude this section, commentators and readers alike suggested that Facebook's suicide prevention programme invades people's private lives, without the necessary checks and balances. They criticized the lack of regulation covering Facebook's programme, where medical suicide prevention usually has to adhere to strict regulations. While many articles wrote that the GDPR is the reason why Facebook's programme cannot be implemented in the EU, others were more skeptical and said Facebook could make it work, but wanted to make a strategic move against the GDPR (as who can be against preventing suicide, a question I will come back to in the fourth section of the results). Here, responsibility was attributed both to regulators and to Facebook itself for adhering to regulations, but also to users of Facebook who, it was suggested, share their data for free and so should not expect any privacy.
Resonating with the response from online mental health communities on Samaritan's Radar ( Brownlie, 2018 ), media posts and comments from the audience expressed a feeling that Facebook's programme was intrusive and threatened the safety both of communities and of individual users. For instance, in this article from Mashable, De Choudhury, “an assistant professor in the School of Interactive Computing at Georgia Tech”, was quoted to say that people might not share suicidal posts when their posts are continuously monitored: De Choudhury says transparency is vital when it comes to AI because transparency instills trust, a sentiment that's in short supply as people worry about technology's potential to fundamentally disrupt their professional and personal lives. Without enough trust in the tool, says De Choudhury, at-risk users may decide against sharing emotionally vulnerable or suicidal posts. [Mashable, 28-11-2017]
This also had to do with how Facebook acted on their estimation of risk. As described in the introduction of this article, in high-risk cases a ‘wellness check’ was instantiated, described by some in the dataset as “suicide by cop”, given the shaky track record of the US-police knocking on people's doors, they argued. One reader argued that this is the worst response to an already upset person, escalating their problems and locking them up against their will.
Indeed, a reader said that they were now limited to paper and pen to express their thoughts, implying that they then lacked the support that a social media post could provide, which this reader described as being ‘isolated’. Similarly, an article from TechnoSkeptic was appropriately skeptical: But on an interpersonal level, being anonymously reported by friends seems unlikely to foster a sense of trust and security for the struggling person, even less so if they suspect the helping hand being offered to them may just be an algorithm. […] [W]hile taking on responsibility for someone else’s mental health is by no means obligatory, Facebook’s attempt to emulate real human comfort seems bound to leave some people feeling even more isolated than before. [TechnoSkeptic, 28-06-2017]
Facebook ixtself argued that such safety concerns were the reason why they were adopting the algorithm in the way they did to start with: Ms Callison-Burch [a Facebook product manager] acknowledged that contact from friends or family was typically more effective than a message from Facebook, but added that it would not always be appropriate for it to inform them. “We're sensitive to privacy and I think we don't always know the personal dynamics between people and their friends in that way, so we're trying to do something that offers support and options," she said. [BBC, 01-03-2017]
While some commentators argued that the lack of transparency and information about the programme's effectiveness is what makes the programme more unsafe for Facebook users, Facebook itself argued that this lack of transparency is part of what makes the programme effective in the first place, as otherwise people will start to “play games with the system”: For instance, Marks says, the outcomes need to be checked for unintended consequences — such as a potential squelching of frank conversations about suicide on Facebook's various platforms. “People … might fear a visit from police, so they might pull back and not engage in an open and honest dialogue," he says. “And I'm not sure that's a good thing." But Facebook's Davis says releasing too many details about how the AI works might be counterproductive. “That information could allow people to play games with the system," Davis says." [NPR, 17-11-2018]
Responding to the argument that people can start playing the system, Torous, Co-Director of the Digital Psychiatry Programme at Beth Israel Deaconess Medical Center, argued in this MedScape article: However, "if it's a robust system, it shouldn't be that fragile," Torous said. “Are they saying the system is that unreliable that slight changes will perturb it and change the predictions? If you're sending out an ambulance, you want to be basing it on some pretty good evidence." [MedScape, 15-01-2019]
In addition to being accused of ‘playing games with the system’, people in general and especially people on social media sites were said to be ‘cunning’. This commentary in The Telegraph said: That illustrates a second problem: humans are cunning. The scary truth about self-harm content on Instagram, or any other social network, is that some of it comes from very committed and secretive communities. Even in 2016, Instagram forbade "glorifying self-injury", and made hashtags such as "#selfharm" unsearchable. But the dedicated pro-cutting community added another "m" to make "#selfharmm", and kept adding extra "m"s as each new version was banned. […] For that reason, tech firms are often reluctant to explain how their AI systems work in detail. [The Telegraph, 08-02-2019]
While this was just one article in the Telegraph, the fact that people are seen as deceitful when they are creating a safe space to talk about self-harm and suicide is an interesting word choice. This links in with the complexity of suicidal behaviour and suicide prevention: on the one hand, safe spaces are needed to open up about issues that are generally still shameful for people to talk about and these safe spaces can actually have preventative effects, but on the other hand it is exactly these so-called ‘safe spaces’ that may turn out to be unsafe, such as in relation to pro-ana or pro-choice websites, where people may actually encourage one another to harm themselves ( Marsh et al., 2022 ; White, 2017 ).
To conclude this section, while Facebook and others (would like to) see Facebook's suicide prevention programme as enhancing safety on the platform and for people with suicidal thoughts, other commentators, including readers, suggested that the programme could actually undermine safety and, as such, invade previously safe spaces. People, it was suggested, might not feel safe to discuss their suicidal thoughts knowing Facebook is watching and intervening when the risk is deemed high. They linked this to the fact that Facebook might have a responsibility to intervene but that the way they intervene now, without transparency and without evidence, does not inspire confidence in people, and hence that they do not actually take their responsibility. However, a response to such concerns was that people might ‘play games with the system’ or they are seen as ‘cunning’, and so offering more transparency to enhance trust is not something Facebook feels able to do.
While in many of the media articles themselves the debate was whether Facebook and/or AI should be engaged in suicide prevention, and this debate was definitely continued in the comments, another question became apparent in readers’ comments: should suicide be prevented at all? This will be a shorter section without excerpts, given that I decided not to use verbatim quotes from readers and it was only readers discussing this question.
Readers problematized the notion of suicide prevention being a public good, or whether people instead have ‘the right to die’, thus arguing that suicide prevention in itself can be invasive. Such readers said that people should have the freedom to decide on their own fate, should not be forced to stay ‘at this party’, and that governments, friends, and least of all social media have any business preventing people from committing suicide, especially when it also puts innocent people at risk. It is not up to Facebook to allow us to die, these commenters said.
When such a notion of ‘why preventing suicide’ came up, though, inevitably other readers stressed the importance of suicide prevention, and they usually argued their case by talking about people wanting to be ‘saved’ and loved ones being grateful for not having the person close to them attempt suicide. Sometimes readers discussed their own personal situations, either of having suicidal ideation themselves or having seen it in a loved one, and as such being grateful suicide prevention efforts exist. A minority of readers made this into a personal argument against the people who suggested that suicide does not need to be prevented. They argued that these people apparently never experienced suicidal ideation themselves or in their loved ones, saying something along the lines of ‘wait until you suffer from suicidal thoughts, then you will also see the added value of suicide prevention’. But the people who were targeted by such an argument and ‘accused’ of not having enough experience to weigh in on the debate sometimes said that they too have had suicidal thoughts, or have had people close to them with suicidal thoughts, and yet they still thought that we have ‘the right to die’.
At the same time, in line with what critical suicide scholars have argued, readers of media items suggested that providing individual prevention is not the best way to alleviate suffering. Rather, they argued that societal structures need to change, such that people may be less likely to find life unworthy of living. As such, these readers placed responsibility for suicide prevention more on society writ large. Furthermore, some of these readers argued that it is exactly social media companies that contribute to lives becoming less worthy of living, going back to the point in section 1 of the results about the hypocrite of Facebook engaging in suicide prevention.

Section: 5. Discussion and conclusion

In this paper, I conducted a media analysis of Facebook's AI programme for suicide prevention. Through an inductive analysis, invasiveness came up as a key theme in the 75 media items and in the readers' comments. ‘Invasiveness’ took different forms, though, and as such the different forms of invasiveness might not (always) align. Invasiveness was related to Facebook invading the health sphere, whereas their interests were seen as commercial mostly/only. It was also related to privacy and privacy laws, where Facebook was seen to invade people's private lives and thoughts without their consent. Thirdly, the consequences of Facebook's programme were also seen to be invasive, undermining rather than enhancing safety, and without any efforts to instill trust in their users. Finally, some people argued that suicide prevention itself is already invasive. In that respect, the notion of ‘invasiveness’ can be of analytical aid in going beyond privacy and surveillance capitalism concerns when thinking about big tech, something which scholars such as Sharon (2020) have argued for. As my analysis shows, the concept of ‘invasiveness’ may encompass privacy but also encompasses other values such as that of autonomy and what it means to live a good life.
Discourses can play a key role in describing, as well as constructing, who is or should be responsible, for instance for preventing suicide. In relation to Facebook doing suicide prevention, agency and responsibility were placed on different actors depending on who or what was seen to be invasive. For instance, often it was Facebook (as a general actor, or sometimes Mark Zuckerberg specifically) seen as invading new domains that they had no business (nor any responsibility) entering. Here, suicide prevention was generally seen as the responsibility of medical professionals. At other times, though, it was AI itself seen to be intrusive, and knowing us better than we know ourselves, where AI, no matter who employs the AI, should not be used for suicide prevention. Moreover, sometimes the language suggested that it was AI doing suicide prevention, rather than a socio-technical assemblage of human and non-human actors. AI was ascribed a range of qualities, such as: understanding the nuances of human language, emulating real human comfort, and improving with age (where humans decline with age, it was said in contrast). Thinking about the Googlization of Health ( Sharon, 2016 , 2020 , 2021 ), it is thus interesting too what values and functions are ascribed to AI in particular, to further investigate the justifications given for Googlization of Health practices.
While the study is not representative of the wider population, I hope it gives further impetus to look at what values play a role in this programme and how they change alongside the implementation and dissemination of Facebook's initiative. For instance, the value of living a good life (and a good death) was problematized in media items and readers' comments. One of the considerations here is whether people at risk of attempting suicide can live a good, or even better, life when social media companies scrutinize their posts. There is no clear-cut answer to such questions, and unsurprisingly no consensus was found in this media analysis. However, the analysis conducted here, studying the (emotive and argumentative) discourses employed in relation to Facebook's initiative can help illuminate what is at stake when we continue to let Facebook employ algorithms for suicide prevention, and when such efforts are not strictly regulated.
This paper deals with a complex case study , in which questions about private actors doing suicide prevention, algorithms involved in suicide prevention, and the invasiveness of suicide prevention more generally all play a role. As such, the paper should be seen as a first attempt to study Facebook's suicide prevention attempt in an empirical way. One of the areas where research is still lacking is to investigate the experiences of people that have been flagged by Facebook's program. Furthermore, Facebook's justifications, both internally and externally, for engaging in suicide prevention would be an interesting future avenue for research. Finally, my data collection took place before Covid-19, and Covid-19 will have further changed the dynamics and different values at stake in Facebook deploying AI for suicide prevention, given that in the last two years digital health has been even more intensely turned to provide and access (mental) health care ( Martinez-Martin et al., 2020 ; Lanzing et al., 2021 ). That too would be interesting to take into account in future research in this area.
One final point that I would like to mention here is a discussion that only came up in readers' comments. This is that suicide prevention in itself should already be seen as problematic, as it interferes with values such as autonomy and the ability to live a good life and a good death. In the media items, an article could go as follows: suicide prevention is important → people are not good at predicting suicide, so maybe AI can help → but should Facebook implement such a programme? However, a minority of readers suggested that the presumption in this argument is already wrong; instead, people have the right to die. This resonates with Tack's (2019) analysis on ‘the logic of life’, as well as Amery's seminal work on voluntary death ( Améry, 1999 ). Such authors protest against the hegemony of life being inherently valuable, and against the corresponding ideas that wanting to die is irrational and people experiencing suicidal thoughts necessarily need to be helped (see also van Wijngaarden et al., 2016 ). When suicide prevention is not bound to more traditional sectors (among which emergency response teams, mental health care, and railway companies), it becomes an even more ubiquitous and normal part of society. This might potentially lead both to de-medicalization and , paradoxically, to even further pathologization of suicidal thoughts where such thoughts are deemed abnormal and something to be cured (this relates to points made by authors in critical suicidology, for instance: Chandler et al., 2022 ; Marsh, 2015 ; Hewitt and Edwards, 2006 ; Salem, 1999 ; Marsh et al., 2022 ; White and Morris, 2019 ; Marsh, 2020 ). This would be an interesting question for further (empirical) research.

Section: Credit author statement

Tineke Broer is sole author of the manuscript, and was solely responsible for the design of, data collection and analysis for, and writing of the paper.

Section: Biographical note

Tineke Broer holds a PhD in Science & Technology Studies, with a particular interest in mental health and health technologies . After having worked as a Post-Doctoral Researcher at the University of Edinburgh, currently Tineke is Assistant Professor at the Tilburg Institute for Law, Technology, and Society, at Tilburg University.

Section: Statement regarding this submission

All authors have agreed to the submission and the article is not currently being considered for publication by any other print or electronic journal.

Section: Acknowledgments

I would like to thank my colleagues at TILT who were present at the Work-In-Progress meeting and provided valuable feedback, in particular Merel Noorman and Sunimal Mendis as discussants. I have presented this study at TILTing Perspectives 2019, and would like to thank the audience for their insightful comments and questions. Finally, I would like to thank the three anonymous reviewers for their careful feedback, as well as the editors of Social Science and Medicine.
