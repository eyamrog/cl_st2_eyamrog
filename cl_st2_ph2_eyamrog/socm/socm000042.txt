Title: Social, ethical, and other value judgments in health economics modelling


Abstract: Abstract

Modelling is a major method of inquiry in health economics. In other modelling-intensive fields, such as climate science, recent scholarship has described how social and ethical values influence model development. However, no similar work has been done in health economics. This study explored the role of social, ethical, and other values in health economics modelling using philosophical theory and qualitative interviews in British Columbia, Canada. Twenty-two professionals working in health economics modelling were interviewed between February and May, 2019. The study findings provide support for four philosophical arguments positing an essential role for social and ethical values throughout scientific inquiry and demonstrate how these arguments apply to health economics modelling. It highlights the role of social values in informing early modelling decisions, shaping model assumptions, making trade-offs between desirable model features, and setting standards of evidence. These results point to several decisions in the modelling process that warrant focus in future health economics research, particularly that which aims to incorporate patient and public values.

Section: 1. Introduction

After decades of debate, the role of values in science is still understood differently by scholars across the disciplines. Certain claims are uncontroversial: scientific research topics, ethical constraints on methods, and evidence-informed policy decisions reflect social values ( Longino, 1990 ; Douglas, 2009 ; Elliott, 2017 ). However, there is still disagreement as to precisely what, if any, decisions can or should be made on the basis of scientific values alone and whether, in fact, it is always possible to distinguish scientific from social values ( Elliott and Steel, 2017 ). This debate is hosted primarily within philosophy of science and has attracted few contributors from health economics. For their part, Rawlins and Culyer (2004) describe social value judgments in cost-effectiveness analysis (CEA), highlighting certain ones made by the National Institute for Health and Care Excellence (NICE) relating to efficiency and equity. They assert that in CEA “the main social value judgments regarding efficiency” (italics added) relate to the measure of health used and scope of costs and benefits included ( Rawlins and Culyer, 2004 , p. 226). We do not take Rawlins and Culyer's (2004) goal as to provide a complete list of social value judgments in CEA. Yet their assertion raises the question as to what such a list might look like, not only in CEA but in health economics modelling generally. The aim of this paper is to examine philosophical arguments that social values are influential throughout scientific inquiry, and to present qualitative evidence that highlights where value judgments occur in health economics modelling .
The rest of this paper is structured as follows: Section 2 reviews key concepts in philosophy's values in science literature, defining terms and summarizing arguments for an essential role of values in science. Section 3 argues there is a need for greater clarity around the role of social value judgments in health economics modelling, particularly in light of initiatives to incorporate patient and public values into health research. Section 4 describes a qualitative study undertaken with health economists in which they were briefed on the philosophical arguments and invited to give their views and relevant examples from practice. Section 5 concludes.

Section: 2. Overcoming science's value-free ideal: overview of the literature

The VFI has been described in detail elsewhere ( Douglas, 2009 ; Reiss, 2017 ). For our purposes, it suffices to restate that in the period following Levi (1960) , VFI proponents have upheld that only scientific criteria (“canons of scientific inference” ( Levi, 1960 , p.355)) should figure in accepting, or assigning a probability to, a hypothesis. The VFI is thus effectively a two-part normative statement, concerning what values can drive decisions in what stages of scientific inquiry. However, there is no standard articulation of the statement and both its parts are debated ( de Melo-Martín and Intemann, 2016 ). Below, we briefly review distinctions in values and stages of scientific inquiry in relation to the VFI.
In an influential lecture, Kuhn (1977) listed five scientific criteria and called them “values”, suggesting accuracy, simplicity, internal and external consistency, breadth of scope, and fruitfulness are things scientists value and use to choose between theories. McMullin (1982) gave a revised list (predictive accuracy, internal coherence, external consistency, unifying power, fertility, simplicity) and called these “epistemic” values, desiderata the pursuit of which assists in the attainment of knowledge (p.18). McMullin contrasted epistemic values with “non-epistemic” values: political, moral, social, religious and other values corresponding to human goals other than knowledge ( McMullin, 1982 , p.19). Following McMullin (1982) , debate over the tenability of the VFI has generally invoked the distinction between epistemic and non-epistemic values, or an analogous distinction, e.g., constitutive versus contextual values ( Longino, 1990 ). In health economics, Rawlins and Culyer (2004) distinguish between social value judgments (“what is good for society”) and scientific value judgments (“what is good and bad in the available science”) (p.224). In this paper, we follow Winsberg's (2012) definition of social values- “the estimations of any agent or group of agents of what is important and valuable—in the typical social and ethical senses—and of what is to be avoided, and to what degree"- ( Winsberg, 2012 , p.112), noting that some social values are ethical values. We contrast social values (as defined above) with scientific criteria, which we understand as epistemic values or values that in some way promote truth ( Steel, 2010 ). We take the term ‘value judgment’ to denote social value judgment.
Social values are what the VFI seeks to bar from scientific inquiry at certain stages- yet to pinpoint these stages is difficult ( de Melo-Martín and Intemann, 2016 ). While Levi (1960) focused on the role of scientific criteria in accepting or assigning probabilities to hypotheses, still more stages in science may be perceived as inappropriate territory for social values. As Longino (1990) notes, positivists tended to invoke the distinction between the “context of discovery” and “context of justification”, allowing for non-empirical influences only in the former, characterized by the scientist's mental or emotional life ( Longino, 1990 , p.64). For her part, Longino (1990) interprets the classical view of value freedom in science as follows: the external stages of science (i.e., choice of research topics, methodological constraints, policy decisions) may be influenced by social values, but “rules of inquiry”, the “conclusions, answers, and explanations reached by means of their use and guidance” may not be (p.84–85). Longino's (1990) interpretation is an influential articulation of the VFI (e.g., followed by Douglas, 2000 ), but not the only one: de Melo-Martín and Intemann (2016) take the VFI to refer to the “core of scientific reasoning”, including “the gathering and characterization of evidence or appraisal and acceptance of hypotheses” (pp. 501–502).
In philosophy of science , at least four arguments suggest social values are essential throughout scientific inquiry ( Douglas, 2016 ; Elliott and McKaughan, 2009 ; Parker and Winsberg, 2018 ; Rolin, 2016 ). As Douglas (2016) notes, these are not fully distinct arguments, but each has a different thrust. We outline them below as they may advance the understanding of where value judgments arise in health economics modelling .
The ‘argument from value-laden background assumptions' (AVLBA) ( Rolin, 2016 ), or ‘gap’ argument ( Brown, 2013 ), highlights that scientists make assumptions throughout their work, e.g., in generating hypotheses and interpreting evidence. Scientists change course if assumptions are disproved, yet not all assumptions can be tested- and changing course can mean rejecting a hypothesis, altering assumptions, or rejecting new evidence, the choice between which requires further assumptions ( Brown, 2013 ; Longino, 1990 ). Thus, scientists cannot proceed without making assumptions, and these may “encode” social values ( Longino, 1990 , p. 216). Many decisions in health economics that are well-recognized as social value judgments may be understood from this angle. For example, decisions around the scope of costs and benefits in a CEA ( Rawlins and Culyer, 2004 ), including the question of whether or not to count productivity losses or other non-health outcomes ( Shearer et al., 2017 ), reflect assumptions about what is valued in society. Debate around the adequacy of the quality-adjusted life year (QALY), and potential benefit of replacing it with another metric, turns on the same sort of assumptions ( Anand, 2003 ). So too does the decision to model health on a 0 to 1 scale with death as the lower bound, despite evidence that many perceive some health states as worse than death ( Macran and Kind, 2001 ).
The “boundary challenge” ( Douglas, 2016 ) asserts there is no clear difference between scientific criteria and social values ( Rooney, 1992 ; Longino, 1995 ), though one is crucial to the VFI ( Douglas, 2009 , pp. 89–90). Rooney (1992) points to the differences in philosophers' lists of epistemic values, suggesting the lack of convergence reflects the impossibility of demarcation. Furthermore, she questions whether it is always clear what values underlie a given scientific decision: models that support the binary conception of gender, for example, might be chosen on the grounds of simplicity and theoretical unification, or a desire to support gender order and hierarchy ( Rooney, 1992 , p. 18). Longino (1995) provides a novel list of feminist values, which she argues perform equally well in guiding theory choice, and notes that even the idea of empirical adequacy (i.e., fit between theory and observations), which some take as the keystone epistemic value, prompts one to ask which data or whose data a theory must be in accordance with (p. 394). Extending this, some theorists advance a related “argument from pluralism” ( Rolin, 2016 ), which suggests that the epistemic/non-epistemic distinction is insignificant because not all scientific values can be satisfied in a single study and social values are used to make trade-offs between them ( Elliott and McKaughan, 2014 ; Khosrowi, 2019 ). For example, Khosrowi (2019) argues that the evidence-based policy paradigm, which demands evidence from randomized controlled trials (RCTs) and meta-analyses, privileges the epistemic values of unbiasedness , precision , and ability to obtain causal conclusions at the expense of discrete moral and political values (e.g., equality, priority for the worst-off) because RCT methods do not foretell the distributive consequences of policies. In the modelling context, Matthewson and Weisberg (2009) highlight the existence of strict tradeoffs, whereby an increase in the magnitude of one desirable quantitative model feature necessarily results in a decrease in the magnitude of another. In general, the notion of trade-offs is useful for analyzing the role of social values in methodological decisions in health economics. For example, Whitty and Oliveira Gonçalves (2018) report that discrete choice experiments to derive preferences may be more acceptable to participants and lead to greater response efficiency, but best–worst scaling methods require a smaller sample size to generate precise estimates and thus have greater statistical efficiency. Their conclusion that the choice between the methods is “likely to be based on normative considerations” (p. 315) is consistent with the boundary challenge.
The argument from inductive risk (AIR), in its original formulation due to Rudner (1953) , is as follows: scientists must accept or reject hypotheses, though evidence is never certain. The decision to accept or reject a hypothesis must therefore be informed by the consequences of error in the situation at hand, and thus invokes ethical values: the scientist makes value judgments. A common rebuttal is that scientists are not required to accept or reject hypotheses, but only to assign probabilities or degrees of belief to hypotheses; in turn, it is policymakers who make value-laden decisions in light of the probabilistic evidence, thus maintaining scientists' value-freedom ( Jeffrey, 1956 ; Betz, 2013 ). However, this reply is undermined by Douglas (2000 , 2009) , who demonstrates that many methodological choices under uncertainty have ethical consequences, e.g., classifying ambiguous data one way or another, or choosing between two equally well-supported models. Furthermore, Steele (2012) underlines that scientists generally must convert degrees of belief into claims interpretable by policymakers, e.g., using qualitative statements of confidence or standardized thresholds, and argues that making these cruder reports forces scientists to choose between more or less cautionary approaches. In health research, the original AIR recalls the principle that the sensitivity and specificity of diagnostic tests should be set taking into account the relative costs (to patients and the health system) of false negatives and positives ( Zweig and Campbell, 1993 ). In the modelling context, the extended AIR applies to the specification of likelihood functions and probability distributions ( Steel, 2015 ) and, more broadly, the management of structural and parameter uncertainty ( Biddle and Winsberg, 2010 ; Winsberg, 2012 ).
A recent line of argumentation against the VFI problematizes its laissez-faire attitude toward social values in early phases of science, asserting that choices in these phases influence study results. Elliott and McKaughan (2009) point out that the degree of evidential support for a given theory or hypothesis depends on what data are available and what alternative theories or hypotheses exist. Therefore, scientists' choice of research topic, selection of study design, and pursuit of a particular theory or hypothesis over others affect their conclusions, suggesting that social value influences at these decision points should not be treated casually, but scrutinized and managed with care. A key characteristic of Elliott and McKaughan's (2009) argument is that it posits a downstream or ‘cascade’ effect of social value judgments in scientific inquiry. This effect is touched on in the AIR literature: Steel (2015) , for example, notes that early choices in model development subject to inductive risk have a downstream effect on what types of data can be considered and the array of hypotheses the model can explore (p. 85). Recent work in simulation modelling has further developed this line of reasoning, which we call the ‘cascade argument’ ( Parker and Winsberg, 2018 ; Winsberg, 2018 , pp. 139–140). Parker and Winsberg (2018) build on the non-controversial point that model development is influenced by purposes and priorities, the research goal shaping what things are represented in a model and how they are represented, including what simplifications, idealizations and distortions are acceptable (p. 128). For example, parameter values may vary as a function of the priorities guiding calibration efforts, e.g., ensuring model output aligns with past observations of specific variables of interest. Not only do Parker and Winsberg (2018) argue that these value judgments affect model estimates, they note that estimates from a given model are routinely incorporated into other models- suggesting the cascade effect of value judgments in modelling is more significant than is typically tracked or appreciated.

Section: 3. Values in health economics modelling: rationale for inquiry

In other modelling areas, such as climate science, recent scholarship has shed light on how social values influence model development ( Biddle and Winsberg, 2010 ; Intemann, 2015 ; Parker and Winsberg, 2018 ; Winsberg, 2018 ). To the best of our knowledge, no similar work has been done in health economics, though there is a strong rationale for it. A common concern is that allowing social values in science will threaten objectivity, driving inquiries toward pre-determined conclusions or ‘wishful thinking’ ( Anderson, 2004 ; Douglas, 2009 ) and undermining democratic decision making ( Betz, 2013 ). Indeed, these concerns are what undergird the long-established value-free ideal (VFI) for science ( Douglas, 2009 ; Reiss, 2017 ). Yet there is broad consensus among philosophers of science that the VFI is untenable, for reasons we outline in Section 3.2 ( Douglas, 2016 ; Elliott and Steel, 2017 ; Reiss, 2017 ). In this context, many contributors argue that social value judgments in science should not be denied, but rather made transparent and informed by the people they affect ( Douglas, 2009 ; Elliott, 2017 ). Rawlins and Culyer (2004) appear to agree, as they emphasize the need to ensure NICE's social value judgments resonate with the public (p. 225).
Current efforts to involve patients in health technology assessment (HTA) ( Abelson et al., 2016 ), health economics modelling ( van Voorn et al., 2016 ), and “patient-oriented” research (Canadian Institutes of Health Research (CIHR), 2014) align with the idea that patient and public values should be incorporated into research. However, current frameworks for patient and public involvement (PPI) do not all refer explicitly to patients' values or to the value judgments required in scientific research. For example, developers of the Guidance for Reporting Involvement of Patients and the Public (GRIPP2) note only that “PPI in research can improve the relevance and overall quality of research, by ensuring that it focuses on the issues of importance to patients” ( Staniszewska et al., 2017 , p. 2). Among frameworks that do speak directly to patients' values, none, to the best of our knowledge, describes where value judgments arise in research. For example, Abelson et al.'s (2016) framework for PPI in HTA advises only that engagement methods should match “the motivation for incorporating societal and/or patient perspectives into the process, and the relevant societal and/or patient values at stake” (p. 260). Another recent framework for improving patient-oriented clinical research marks the extent to which the research question “involves patient values and preferences” ( von Niederhäusern et al., 2018 , p. 9) as a dimension of research quality, but includes among its list of quality questions only the item “Are patient representatives/advocates and their needs and values involved in the development of the research question?” (p. 10). Thus, current frameworks on PPI and patient-oriented research either leave open what it means to incorporate patients' values into research or give the impression that patients should be involved only in choosing the research topic. Furthermore, to the best of our knowledge, there is currently no direct guidance for involving patients in health economics modelling. van Voorn et al. (2016) make a call for this, noting that involving patients in health economics modelling may present both benefits and challenges.
The lack of guidance for involving patients in health economics modelling represents one knowledge gap and points to another: where and how value judgments arise in health economics modelling. In our view, this knowledge is essential to guide patient and public involvement in modelling, as the ability to anticipate and recognize value judgments is an obvious pre-condition for managing them. Furthermore, this knowledge would be beneficial for health economists, particularly as PPI and patient-oriented research efforts intensify. As Coast (1999) notes, health economists seldom discuss issues of epistemology, but they are often considered to work either in the normative or positive area of the field. This reflects the assumption that there is a fact-value dichotomy and positive economics is the value-free branch of the field ( Reiss, 2017 ). Such a view is undermined by work in philosophy of science and economics ( Putnam and Walsh, 2012 ), yet there is indication that some health economists still espouse it. For example, in an online response to Coast (2004) , Claxton and Sculpher (2005) charged: At the heart of the paper by Coast is a fundamental misunderstanding and a clear preference for the comfort of decision makers over explicit and transparent social decision making . She really must distinguish between those issues which are about society's health values and those which are about positive scientific questions regarding how to estimate costs and effects of an intervention and how to represent the uncertainty surrounding these estimates. (“Cost consequences: implicit, opaque and anti-scientific”, BMJ.com , March 15, 2005.)
This reflects the belief that it is possible to distinguish between positive and normative questions in the field and that transparent decision-making is supported by doing so. Coast's (2005) online rebuttal is persuasive and worth reprinting at length: … estimation of costs and effects of an intervention is NOT a positive scientific question; it is inherently normative, involving a large number of value judgments about which perspective is important, which costs should be included, how they should be valued and by whom, which effects should be included, how they should be valued and by whom, how time preference should be taken into account and so on. Representation of uncertainty around these estimates may well be closer to a “positive scientific question”, but even there value judgments are being made about the aspects of uncertainty that are important and the appropriate ways in which to take these into account. (“Re: Cost consequences: implicit, opaque and anti-scientific”, BMJ.com , March 23, 2005.)
Although there is little more than anecdotal evidence of health economists' conflicting views on values in science, there are other reasons to believe the field would benefit from greater clarity on this topic. For one, although health economists generally support increasing transparency in their work, there is disagreement over what exactly should be done to achieve this ( Sampson et al., 2019 ). For example, Cohen and Wong (2017) and Padula et al. (2017) dispute whether sharing model code is important to increase transparency. A possible explanation for this disagreement could be different interpretations of transparency's purpose. According to ISPOR-SMDM (the International Society for Pharmacoeconomics and Outcomes Research/Society for Medical Decision-making), transparency serves two purposes: providing a non-quantitative description of the model to those who want to understand it “in a general way” and providing technical information to those who want to evaluate its mathematical and programming details ( Eddy et al., 2012 , p. 844). Furthermore, they comment that "[u]ltimately, what matters is whether a model replicates what occurs in reality” ( Eddy et al., 2012 , p. 845). These claims are inconsistent with the view that transparency in science is important because it is a pre-condition for understanding social value judgments in the research process ( Elliott, 2017 ). In summary, there are reasons to seek greater clarity around the role of values in health economics, particularly in modelling, a major method of inquiry.

Section: 4. Value judgments in health economics modelling: qualitative study

We aimed to explore the role of value judgments in health economics modelling, following the four philosophical arguments described in Sections 1 Introduction , 2 Overcoming science's value-free ideal: overview of the literature , 3 Values in health economics modelling: rationale for inquiry .
Qualitative interviews were conducted with professionals working in health economics modelling in British Columbia (BC), Canada, between February and May, 2019. Ethical approval was obtained from BC's Provincial Research Ethics Platform prior to conducting the interviews (certificate number H18-03694).
Participants were recruited through an academic community group that organizes events for health economists in BC, whose members work in a variety of positions in university, hospital, and institutional research settings. The group's email list was accessed through its organizers and a standard invitation was sent to all members, along with a brief outlining study definitions and discussion topics ( Appendix 1 ). To participate, individuals had to have professional experience in health economics modelling and be fluent in English.
A semi-structured interview guide was developed to collect qualitative data on values and value judgments in health economics modelling ( Appendix 2 ). Details regarding interview questions and use of philosophical terms are described in Appendix 3 . The focus of the current paper is the six questions designed to explore whether decisions in health economics modelling reflect four mechanisms of social values influence described in the philosophical literature (see Sections 1 Introduction , 2 Overcoming science's value-free ideal: overview of the literature , 3 Values in health economics modelling: rationale for inquiry ). This included one question pertinent to the AVLBA (“would there be an example from you own work where a ‘background assumption’ might have introduced social values into the model?), two pertinent to the boundary challenge (“Where scientific criteria might have introduced social values? Where a tradeoff between two scientific criteria was informed by social values?), two pertinent to the AIR (“Where the process for deciding when there was ‘enough evidence’ to make a claim was informed by social values? Where the process for deciding when there was ‘enough evidence’ to take action under uncertainty was informed by social values?”) and one pertinent to the cascade argument (“Where early choices in model construction might have introduced social values?”). The interview guide was reviewed by a philosopher of science and modelling external to the research team and pilot tested (unrecorded) with two individuals with expertise in statistical modelling. Lead author SH conducted all interviews in-person.
Data analysis was performed by SH and GW, who are experienced in health economics modelling and reviewed interview transcripts closely. Following Braun and Clarke's (2006) theoretical approach to thematic analysis, SH developed a list of codes to organize participants' responses to the six questions described in Section 4.3 and identify themes in participants' examples from modelling practice (i.e., data-driven themes within theory-driven categories). N-vivo 11 software was used to facilitate data management and development of coding schemes. SH and GW then reviewed the codes and coded material together. Examples included in the current report were selected and analyzed by SH and the interpretation affirmed by GW (see Appendix 3 for details).
Twenty-two participants were interviewed before close of the data collection period in May, 2019. All participants had expertise in one or more areas of health economics modelling ( Appendix 3 ). This report focuses on their responses to the six questions described in Section 4.3 and data assisting the interpretation of those responses. No names are used and the pronoun ‘they’ is used for all 22 participants (P1–P22) in order to protect their identities. Details that could identify specific models have been removed.
Theorists argue that background assumptions function to “encode” social values in scientific studies ( Longino, 1990 , p.216). Participants detailed a variety of assumptions which introduced social values into health economics models, beginning with the basic assumption that the health intervention modelled is desired by people (P2, P6, P17, P18). Describing models of diagnostic or screening tests, participants remarked that such studies assume that diagnosis is a good thing, even though, in practice, people may not want to be informed of their status (P17, P18), there may be no treatment for the disease (P2), or people may not want to be treated even if treatment is available (P18). Similarly, several participants mentioned that a model's outcome measure reflects assumptions about what is important to people (P5, P9, P12, P16, P22). These comments reflect the uncontroversial point that scientific research topics evince social values, at least to the extent that research topics manifest as independent and dependent variables in models. However, not all participants spoke of this as obvious or could conjure examples of model assumptions introducing social values.
P3 reported that they had once built an economic model of a public health intervention (referred to hereafter as ‘Model X′) with collaborators who believed that implementing the intervention would benefit society; they noted, “you start from the assumption that [the intervention] is good for you … that's why you are trying to give it to people” . Similarly, P4 described working on an economic model which originated with a researcher who favoured funding the intervention, who approached them “saying we need to do work to convince government that they should be investing in and providing free [intervention]" . Recalling the principle that model scope reflects social values ( Rawlins and Culyer, 2004 ), P17 described widening a model's scope to better demonstrate the benefit of an intervention: “We decided to go very complex because … just looking at the up-front cost would not reflect the later benefits of that technology”. Others highlighted that some modelling decisions have an indirect influence on what costs and benefits are included, such as choice of time horizon (P4) and population age-cut off (P3). For example, describing Model X, P3 noted that model results were sensitive to the population age cut-off due to its influence on the scope of costs and benefits included. They explained, “having the cut off of 60 years old, you have more people at risk. So, you're preventing more nasty outcomes [and] probably will have a higher impact on a cost effectiveness” . Additionally, P3 noted, a systematic review had suggested the intervention could have an effect on a secondary adverse outcome among the elderly. P3 expressed doubts about the quality of the evidence on that secondary effect, commenting “ Yeah, don't believe that … from the evidence, I don't think it's there” . However, they conceded that the effect was plausible “ if people are deficient and they are weak” and could potentially translate to benefit “ if you had that small risk at a population level , then, you have an effect”. They described their eventual decision “to not make it all about children but also to include the elder population” as discretionary, remarking “some people would choose not to put that in the model. I put that in the model and, I think, it impacted [the results]." While P3 demonstrated how social value considerations influence choice of population, not all participants had this understanding; P9, for example, said “when you specify the target populations, I think, you don't need to think about the value judgment."
Beyond those influencing model scope, participants spoke of assumptions underlying quantitative features of models, which could influence their results. This included modelling utilities as fixed over time and implementing fixed cycle lengths in Markov models (P1). P1 reflected that these choices are understood to be idealizations, not accurate representations of reality, noting “if you choose a one-year cycle length any events that happen before a year will have to take a year to transpire, which is inaccurate” . P1 further remarked that while they could not predict the downstream consequences of such choices (e.g., on funding decisions and health outcomes), there would be some: “I can't really think through like you know, shooting from the hip … which direction that's going to work but it means that … one kind of treatment has home field advantage in that kind of air quotey way” . Other participants referred to quantitative assumptions that are necessary due to lack of data. Participants highlighted that data may be unavailable for population subgroups of interest, and that results pertaining to those subgroups can only be generated if the model assumes that, for example, disease risk is the same across age groups (P19) or sexual behaviour is generalizable across populations (P21). P19 suggested that using model results without understanding these assumptions could have social consequences, stressing that, for this reason, it is scientists' responsibility to clarify their assumptions. P21's view was that generalizing evidence from one population to another introduces social values, but they emphasized that researchers aim to assess the consequences of such assumptions using sensitivity analysis and other tests of robustness, noting “it introduces social values but we try to be aware of the impact" .
The VFI for science is untenable if it is impossible to meaningfully distinguish scientific from social values ( Douglas, 2016 ). Theorists who challenge the VFI on this basis advance two related arguments: one, scientific criteria may introduce social values, i.e., reflect social value commitments; two, not all scientific criteria can be satisfied at once and social values determine which criteria are privileged. When briefed on these arguments and relevant examples (from Rooney, 1992 ; Elliott and McKaughan, 2014 ; Matthewson and Weisberg, 2009 ), study participants gave additional evidence from health economics supporting each one. Reflecting on whether scientific criteria may introduce social values, several participants engaged with an example regarding sex and gender. P8 reflected that they had never built a model that included gender, only “biological sex”, while P4 said “I can't remember seeing a model that's had anything other than male/female”. P15 remarked that it is difficult to model the effects of sex and gender because datasets tend to include only a dichotomous variable whose significance is unclear: “when there is only an M and an F box on a form or on a checklist or registration, then that is all I have and then gets perpetuated within the analysis … I may try to speculate about whether we're observing impacts of sex or gender given this imperfect measure, but it absolutely carries through into the analysis and results.” P15 emphasized that the influence of social values on their models is mediated by data collection efforts: “I can only analyse what someone has collected, and what someone has chosen to collect directly reflects social values”; the same point was made by P8 and P17. P17 added that, in some models, sex would be a more appropriate variable to include than gender: “if you're simulating risks of a certain disease that's known to be associated with your … sex at birth, then you want to simulate that … I know there's a social value that comes with that gender thing, but then if you're simulating risk, then you should keep that distinction binary” . However, they acknowledged that such models reflect research interests: “It comes back to what you're trying to model".
For some participants, it was the task of analyzing non-binary gender data that raised concerns. Specifically, P12 said the possibility that only a small number of people would identify neither as male or female “makes it a lot more challenging” . For them, there was an apparent tension between the social value of the non-binary conception of gender and the scientific value of working with sufficiently large sample sizes: “I recognise the social value implications of the binary versus non-binary. As an analyst, I didn't know how to handle it, and I don't want to say it's uncomfortable, it's just really hard to know – will treating it as non-binary really influence the results? I don't know, there's a very, very small number, say, in our sample, that were in there."
Reflecting on analytic solutions, P12 mentioned the possibility of excluding data from non-binary people, though they acknowledged that this would have implications. They asked: “do you include, do you exclude that small number of individuals? What does that mean for representativeness?” . P13 voiced a similar position, suggesting it is important to consider non-binary genders but their current modelling methods are not sufficient to represent them: “I believe it's important enough so that when the questionnaire was designed they considered that, but in the end how one can actually analyse it as a distinct group? I think it's too hard.” Other participants pointed to similar dilemmas in modelling practice, where, seemingly, any solution would align with some values but not others. P7 described a dilemma in modelling the effect of ethnicity: “I'm doing this study right now where I use data from three studies … I have good data especially in one of them and not in the two others. So, I have two choices. Either I pull the three together and I let go of ethnicity or I stick with ethnicity and my sample size is now way smaller. My model is way worse in terms of how it will perform. Which one you go with?"
While these examples suggest that sample size- or power - may sometimes be in tension with other values in modelling, P5 pointed to other possible conflicts that could introduce social values. They reflected on the choice of outcome measure in a model of physical rehabilitation, contrasting possibilities from physiotherapy ( “what angle can they bend their arm to now?”) vs. occupational therapy ( “satisfaction with function, you know, ‘which of the following tasks can you do’, and then you come up with a score at the end” ). Describing the choice to model the former, the participant questioned whether the team had valued ease of analysis, musing “I wonder whether it's much easier to build a model when you've just got centimeters or degrees gained in movement than it is … when you've got this kind of composite outcome, like, four points out of a hundred improvement on this weird qualitative measure.” They said they had “made what we thought of as a scientific criteria judgment” because degrees gained in movement was a “robust measure” , but reflected that it “would have had the effect of not including kind of patient experience, patient satisfaction and patient function … ".
Engaging with the idea that not all scientific criteria can be satisfied at once, many participants gave examples of trade-offs in modelling. This included essential trade-offs between a model's simplicity and complexity (P3, P18, P20), simplicity and comprehensiveness (P11), or feasibility and comprehensiveness (P1, P15). P20 remarked that “you have to sacrifice” and choose a simpler, rather than more complicated, model when the latter is “not doable”. To illustrate, they gave the example of choosing between a Markov and a macrosimulation model, saying the former has disadvantages but for the latter "[you] need more information.” P15 reinforced that lack of information may necessitate model simplification, e.g., reducing the number of clinical pathways represented, “cutting them out because we just don't have the information … enough robust data to be able to fuel all of those different pathways” . P1 suggested that feasibility may have the same result: “I could in theory program a model that has 150 different health states through which people move but I don't want to” . Others added that a model's complexity may trade off with its interpretability (P11, P13, P18); in P18's words, “if you go down the complex routes it's not easy to communicate and you might lose people and sort of confuse [them]".
Others reinforced Elliott and McKaughan's (2014) point that a trade-off may be required between accuracy and speed of generating results (P3, P4, P9, P21). For example, P9 described developing a cost-effectiveness model of a drug whose price-per-dose in the relevant setting was not yet available. They noted that to “get a result as soon as possible, you can choose input parameter from elsewhere”, e.g., another setting, but remarked that this model's results were highly sensitive to price-per-dose. For this reason, the participant concluded: “You have to wait to get an exact price per dose from the company … is it really worse to wait to get some input parameters … ?" . However, other participants reflected that waiting is not always possible (P3, P21). In P21's words: "It's all done to try to help policy makers make better decisions and the opportunity cost here is that decisions are happening … without this evidence that you're bringing" .
Speaking of populating parameter values, P1 suggested this process may or may not invoke modellers' social values. They explained that if the only difference between two data sources is the sample size, they would use the value from the larger sample. However, they said, if neither data source is clearly superior (“ for reasons A, B, C it's better to choose this first study … and for reasons D, E and F, I should choose this other” ) then their decision is informed by both “ which … scientific criteria have the higher priority” as well as the expected impact on the results ( “Am I going to be poorly serving a population? Will the downstream implications … be influenced if I choose this group versus that group … ?" ). P3 confirmed that modellers may consider the effect on model results when populating parameter values. Describing Model X, they noted that previous models had not generated subgroup estimates due to lack of evidence: “the models before … didn't do by ethnic groups because, they said, ‘Oh, we don't have data on this group'." . They saw this as problematic because of perceived need, commenting “these groups are more at a disadvantage and they also don't advocate” . For P3, the solution was to consider both the evidence and perceived need when building the model, noting “ I did subgroups by ethnic group and I did the highest risk for those groups" .
P2 also described a modelling decision informed by the predicted effect on model results. In that situation, the available outcome measure was not the one of real interest, but a proxy, “an intermediate outcome between the intervention and the actual hypothesized health impact”. P2 noted that the researchers had questioned whether the proxy measure would be sufficient, “ valuable information for decision-makers” , and raised the possibility of extrapolating from the proxy measure to the outcome of interest. P2 emphasized that extrapolation posed risks, in that it “ introduces a whole lot of uncertainty at each step” and leaves “some question as to how valid the results ultimately are”. Nonetheless, P2 reported that the extrapolation was done despite these risks, noting the decision was “probably influenced” by “the broader team's desire to demonstrate value to stakeholders and faith in that intervention".
The AIR ( Douglas, 2000 , 2009 ) posits that scientists must consider the ethical consequences of error when making claims and taking other action under uncertainty. Many participants affirmed that they consider at least the possibility of error when deciding when to make a claim (P2, P3, P15, P18, P21). For example, P18 described completing a model whose results contradicted all other evidence; they noted it used “slightly different data, slightly different methods” than previous models, but its surprising results prompted a lengthy re-analysis, “a five year period of trying to understand what's wrong in what we've done”. P21 emphasized that the possibility of error is the rationale for sensitivity analysis and robustness checks; this point was reinforced by P9.
Others spoke to considering the consequences of error, which differ across research contexts (P2, P3, P15). P2 said they tend to be “pretty cautious about claims” because of uncertainty, commenting that if their work had a more direct impact on human health, “I would probably start having anxiety attacks”. Conversely, P15 remarked “I'm not looking at policy changes that like could kill people or that would send them to hospital” , so having “some indication” of policy effects is useful “even if we haven't done like the perfect randomized control trial”. Unlike these participants, P16 denied that decisions around claims are informed by social values. Rather, they said decisions were driven by the “purpose of the economic evaluation”, and emphasized a perceived difference between this purpose and that of clinical evaluations, which “comes back to the difference between the estimation and the inference”. Following this rationale, P16 remarked: “if the probability that intervention's cost-effective is only 0.51 then you should go for it, because if you don't then you're preferring that .49 over the 0.51". They acknowledged that this is “ignoring how confident you want to be about the decision” and “ignoring costs of implementation” , but questioned “if that point estimate is your best estimate then why would you not go for it?”. They did not comment on whether such an approach would raise ethical concerns.
Participants described considering the consequences of error when taking other action under uncertainty, such as using administrative data for secondary purposes (P14) and selecting evidence for inclusion in models (P3, P5, P9, P12, P17, P22). P17 and P5 each reported that they had once declined to develop a model because there was too little high-quality evidence to inform it; in P17's words: “we made the decision not to simulate anything in that area because the lack of evidence would just produce results that were just fantasy”. Many participants emphasized that certain techniques can be used to highlight uncertainty (e.g., P1: cost-effectiveness acceptability curves), bypass the need for observed data (e.g., P12: threshold analysis; P22: uniform distributions for modelling inputs with considerable uncertainty), and estimate the value of information (P6, P22). In selecting parameter values and planning sensitivity analysis, P3 described considering the probability of harm versus benefit. Referring to Model X, which they said examined a lost-cost, low-risk public health intervention with a potential to benefit underserved groups, P3 remarked “I could play around with the numbers a little bit and it's not like I'm going to kill anyone".
One challenge to the VFI highlights that early value judgments in research have a downstream effect on study results ( Elliott and McKaughan, 2014 ; Parker and Winsberg, 2018 ). No participants denied this was true, though some reinforced the perception that such influences are considered “epistemically uninteresting” ( Elliott and McKaughan, 2009 , p. 600). P20, for example, affirmed that “every decision that you make when you're trying to define your question definitely affects your conclusion” but suggested it is normal for the research goal to determine the research results; they remarked: “something that I chose to do is affecting my conclusion, but this is how it's supposed to be”. However, other participants highlighted modelling decisions with potentially problematic cascade effects. For example, P20 mentioned that “different model structure can affect the cost-effectiveness result” , while P6 worried that choice of predictor variables could result in discrimination. Specifically, P6 said their research team had agreed that age and sex should not be included as predictor variables in CEA to “escape from the obvious mistake of [concluding] this drug is cost effective in men and is not cost effective in women” , i.e., to ensure equal access to therapy. However, they mentioned that it was unclear whether age and sex should be also be left out of clinical prediction models. They expressed doubt over this- “Do we have to have prediction tools that are ignorant of people's age and sex?"- but acknowledged that if clinical prediction were later used to develop decision rules around treatment initiation, this would have the same effect on treatment access: " … they are becoming co-efficient and go into the regression model. You're just hiding that discrimination but it's there."
Parker and Winsberg (2018) highlight that not only do estimates in a given model reflect value judgments, i.e., choices driven by the model's purpose and priorities, but that model estimates are frequently incorporated into subsequent models. This resonated with P22. Describing a model they developed years ago that had attracted considerable attention, they noted that speed of generating results had been a priority that had influenced model development, including its assumptions. P22 remarked that those assumptions continued to be incorporated into other models, which for them raised questions: “ … me and another guy made it up in a week. That and the assumptions, literally in a week. A big week, because we were on a deadline … I still see people today using those same assumptions that we made in a week, today, in like [distant country]. Are they the right assumptions? I don't know. They seem alright, but like, you see how that cascade happens."

Section: 5. Conclusions

To date, little work in health economics has engaged with philosophy's values in science literature, and some health economists maintain that aspects of modelling are value-free. This paper has aimed to examine philosophical arguments that social values are influential throughout scientific inquiry, and apply them to gain clarity around where value judgments occur in health economics modelling. Gaining this clarity is increasingly important, given efforts to involve patients and the public more meaningfully in health research ( Canadian Institutes for Health Research, 2014 ). Health economists have a new reason to reflect on the influence of social values throughout the modelling process, and identify the specific decision-points at which patient and public values could potentially be incorporated. This knowledge would help concretize discussions around when and how to incorporate patient and public values into health economics research. Knowing where value judgments take place is necessary to decide how to inform them.
To manage values in science, Elliott (2017) advises that value judgments should be made as transparent as possible, noting that one purpose of transparent reporting is to allow others to identify how research supports or is influenced by particular values (pp. 10, 14). In this context, our study points to three potential ways to improve best practice guidelines in health economics, including the ISPOR-SMDM guidelines on model transparency and validation ( Eddy et al., 2012 ). First, while the ISPOR-SMDM guidelines endorse transparent reporting of model features and assumptions, the purpose of transparency is framed as being to help readers understand the model in a general way and support replication efforts ( Eddy et al., 2012 , p. 844). While these are important aims, it should be acknowledged that transparency also supports the goal of identifying and critiquing the values that influenced a model's development. Second, in order to make value influences as transparent as possible, it would be helpful for guidelines to use explicit language that indicates that many modelling decisions in health economics involve value judgments. Our findings reinforce that this includes decisions around the scope of costs and benefits ( Rawlins and Culyer, 2004 ), but also other decisions (e.g., population age-cut off, time horizon), that indirectly influence model results and may align or conflict with different values. Our participants also described a variety of value-laden assumptions in modelling, including assumptions about what patients and the public desire and what sorts of losses and distortions of information are tolerable. These assumptions reflect a variety of specific decisions that extend from the research question, all of which invoke social values around what is most important to know. These assumptions are somewhat different from the “assumptions about uncertain elements” addressed in the ISPOR-SMDM guidelines ( Eddy et al., 2012 , p. 848), which reflect social and ethical values around the perceived need for certainty and the seriousness of error. In the future, the current study may inform a more systematic analysis of modelling decisions and their relationship to social values, which may be helpful in developing future modelling guidelines.
Beyond the above, we think it would be beneficial for guidelines to recommend that model developers report their relevant values. Such a recommendation would be similar to advice given to qualitative researchers, who are encouraged to make a statement on how their position and perspective shaped the research design and interpretation of results ( Grbich, 2013 , p. 11). Our study suggests that researcher values can influence health economics models, such as Model X, whose development decisions were informed in part by their predicted effect on results and reflected researcher support for an intervention. It is outside the scope of this paper to argue for or against developing models that emphasize the potential benefit of an intervention. However, we might anticipate that patient and public groups, as well as innovators and researchers, will sometimes wish to do so. In light of this, it would be a small but important step for modelling guidelines to recommend that research teams make a statement on their perspectives, values, and priorities. This would also present a way to evaluate efforts to incorporate patient and public values into research, by observing the diversity of those statements over time.
In addition to transparency, Elliott (2017) recommends two other principles to help manage values in science: representativeness (value judgments should reflect social and ethical priorities) and engagement (value judgments should be scrutinized by diverse communities of stakeholders). Elliott's three principles, and the insights of our study participants, should inform the discourse in patient-oriented health economics.

Section: Funding source and role

This study was funded by the BC SUPPORT Unit Health Economics and Simulation Modelling Methods Cluster as part of the Canadian Institute for Health Research's Strategy for Patient-Oriented Research. The funder had no role in the design or conduct of the study nor influence on its results.

Section: CRediT authorship contribution statement

Stephanie Harvard: Conceptualization, Methodology, Data curation, Formal analysis, Writing - original draft. Gregory R. Werker: Formal analysis, Writing - review & editing. Diego S. Silva: Writing - review & editing, Supervision, Methodology.

Section: Acknowledgments

The authors gratefully acknowledge Eric Winsberg, University of South Florida, for his review of the interview guide used in this study; Martine August, University of Waterloo , for her comments on a previous draft of this manuscript; and Alison McLean and Don Grant, patient partners in research. The authors acknowledge financial support for this project from the BC SUPPORT Unit Health Economics and Simulation Modelling (HESM) Methods Cluster (Award Number: HESM-002), which is part of British Columbia's Academic Health Science Network. The BC SUPPORT Unit receives funding from the Canadian Institutes of Health Research and the Michael Smith Foundation for Health Research .
