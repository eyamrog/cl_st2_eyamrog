{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "810d9f10-ec19-4b09-8f90-e983e460b319",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://laelgelcpublic.s3.sa-east-1.amazonaws.com/lael_50_years_narrow_white.png.no_years.400px_96dpi.png\" width=\"300\" alt=\"LAEL 50 years logo\">\n",
    "<h3>APPLIED LINGUISTICS GRADUATE PROGRAMME (LAEL)</h3>\n",
    "</center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c2c96-2fc3-4a1a-995b-c388036a2a15",
   "metadata": {},
   "source": [
    "# Corpus Linguistics - Study 2 - Phase 3.1 - eyamrog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b2a221-54af-4c1c-a7fd-c80cda278dcc",
   "metadata": {},
   "source": [
    "The aim of this phase is to develop solutions to scrape text from each journal's article HTML page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a685d8b0-7715-45a6-9489-2d3db9b346c8",
   "metadata": {},
   "source": [
    "## Required Python packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b526a82b-22a6-4d28-afa2-eb6e7bcca4fb",
   "metadata": {},
   "source": [
    "- beautifulsoup4\n",
    "- lxml\n",
    "- pandas\n",
    "- requests\n",
    "- selenium\n",
    "- tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa922755-c4d6-4008-9aad-d35e33b18ed7",
   "metadata": {},
   "source": [
    "## Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "182dbbab-65ee-4695-a418-3e9d9d8599e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3ad412-2346-43d3-8607-08705487f1b2",
   "metadata": {},
   "source": [
    "## Define input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7657874-dc0a-4a7b-ae4b-bb3ca3fd7fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = 'cl_st2_ph2_eyamrog'\n",
    "output_directory = 'cl_st2_ph31_eyamrog'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32b4a24-8a37-4302-9c3e-790aa6dee914",
   "metadata": {},
   "source": [
    "## Create output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dce68e3-44e7-4a7e-bc25-73c21ca6fbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the output directory already exists. If it does, do nothing. If it doesn't exist, create it.\n",
    "if os.path.exists(output_directory):\n",
    "    print('Output directory already exists.')\n",
    "else:\n",
    "    try:\n",
    "        os.makedirs(output_directory)\n",
    "        print('Output directory successfully created.')\n",
    "    except OSError as e:\n",
    "        print('Failed to create the directory:', e)\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e32a21-1ea8-4563-aa6f-ee00cceeb3f6",
   "metadata": {},
   "source": [
    "### Create output subdirectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02ec67dc-d7bd-4dc3-85e3-611c633ef03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory(path):\n",
    "    \"\"\"Creates a subdirectory if it doesn't exist.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        try:\n",
    "            os.makedirs(path)\n",
    "            print(f\"Successfully created the directory: {path}\")\n",
    "        except OSError as e:\n",
    "            print(f\"Failed to create the {path} directory: {e}\")\n",
    "            sys.exit(1)\n",
    "    else:\n",
    "        print(f\"Directory already exists: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cace8b39-99d7-4407-8a1e-768144c6d575",
   "metadata": {},
   "source": [
    "## Set up logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8340bd82-f602-48a1-b8c3-61e52e94f04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_filename = f\"{output_directory}/{output_directory}.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24b5f2ab-69bd-49f5-97d3-19c08d5a997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    filename=log_filename\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bae7cc4-88c6-474d-bf2f-810ec7375c7a",
   "metadata": {},
   "source": [
    "## Health Sciences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18c6376-2357-4a21-b421-a159999c0ed5",
   "metadata": {},
   "source": [
    "### [Nature Medicine](https://www.nature.com/nm/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1cfa35-9e76-4a21-8815-f28fbee0dfb7",
   "metadata": {},
   "source": [
    "#### Create output subdirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d1275c-086c-4065-9978-68f6da4bba84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Nature Medicine'\n",
    "id = 'natm'\n",
    "path = os.path.join(output_directory, id)\n",
    "create_directory(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546fd353-4b6a-4cbe-937a-69fdadaf30ca",
   "metadata": {},
   "source": [
    "#### Import the data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414b65be-8370-4fe0-965f-bbe74abe50d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nature_medicine_open_access = pd.read_json(f\"{input_directory}/nature_medicine_open_access.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c455386c-ff89-49c7-aadf-bba48f5f8121",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nature_medicine_open_access['Published'] = pd.to_datetime(df_nature_medicine_open_access['Published'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162a394d-c6a1-4871-8fb7-4f15a436c6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nature_medicine_open_access = df_nature_medicine_open_access.loc[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea098561-be32-41c3-a73d-639c98875412",
   "metadata": {},
   "outputs": [],
   "source": [
    "natm_urls = df_nature_medicine_open_access['URL'].tolist()\n",
    "natm_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef8552a-9189-42b2-b5b4-a562618e774c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(df, path):\n",
    "    \"\"\"Extracts text from HTML files and saves as text files.\"\"\"\n",
    "\n",
    "    for article_id in df['ID']:\n",
    "        html_file = os.path.join(path, f\"{article_id}.html\")\n",
    "        txt_file = os.path.join(path, f\"{article_id}.txt\")\n",
    "\n",
    "        # Check if the HTML file exists\n",
    "        if not os.path.exists(html_file):\n",
    "            logging.error(f\"Skipping {html_file}: File not found\")\n",
    "            continue\n",
    "\n",
    "        # Read HTML content\n",
    "        with open(html_file, 'r', encoding='utf-8') as file:\n",
    "            soup = BeautifulSoup(file, 'lxml')\n",
    "\n",
    "        # Initialise text variable\n",
    "        text = ''\n",
    "\n",
    "        # Web Scraping - Begin\n",
    "\n",
    "        # Extract the 'Title'\n",
    "        title = soup.find('h1', class_='c-article-title')\n",
    "        if title:\n",
    "            title_text = ' '.join(title.get_text(' ', strip=True).split())\n",
    "            text += f\"Title: {title_text}\\n\\n\"\n",
    "\n",
    "        # Extract the 'Abstract'\n",
    "        abstract_section = soup.find('div', id='Abs1-section')\n",
    "        if abstract_section:\n",
    "            abstract_h2_title = abstract_section.find('h2', class_='c-article-section__title')\n",
    "            if abstract_h2_title:\n",
    "                abstract_h2_title_text = ' '.join(abstract_h2_title.get_text(' ', strip=True).split())\n",
    "                text += f\"\\nAbstract: {abstract_h2_title_text}\\n\\n\"\n",
    "            abstract_content = abstract_section.find('div', class_='c-article-section__content')\n",
    "            if abstract_content:\n",
    "                for paragraph in abstract_content.find_all('p', recursive=False):\n",
    "                    # Remove <sup> elements containing references\n",
    "                    for sup_tag in paragraph.find_all('sup'):\n",
    "                        sup_tag.decompose()\n",
    "                    # Extract the paragraph text\n",
    "                    paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                    text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "        # Extract the 'main content'\n",
    "        main_content = soup.find('div', class_='main-content')\n",
    "        if main_content:\n",
    "            for main_content_section in main_content.find_all('section', recursive=False):\n",
    "                # Extract sections\n",
    "                for section in main_content_section.find_all('div', class_='c-article-section', recursive=False):\n",
    "                    # Extract section title\n",
    "                    section_h2_title = section.find('h2')\n",
    "                    if section_h2_title:\n",
    "                        section_h2_title_text = ' '.join(section_h2_title.get_text(' ', strip=True).split())\n",
    "                        text += f\"\\nSection: {section_h2_title_text}\\n\\n\"\n",
    "                    # Extract section content\n",
    "                    section_content = section.find('div', class_='c-article-section__content')\n",
    "                    if section_content:\n",
    "                        #for content in section_content.find_all(['h3', 'h4', 'h5', 'p'], recursive=False):\n",
    "                        for content in section_content.find_all(['p'], recursive=False):\n",
    "                            # Remove <sup> elements containing references\n",
    "                            for sup_tag in content.find_all('sup'):\n",
    "                                sup_tag.decompose()\n",
    "                            # Extract the content text\n",
    "                            content_text = ' '.join(content.get_text(' ', strip=True).split())\n",
    "                            text += f\"{content_text}\\n\"\n",
    "\n",
    "        # Extract the 'u-mt-32'\n",
    "        u_mt_32 = soup.find('div', class_='u-mt-32')\n",
    "        if u_mt_32:\n",
    "            data_availability_section = u_mt_32.find('section', attrs={'data-title': 'Data availability'})\n",
    "            if data_availability_section:\n",
    "                for section in data_availability_section.find_all('div', class_='c-article-section', recursive=False):\n",
    "                    # Extract section title\n",
    "                    section_h2_title = section.find('h2')\n",
    "                    if section_h2_title:\n",
    "                        section_h2_title_text = ' '.join(section_h2_title.get_text(' ', strip=True).split())\n",
    "                        text += f\"\\nSection: {section_h2_title_text}\\n\\n\"\n",
    "                    # Extract section content\n",
    "                    section_content = section.find('div', class_='c-article-section__content')\n",
    "                    if section_content:\n",
    "                        for content in section_content.find_all('p', recursive=False):\n",
    "                            # Remove <sup> elements containing references\n",
    "                            for sup_tag in content.find_all('sup'):\n",
    "                                sup_tag.decompose()\n",
    "                            # Extract the content text\n",
    "                            content_text = ' '.join(content.get_text(' ', strip=True).split())\n",
    "                            text += f\"{content_text}\\n\"\n",
    "\n",
    "            acknowledgements_section = u_mt_32.find('section', attrs={'data-title': 'Acknowledgements'})\n",
    "            if acknowledgements_section:\n",
    "                for section in acknowledgements_section.find_all('div', class_='c-article-section', recursive=False):\n",
    "                    # Extract section title\n",
    "                    section_h2_title = section.find('h2')\n",
    "                    if section_h2_title:\n",
    "                        section_h2_title_text = ' '.join(section_h2_title.get_text(' ', strip=True).split())\n",
    "                        text += f\"\\nSection: {section_h2_title_text}\\n\\n\"\n",
    "                    # Extract section content\n",
    "                    section_content = section.find('div', class_='c-article-section__content')\n",
    "                    if section_content:\n",
    "                        for content in section_content.find_all('p', recursive=False):\n",
    "                            # Remove <sup> elements containing references\n",
    "                            for sup_tag in content.find_all('sup'):\n",
    "                                sup_tag.decompose()\n",
    "                            # Extract the content text\n",
    "                            content_text = ' '.join(content.get_text(' ', strip=True).split())\n",
    "                            text += f\"{content_text}\\n\"\n",
    "\n",
    "        # Web Scraping - End\n",
    "\n",
    "        # Save text to a text file\n",
    "        with open(txt_file, 'w', encoding='utf-8', newline='\\n') as file:\n",
    "            file.write(text)\n",
    "\n",
    "        logging.info(f\"Saved text for {article_id} to {txt_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bfcc33-b3ef-4b22-8a45-00870d1ddfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text(df_nature_medicine_open_access, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e7cd58-2f2b-4d00-812e-17907943a65a",
   "metadata": {},
   "source": [
    "### [Annual Review of Public Health](https://www.annualreviews.org/content/journals/publhealth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fc5039-bb6d-4316-87af-b2592b894f55",
   "metadata": {},
   "source": [
    "#### Create output subdirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f54b2768-6e7a-4b45-a958-6aeba669836f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: cl_st2_ph31_eyamrog\\arph\n"
     ]
    }
   ],
   "source": [
    "# 'Annual Review of Public Health'\n",
    "id = 'arph'\n",
    "path = os.path.join(output_directory, id)\n",
    "create_directory(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ab4b87-ceb6-430f-ab20-6c360e7ab344",
   "metadata": {},
   "source": [
    "#### Import the data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eaaf49fe-13bb-47c4-9f44-458bea3f8eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar_public_health = pd.read_json(f\"{input_directory}/ar_public_health.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c072f94-3cf8-4a13-9101-aa28863dd0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar_public_health['Published'] = pd.to_datetime(df_ar_public_health['Published'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1abe9697-6d79-40b0-9c84-4528549ff067",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar_public_health = df_ar_public_health.loc[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f727d9c0-5a01-4120-909c-8ec14ddddbf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.annualreviews.org/content/journals/10.1146/annurev-publhealth-121019-053834',\n",
       " 'https://www.annualreviews.org/content/journals/10.1146/annurev-publhealth-051920-114020',\n",
       " 'https://www.annualreviews.org/content/journals/10.1146/annurev-publhealth-012420-105104',\n",
       " 'https://www.annualreviews.org/content/journals/10.1146/annurev-publhealth-051920-110928',\n",
       " 'https://www.annualreviews.org/content/journals/10.1146/annurev-publhealth-060220-042648']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arph_urls = df_ar_public_health['URL'].tolist()\n",
    "arph_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09d70901-70a7-4cfe-b291-55136d2256a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(df, path):\n",
    "    \"\"\"Extracts text from HTML files and saves as text files.\"\"\"\n",
    "\n",
    "    for article_id in df['ID']:\n",
    "        html_file = os.path.join(path, f\"{article_id}.html\")\n",
    "        txt_file = os.path.join(path, f\"{article_id}.txt\")\n",
    "\n",
    "        # Check if the HTML file exists\n",
    "        if not os.path.exists(html_file):\n",
    "            logging.error(f\"Skipping {html_file}: File not found\")\n",
    "            continue\n",
    "\n",
    "        # Read HTML content\n",
    "        with open(html_file, 'r', encoding='utf-8') as file:\n",
    "            soup = BeautifulSoup(file, 'lxml')\n",
    "\n",
    "        # Initialise text variable\n",
    "        text = ''\n",
    "\n",
    "        # Extract the 'Title'\n",
    "        title_tag = soup.find('span', class_='article-title')\n",
    "        if title_tag:\n",
    "            title = ' '.join(title_tag.get_text(' ', strip=True).split())\n",
    "            text += f\"Title: {title}\\n\\n\"\n",
    "\n",
    "        # Extract 'article sections'\n",
    "        for section in soup.find_all('div', class_='articleSection'): # Finds all 'div.articleSection' elements (both top-level and nested).\n",
    "            if not section.find_parent('div', class_='articleSection'): # Keeps only the top-level sections because it filters out nested ones by checking if the 'div.articleSection' has a parent that is also 'div.articleSection'\n",
    "                \n",
    "                # Extract section title\n",
    "                section_title_tag = section.find('div', class_='tl-main-part title')\n",
    "                if section_title_tag:\n",
    "                    section_title = ' '.join(section_title_tag.get_text(' ', strip=True).split())\n",
    "                    text += f\"\\nSection: {section_title}\\n\\n\"\n",
    "\n",
    "                # Extract paragraphs (only from top-level sections)\n",
    "                for paragraph in section.find_all('p'):\n",
    "                    paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                    text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "                # Extract subsections\n",
    "                for subsection in section.find_all('div', recursive=False):\n",
    "                    #label_tag = subsection.find('span', class_='label')\n",
    "                    #if label_tag:\n",
    "                    #    label_text = ' '.join(label_tag.get_text(' ', strip=True).split())\n",
    "                    #    text += f\"\\nSection: {label_text} \"\n",
    "                    #subsection_tag = subsection.find('span', class_='tl-lowest-section')\n",
    "                    #if subsection_tag:\n",
    "                    #    subsection_title = ' '.join(subsection_tag.get_text(' ', strip=True).split())\n",
    "                    #    text += f\"{subsection_title}\\n\\n\"\n",
    "                    for paragraph in subsection.find_all('p'):\n",
    "                        paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                        text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "        # Save text to a text file\n",
    "        with open(txt_file, 'w', encoding='utf-8', newline='\\n') as file:\n",
    "            file.write(text)\n",
    "\n",
    "        logging.info(f\"Saved text for {article_id} to {txt_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7272b3ab-2ad5-42f8-bb2e-41851c444a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text(df_ar_public_health, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f5f473-3883-4b0e-914e-58419006c98f",
   "metadata": {},
   "source": [
    "### [Lancet Public Health](https://www.thelancet.com/journals/lanpub/home)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ab6f0f-e159-4db4-a8ae-ca8a741041c4",
   "metadata": {},
   "source": [
    "#### Create output subdirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3779ce-408d-4f0b-b191-85b2628727aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Lancet Public Health'\n",
    "id = 'laph'\n",
    "path = os.path.join(output_directory, id)\n",
    "create_directory(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df40946-c5ca-416b-afe4-c9006e54295b",
   "metadata": {},
   "source": [
    "#### Import the data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce33fb27-f04b-4436-b98b-2455158768dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lancet_public_health_open_access = pd.read_json(f\"{input_directory}/lancet_public_health_open_access.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939146c7-58a0-4797-b477-ac8daee1130d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lancet_public_health_open_access['Published'] = pd.to_datetime(df_lancet_public_health_open_access['Published'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ea8d2b-0047-4402-9b73-6888f6c647ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lancet_public_health_open_access = df_lancet_public_health_open_access.loc[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cef33c-4647-4103-a676-9ff391d397d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "laph_urls = df_lancet_public_health_open_access['URL'].tolist()\n",
    "laph_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd2c75d-c2ce-43cc-848f-84d466688fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(df, path):\n",
    "    \"\"\"Extracts text from HTML files and saves as text files.\"\"\"\n",
    "\n",
    "    for article_id in df['ID']:\n",
    "        html_file = os.path.join(path, f\"{article_id}.html\")\n",
    "        txt_file = os.path.join(path, f\"{article_id}.txt\")\n",
    "\n",
    "        # Check if the HTML file exists\n",
    "        if not os.path.exists(html_file):\n",
    "            logging.error(f\"Skipping {html_file}: File not found\")\n",
    "            continue\n",
    "\n",
    "        # Read HTML content\n",
    "        with open(html_file, 'r', encoding='utf-8') as file:\n",
    "            soup = BeautifulSoup(file, 'lxml')\n",
    "\n",
    "        # Initialise text variable\n",
    "        text = ''\n",
    "\n",
    "        # Extract the 'Title'\n",
    "        title_tag = soup.find('h1', property='name')\n",
    "        if title_tag:\n",
    "            title = ' '.join(title_tag.get_text(' ', strip=True).split())\n",
    "            text += f\"Title: {title}\\n\\n\"\n",
    "\n",
    "        # Extract the 'Abstract'\n",
    "        abstract_section = soup.find('section', property='abstract')\n",
    "        if abstract_section:\n",
    "            abstract_tag = abstract_section.find('h2', property='name')\n",
    "            if abstract_tag:\n",
    "                abstract = ' '.join(abstract_tag.get_text(' ', strip=True).split())\n",
    "                text += f\"\\nAbstract: {abstract}\\n\\n\"\n",
    "\n",
    "            for section_h3 in abstract_section.find_all('section', recursive=False):\n",
    "                section_h3_title_tag = section_h3.find('h3')\n",
    "                if section_h3_title_tag:\n",
    "                    section_h3_title = ' '.join(section_h3_title_tag.get_text(' ', strip=True).split())\n",
    "                    text += f\"\\nSection: {section_h3_title}\\n\\n\"\n",
    "\n",
    "                # Extract paragraphs within each section\n",
    "                paragraphs = section_h3.find_all('div', role='paragraph', recursive=False)\n",
    "                for paragraph in paragraphs:\n",
    "                    # Remove reference citations embedded in <span> tags\n",
    "                    for ref_tag in paragraph.find_all('span', class_='dropBlock reference-citations'):\n",
    "                        ref_tag.decompose()\n",
    "\n",
    "                    # Extract the paragraph text\n",
    "                    paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                    text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "        # Extract the 'article body'\n",
    "        body_section = soup.find('section', property='articleBody')\n",
    "        if body_section:\n",
    "            body_section_core_container = body_section.find('div', class_='core-container')\n",
    "            if body_section_core_container:\n",
    "                # Extract sectioned content\n",
    "                for section_h2 in body_section_core_container.find_all('section', recursive=False):\n",
    "                    section_text = ''  # Reset for each section\n",
    "\n",
    "                    # Extract section title (h2)\n",
    "                    section_h2_title_tag = section_h2.find('h2')\n",
    "                    if section_h2_title_tag:\n",
    "                        section_h2_title = ' '.join(section_h2_title_tag.get_text(' ', strip=True).split())\n",
    "                        section_text += f\"\\nSection: {section_h2_title}\\n\\n\"\n",
    "                    \n",
    "                    # Extract h2 paragraphs, if there are any\n",
    "                    paragraphs = section_h2.find_all('div', role='paragraph', recursive=False)\n",
    "                    for paragraph in paragraphs:\n",
    "                        # Remove nested paragraphs in the paragraph to drop the paragraphs in the 'Research in context' box\n",
    "                        for nested_paragraph in paragraph.find_all('div', role='paragraph'):\n",
    "                            nested_paragraph.decompose()\n",
    "                        # Remove reference citations embedded in <span> tags\n",
    "                        for ref_tag in paragraph.find_all('span', class_='dropBlock reference-citations'):\n",
    "                            ref_tag.decompose()\n",
    "\n",
    "                        # Extract the paragraph text\n",
    "                        paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                        section_text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "                    for section_h3 in section_h2.find_all('section'):\n",
    "                        ## Extract subsection title (h3)\n",
    "                        #section_h3_title_tag = section_h3.find('h3')\n",
    "                        #if section_h3_title_tag:\n",
    "                        #    section_h3_title = ' '.join(section_h3_title_tag.get_text(' ', strip=True).split())\n",
    "                        #    section_text += f\"\\nSection: {section_h3_title}\\n\\n\"\n",
    "\n",
    "                        # Extract h3 paragraphs\n",
    "                        paragraphs = section_h3.find_all('div', role='paragraph', recursive=False)\n",
    "                        for paragraph in paragraphs:\n",
    "                            # Remove reference citations embedded in <span> tags\n",
    "                            for ref_tag in paragraph.find_all('span', class_='dropBlock reference-citations'):\n",
    "                                ref_tag.decompose()\n",
    "\n",
    "                            # Extract the paragraph text\n",
    "                            paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                            section_text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "                        for section_h4 in section_h3.find_all('section'):\n",
    "                            ## Extract subsection title (h4)\n",
    "                            #section_h4_title_tag = section_h4.find('h4')\n",
    "                            #if section_h4_title_tag:\n",
    "                            #    section_h4_title = ' '.join(section_h4_title_tag.get_text(' ', strip=True).split())\n",
    "                            #    section_text += f\"\\nSection: {section_h4_title}\\n\\n\"\n",
    "\n",
    "                            # Extract h4 paragraphs\n",
    "                            paragraphs = section_h4.find_all('div', role='paragraph', recursive=False)\n",
    "                            for paragraph in paragraphs:\n",
    "                                # Remove reference citations embedded in <span> tags\n",
    "                                for ref_tag in paragraph.find_all('span', class_='dropBlock reference-citations'):\n",
    "                                    ref_tag.decompose()\n",
    "\n",
    "                                # Extract the paragraph text\n",
    "                                paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                                section_text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "                            for section_h5 in section_h4.find_all('section'):\n",
    "                                ## Extract subsection title (h5)\n",
    "                                #section_h5_title_tag = section_h5.find('h5')\n",
    "                                #if section_h5_title_tag:\n",
    "                                #    section_h5_title = ' '.join(section_h5_title_tag.get_text(' ', strip=True).split())\n",
    "                                #    section_text += f\"\\nSection: {section_h5_title}\\n\\n\"\n",
    "\n",
    "                                # Extract h5 paragraphs\n",
    "                                paragraphs = section_h5.find_all('div', role='paragraph', recursive=False)\n",
    "                                for paragraph in paragraphs:\n",
    "                                    # Remove reference citations embedded in <span> tags\n",
    "                                    for ref_tag in paragraph.find_all('span', class_='dropBlock reference-citations'):\n",
    "                                        ref_tag.decompose()\n",
    "\n",
    "                                    # Extract the paragraph text\n",
    "                                    paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                                    section_text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "                    text += section_text  # Append structured section text\n",
    "\n",
    "        # Save text to a text file\n",
    "        with open(txt_file, 'w', encoding='utf-8', newline='\\n') as file:\n",
    "            file.write(text)\n",
    "\n",
    "        logging.info(f\"Saved text for {article_id} to {txt_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4520ca4a-f4de-4da1-8eac-018257746589",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text(df_lancet_public_health_open_access, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f194ad6f-39a0-4d59-be27-eb2319a43367",
   "metadata": {},
   "source": [
    "### [New England Journal of Medicine](https://www.nejm.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53333b81-e126-48f3-ab64-50f726a4d36e",
   "metadata": {},
   "source": [
    "#### Create output subdirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5591cc98-dc8a-468f-bcc6-cdad8921d872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'New England Journal of Medicine'\n",
    "id = 'nejm'\n",
    "path = os.path.join(output_directory, id)\n",
    "create_directory(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7de4569-b600-442d-b154-c9e01944fb98",
   "metadata": {},
   "source": [
    "#### Import the data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee3cefb-8c5f-46b1-abce-dff7e0d78875",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_england_journal_of_medicine_open_access = pd.read_json(f\"{input_directory}/new_england_journal_of_medicine_open_access.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4d264d-ef90-4f01-b817-98a9b4650485",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_england_journal_of_medicine_open_access['Published'] = pd.to_datetime(df_new_england_journal_of_medicine_open_access['Published'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d8f249-0057-47f3-b207-271509c2e458",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_england_journal_of_medicine_open_access = df_new_england_journal_of_medicine_open_access.loc[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b24162-e1aa-49d9-82b0-9b5b83c875e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nejm_urls = df_new_england_journal_of_medicine_open_access['URL'].tolist()\n",
    "nejm_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a8de94-c657-4c33-85d4-d8c5add9d2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(df, path):\n",
    "    \"\"\"Extracts text from HTML files and saves as text files.\"\"\"\n",
    "\n",
    "    for article_id in df['ID']:\n",
    "        html_file = os.path.join(path, f\"{article_id}.html\")\n",
    "        txt_file = os.path.join(path, f\"{article_id}.txt\")\n",
    "\n",
    "        # Check if the HTML file exists\n",
    "        if not os.path.exists(html_file):\n",
    "            logging.error(f\"Skipping {html_file}: File not found\")\n",
    "            continue\n",
    "\n",
    "        # Read HTML content\n",
    "        with open(html_file, 'r', encoding='utf-8') as file:\n",
    "            soup = BeautifulSoup(file, 'lxml')\n",
    "\n",
    "        # Initialise text variable\n",
    "        text = ''\n",
    "\n",
    "        # Extract the 'Title'\n",
    "        title_tag = soup.find('h1', property='name')\n",
    "        if title_tag:\n",
    "            title = ' '.join(title_tag.get_text(' ', strip=True).split())\n",
    "            text += f\"Title: {title}\\n\\n\"\n",
    "\n",
    "        # Extract the 'Abstract'\n",
    "        abstract_section = soup.find('section', property='abstract')\n",
    "        if abstract_section:\n",
    "            abstract_tag = abstract_section.find('h2', property='name')\n",
    "            if abstract_tag:\n",
    "                abstract = ' '.join(abstract_tag.get_text(' ', strip=True).split())\n",
    "                text += f\"\\nAbstract: {abstract}\\n\\n\"\n",
    "\n",
    "            for section_h3 in abstract_section.find_all('section', recursive=False):\n",
    "                section_h3_title_tag = section_h3.find('h3')\n",
    "                if section_h3_title_tag:\n",
    "                    section_h3_title = ' '.join(section_h3_title_tag.get_text(' ', strip=True).split())\n",
    "                    text += f\"\\nSection: {section_h3_title}\\n\\n\"\n",
    "\n",
    "                # Extract paragraphs within each section\n",
    "                paragraphs = section_h3.find_all('div', role='paragraph', recursive=False)\n",
    "                for paragraph in paragraphs:\n",
    "                    # Remove reference citations embedded in <span> tags\n",
    "                    for sup_tag in paragraph.find_all('sup'):\n",
    "                        sup_tag.decompose()\n",
    "\n",
    "                    # Extract the paragraph text\n",
    "                    paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                    text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "        # Extract the article body\n",
    "        body_section = soup.find('section', property='articleBody')\n",
    "        if body_section:\n",
    "            body_section_core_container = body_section.find('div', class_='core-container')\n",
    "            if body_section_core_container:\n",
    "                # Extract the initial paragraphs that precede the first section (introduction)\n",
    "                text += f\"\\nIntroduction\\n\\n\" # Insert the 'Introduction' title\n",
    "                paragraphs = body_section_core_container.find_all('div', role='paragraph', recursive=False) # Prevents nested extraction\n",
    "                for paragraph in paragraphs:\n",
    "                    # Remove reference citations embedded in <span> tags\n",
    "                    for sup_tag in paragraph.find_all('sup'):\n",
    "                        sup_tag.decompose()\n",
    "\n",
    "                    # Extract the paragraph text\n",
    "                    paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                    text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "                # Extract sectioned content\n",
    "                for section_h2 in body_section_core_container.find_all('section', recursive=False):\n",
    "                    section_text = ''  # Reset for each section\n",
    "\n",
    "                    # Extract section title (h2)\n",
    "                    section_h2_title_tag = section_h2.find('h2')\n",
    "                    if section_h2_title_tag:\n",
    "                        section_h2_title = ' '.join(section_h2_title_tag.get_text(' ', strip=True).split())\n",
    "                        section_text += f\"\\nSection: {section_h2_title}\\n\\n\"\n",
    "                    \n",
    "                    # Extract h2 paragraphs, if there are any\n",
    "                    paragraphs = section_h2.find_all('div', role='paragraph', recursive=False)\n",
    "                    for paragraph in paragraphs:\n",
    "                        # Remove reference citations embedded in <span> tags\n",
    "                        for sup_tag in paragraph.find_all('sup'):\n",
    "                            sup_tag.decompose()\n",
    "\n",
    "                        # Extract the paragraph text\n",
    "                        paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                        section_text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "                    for section_h3 in section_h2.find_all('section', recursive=False):\n",
    "                        ## Extract subsection title (h3)\n",
    "                        #section_h3_title_tag = section_h3.find('h3')\n",
    "                        #if section_h3_title_tag:\n",
    "                        #    section_h3_title = ' '.join(section_h3_title_tag.get_text(' ', strip=True).split())\n",
    "                        #    section_text += f\"\\nSection: {section_h3_title}\\n\\n\"\n",
    "\n",
    "                        # Extract h3 paragraphs\n",
    "                        paragraphs = section_h3.find_all('div', role='paragraph', recursive=False)\n",
    "                        for paragraph in paragraphs:\n",
    "                            # Remove reference citations embedded in <span> tags\n",
    "                            for sup_tag in paragraph.find_all('sup'):\n",
    "                                sup_tag.decompose()\n",
    "\n",
    "                            # Extract the paragraph text\n",
    "                            paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                            section_text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "                        for section_h4 in section_h3.find_all('section', recursive=False):\n",
    "                            ## Extract subsection title (h4)\n",
    "                            #section_h4_title_tag = section_h4.find('h4')\n",
    "                            #if section_h4_title_tag:\n",
    "                            #    section_h4_title = ' '.join(section_h4_title_tag.get_text(' ', strip=True).split())\n",
    "                            #    section_text += f\"\\nSection: {section_h4_title}\\n\\n\"\n",
    "\n",
    "                            # Extract h4 paragraphs\n",
    "                            paragraphs = section_h4.find_all('div', role='paragraph', recursive=False)\n",
    "                            for paragraph in paragraphs:\n",
    "                                # Remove reference citations embedded in <span> tags\n",
    "                                for sup_tag in paragraph.find_all('sup'):\n",
    "                                    sup_tag.decompose()\n",
    "\n",
    "                                # Extract the paragraph text\n",
    "                                paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                                section_text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "                            for section_h5 in section_h4.find_all('section', recursive=False):\n",
    "                                ## Extract subsection title (h5)\n",
    "                                #section_h5_title_tag = section_h5.find('h5')\n",
    "                                #if section_h5_title_tag:\n",
    "                                #    section_h5_title = ' '.join(section_h5_title_tag.get_text(' ', strip=True).split())\n",
    "                                #    section_text += f\"\\nSection: {section_h5_title}\\n\\n\"\n",
    "\n",
    "                                # Extract h5 paragraphs\n",
    "                                paragraphs = section_h5.find_all('div', role='paragraph', recursive=False)\n",
    "                                for paragraph in paragraphs:\n",
    "                                    # Remove reference citations embedded in <span> tags\n",
    "                                    for sup_tag in paragraph.find_all('sup'):\n",
    "                                        sup_tag.decompose()\n",
    "\n",
    "                                    # Extract the paragraph text\n",
    "                                    paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                                    section_text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "                    text += section_text  # Append structured section text\n",
    "\n",
    "        # Save text to a text file\n",
    "        with open(txt_file, 'w', encoding='utf-8', newline='\\n') as file:\n",
    "            file.write(text)\n",
    "\n",
    "        logging.info(f\"Saved text for {article_id} to {txt_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee9f001-0f5e-4e77-bb93-664ea5a2041d",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text(df_new_england_journal_of_medicine_open_access, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ae6b83-4479-4ca7-9411-ba35aac9ff98",
   "metadata": {},
   "source": [
    "## Biological Sciences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cbac84-dc51-49cf-852b-82bb2182c7b0",
   "metadata": {},
   "source": [
    "### [Cell](https://www.cell.com/cell/home)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789d47dd-ee83-45be-885e-d11d11286aa2",
   "metadata": {},
   "source": [
    "#### Create output subdirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206c47cc-9b19-4cd6-9544-2b3c25230251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Cell'\n",
    "id = 'cell'\n",
    "path = os.path.join(output_directory, id)\n",
    "create_directory(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5060d505-54cd-4d32-b01a-1efbff2defa6",
   "metadata": {},
   "source": [
    "#### Import the data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f79848-8e38-43c8-b9b5-13c778d6230f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cell_open_access = pd.read_json(f\"{input_directory}/cell_open_access.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b909b3e-b42d-45cd-85cf-6db3ce9a43b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cell_open_access['Published'] = pd.to_datetime(df_cell_open_access['Published'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbab9d47-2a3d-4b4a-b995-9e4a64906597",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cell_open_access = df_cell_open_access.loc[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b863e99-b157-4627-9ebd-c8abeaec1445",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_urls = df_cell_open_access['URL'].tolist()\n",
    "cell_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e0a945-4cc5-497d-ac11-47d9eab09792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(df, path):\n",
    "    \"\"\"Extracts text from HTML files and saves as text files.\"\"\"\n",
    "\n",
    "    for article_id in df['ID']:\n",
    "        html_file = os.path.join(path, f\"{article_id}.html\")\n",
    "        txt_file = os.path.join(path, f\"{article_id}.txt\")\n",
    "\n",
    "        # Check if the HTML file exists\n",
    "        if not os.path.exists(html_file):\n",
    "            logging.error(f\"Skipping {html_file}: File not found\")\n",
    "            continue\n",
    "\n",
    "        # Read HTML content\n",
    "        with open(html_file, 'r', encoding='utf-8') as file:\n",
    "            soup = BeautifulSoup(file, 'lxml')\n",
    "\n",
    "        # Initialise text variable\n",
    "        text = ''\n",
    "\n",
    "        # Web Scraping - Begin\n",
    "\n",
    "        # Extract the 'Title'\n",
    "        title = soup.find('h1', property='name')\n",
    "        if title:\n",
    "            title_text = ' '.join(title.get_text(' ', strip=True).split())\n",
    "            text += f\"Title: {title_text}\\n\\n\"\n",
    "\n",
    "        # Extract the 'Abstract'\n",
    "        abstract_section = soup.find('div', id='abstracts')\n",
    "        if abstract_section:\n",
    "            author_abstract_section = abstract_section.find('section', id='author-abstract')\n",
    "            if author_abstract_section:\n",
    "                author_abstract_h2_title = author_abstract_section.find('h2', property='name')\n",
    "                if author_abstract_h2_title:\n",
    "                    author_abstract_h2_title_text = ' '.join(author_abstract_h2_title.get_text(' ', strip=True).split())\n",
    "                    text += f\"\\nAbstract: {author_abstract_h2_title_text}\\n\\n\"\n",
    "                for paragraph in author_abstract_section.find_all('div', role='paragraph', recursive=False):\n",
    "                    paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                    text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "        # Extract the 'body'\n",
    "        body_section = soup.find('section', id='bodymatter')\n",
    "        if body_section:\n",
    "            body_core_container = body_section.find('div', class_='core-container')\n",
    "            if body_core_container:\n",
    "                # Extract sections\n",
    "                for section_h2 in body_core_container.find_all('section', recursive=False):\n",
    "                    # Extract section title\n",
    "                    section_h2_title = section_h2.find('h2')\n",
    "                    if section_h2_title:\n",
    "                        section_h2_title_text = ' '.join(section_h2_title.get_text(' ', strip=True).split())\n",
    "                        text += f\"\\nSection: {section_h2_title_text}\\n\\n\"\n",
    "                    # Extract section paragraphs\n",
    "                    for paragraph in section_h2.find_all('div', role='paragraph', recursive=False):\n",
    "                        paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                        text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "                    # Extract subsections\n",
    "                    for section_h3 in section_h2.find_all('section', recursive=False):\n",
    "                        ## Extract subsection title\n",
    "                        #section_h3_title = section_h3.find('h3')\n",
    "                        #if section_h3_title:\n",
    "                        #    section_h3_title_text = ' '.join(section_h3_title.get_text(' ', strip=True).split())\n",
    "                        #    text += f\"\\nSubsection: {section_h3_title_text}\\n\\n\"\n",
    "                        # Extract subsection paragraphs\n",
    "                        for paragraph in section_h3.find_all('div', role='paragraph', recursive=False):\n",
    "                            paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                            text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "                        # Extract subsubsections\n",
    "                        for section_h4 in section_h3.find_all('section', recursive=False):\n",
    "                            ## Extract subsubsection title\n",
    "                            #section_h4_title = section_h4.find('h4')\n",
    "                            #if section_h4_title:\n",
    "                            #    section_h4_title_text = ' '.join(section_h4_title.get_text(' ', strip=True).split())\n",
    "                            #    text += f\"\\nSubsubsection: {section_h4_title_text}\\n\\n\"\n",
    "                            # Extract subsubsection paragraphs\n",
    "                            for paragraph in section_h4.find_all('div', role='paragraph', recursive=False):\n",
    "                                paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                                text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "                            # Extract subsubsubsections\n",
    "                            for section_h5 in section_h4.find_all('section', recursive=False):\n",
    "                                ## Extract subsubsubsection title\n",
    "                                #section_h5_title = section_h5.find('h5')\n",
    "                                #if section_h5_title:\n",
    "                                #    section_h5_title_text = ' '.join(section_h5_title.get_text(' ', strip=True).split())\n",
    "                                #    text += f\"\\nSubsubsubsection: {section_h5_title_text}\\n\\n\"\n",
    "                                # Extract subsubsubsection paragraphs\n",
    "                                for paragraph in section_h5.find_all('div', role='paragraph', recursive=False):\n",
    "                                    paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                                    text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "        # Web Scraping - End\n",
    "\n",
    "        # Save text to a text file\n",
    "        with open(txt_file, 'w', encoding='utf-8', newline='\\n') as file:\n",
    "            file.write(text)\n",
    "\n",
    "        logging.info(f\"Saved text for {article_id} to {txt_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92759545-f01c-4d95-b11f-5332a59945d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text(df_cell_open_access, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370b99dc-b1d0-41b4-b543-b4d0ed244b56",
   "metadata": {},
   "source": [
    "### [American Journal of Human Biology](https://onlinelibrary.wiley.com/journal/15206300?msockid=0525cb73d9a76a060b80df20d87e6b4b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de4e155-1193-48d3-af9e-3963d57fd40d",
   "metadata": {},
   "source": [
    "#### Create output subdirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d264e64b-efd0-4c4c-b92c-0d70818a3da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'American Journal of Human Biology'\n",
    "id = 'ajhb'\n",
    "path = os.path.join(output_directory, id)\n",
    "create_directory(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d4a177-82d8-4d56-a7f5-5e3387ef9c01",
   "metadata": {},
   "source": [
    "#### Import the data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba78c8e-6e20-4bf9-aba7-c228663f2abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_american_journal_human_biology_open_access = pd.read_json(f\"{input_directory}/american_journal_human_biology_open_access.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7735dfe-bb52-4849-9395-318240553e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_american_journal_human_biology_open_access['Published'] = pd.to_datetime(df_american_journal_human_biology_open_access['Published'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d30bd6e-8732-4e3a-be21-066810a99911",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_american_journal_human_biology_open_access = df_american_journal_human_biology_open_access.loc[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be0feb6-3c47-477e-9aa3-2f66619c6dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ajhb_urls = df_american_journal_human_biology_open_access['URL'].tolist()\n",
    "ajhb_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0deb5bb-25ca-4efb-b6d6-c657a309b4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(df, path):\n",
    "    \"\"\"Extracts text from HTML files and saves as text files.\"\"\"\n",
    "\n",
    "    for article_id in df['ID']:\n",
    "        html_file = os.path.join(path, f\"{article_id}.html\")\n",
    "        txt_file = os.path.join(path, f\"{article_id}.txt\")\n",
    "\n",
    "        # Check if the HTML file exists\n",
    "        if not os.path.exists(html_file):\n",
    "            logging.error(f\"Skipping {html_file}: File not found\")\n",
    "            continue\n",
    "\n",
    "        # Read HTML content\n",
    "        with open(html_file, 'r', encoding='utf-8') as file:\n",
    "            soup = BeautifulSoup(file, 'lxml')\n",
    "\n",
    "        # Initialise text variable\n",
    "        text = ''\n",
    "\n",
    "        # Web Scraping - Begin\n",
    "\n",
    "        # Extract the 'Title'\n",
    "        title = soup.find('h1', class_='citation__title')\n",
    "        if title:\n",
    "            title_text = ' '.join(title.get_text(' ', strip=True).split())\n",
    "            text += f\"Title: {title_text}\\n\\n\"\n",
    "\n",
    "        # Capture the 'article body'\n",
    "        article_body = soup.find('div', class_='article__body')\n",
    "        \n",
    "        # Extract the 'Abstract'\n",
    "        if article_body:\n",
    "            abstract_section = article_body.find('section', class_='article-section__abstract')\n",
    "            if abstract_section:\n",
    "                h2_title = abstract_section.find('h2')\n",
    "                if h2_title:\n",
    "                    h2_title_text = ' '.join(h2_title.get_text(' ', strip=True).split())\n",
    "                    text += f\"\\nAbstract: {h2_title_text}\\n\\n\"\n",
    "                abstract_content = abstract_section.find('div', class_='article-section__content en main')\n",
    "                if abstract_content:\n",
    "                    for paragraph in abstract_content.find_all('p', recursive=False):\n",
    "                        paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                        text += f\"{paragraph_text}\\n\"\n",
    "                    for section in abstract_content.find_all('section', recursive=False):\n",
    "                        h3_title = section.find('h3')\n",
    "                        if h3_title:\n",
    "                            h3_title_text = ' '.join(h3_title.get_text(' ', strip=True).split())\n",
    "                            text += f\"\\nSection: {h3_title_text}\\n\\n\"\n",
    "                        for paragraph in section.find_all('p', recursive=False):\n",
    "                            paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                            text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "        # Extract the 'body'\n",
    "        if article_body:\n",
    "            body_section = article_body.find('section', class_='article-section article-section__full')\n",
    "            if body_section:\n",
    "                for h2_section in body_section.find_all('section', class_='article-section__content', recursive=False):\n",
    "                    h2_title = h2_section.find('h2')\n",
    "                    if h2_title:\n",
    "                        h2_title_text = ' '.join(h2_title.get_text(' ', strip=True).split())\n",
    "                        text += f\"\\nSection: {h2_title_text}\\n\\n\"\n",
    "                    for paragraph in h2_section.find_all('p', recursive=False):\n",
    "                        paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                        text += f\"{paragraph_text}\\n\"\n",
    "                    for h3_section in h2_section.find_all('section', recursive=False):\n",
    "                        #h3_title = h3_section.find('h3')\n",
    "                        #if h3_title:\n",
    "                        #    h3_title_text = ' '.join(h3_title.get_text(' ', strip=True).split())\n",
    "                        #    text += f\"\\nSection: {h3_title_text}\\n\\n\"\n",
    "                        for paragraph in h3_section.find_all('p', recursive=False):\n",
    "                            paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                            text += f\"{paragraph_text}\\n\"\n",
    "                        for h4_section in h3_section.find_all('section', recursive=False):\n",
    "                            #h4_title = h4_section.find('h4')\n",
    "                            #if h4_title:\n",
    "                            #    h4_title_text = ' '.join(h4_title.get_text(' ', strip=True).split())\n",
    "                            #    text += f\"\\nSection: {h4_title_text}\\n\\n\"\n",
    "                            for paragraph in h4_section.find_all('p', recursive=False):\n",
    "                                paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                                text += f\"{paragraph_text}\\n\"\n",
    "                            for h5_section in h4_section.find_all('section', recursive=False):\n",
    "                                #h5_title = h5_section.find('h5')\n",
    "                                #if h5_title:\n",
    "                                #    h5_title_text = ' '.join(h5_title.get_text(' ', strip=True).split())\n",
    "                                #    text += f\"\\nSection: {h5_title_text}\\n\\n\"\n",
    "                                for paragraph in h5_section.find_all('p', recursive=False):\n",
    "                                    paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                                    text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "        # Extract the 'Acknowledgements'\n",
    "        if body_section:\n",
    "            for h2_section in body_section.find_all('div', class_='article-section__content', recursive=False):\n",
    "                h2_title = h2_section.find('h2')\n",
    "                if h2_title:\n",
    "                    h2_title_text = ' '.join(h2_title.get_text(' ', strip=True).split())\n",
    "                    text += f\"\\nSection: {h2_title_text}\\n\\n\"\n",
    "                for paragraph in h2_section.find_all('p', recursive=False):\n",
    "                    paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                    text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "        # Web Scraping - End\n",
    "\n",
    "        # Save text to a text file\n",
    "        with open(txt_file, 'w', encoding='utf-8', newline='\\n') as file:\n",
    "            file.write(text)\n",
    "\n",
    "        logging.info(f\"Saved text for {article_id} to {txt_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c461d9d4-4fcf-4b7d-9deb-5daf68836c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text(df_american_journal_human_biology_open_access, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cc94d3-6310-4294-8ae4-83b0404f4187",
   "metadata": {},
   "source": [
    "## Human Sciences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5d6440-84a0-4644-8a3a-af917dcb3e8a",
   "metadata": {},
   "source": [
    "### [Annual Review of Anthropology](https://www.annualreviews.org/content/journals/anthro)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64a35c9-9690-4d2d-b0a3-8eb1255444d1",
   "metadata": {},
   "source": [
    "#### Create output subdirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d59df1e4-aebe-466e-8630-693ce28e42e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: cl_st2_ph31_eyamrog\\aran\n"
     ]
    }
   ],
   "source": [
    "# 'Annual Review of Anthropology'\n",
    "id = 'aran'\n",
    "path = os.path.join(output_directory, id)\n",
    "create_directory(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d204cc-d743-4d14-a04c-a3548aa87622",
   "metadata": {},
   "source": [
    "#### Import the data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ee6c2c9-3860-443c-934e-db2e5ed2491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar_anthropology = pd.read_json(f\"{input_directory}/ar_anthropology.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2185a6ec-fff0-4da4-bd77-75059ff47428",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar_anthropology['Published'] = pd.to_datetime(df_ar_anthropology['Published'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17909147-b309-4380-b737-e8718beed147",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar_anthropology = df_ar_anthropology.loc[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba121fd1-8a05-4488-b2c3-ef81cb197916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.annualreviews.org/content/journals/10.1146/annurev-an-51-082222-100001',\n",
       " 'https://www.annualreviews.org/content/journals/10.1146/annurev-anthro-070120-111609',\n",
       " 'https://www.annualreviews.org/content/journals/10.1146/annurev-anthro-041420-112543',\n",
       " 'https://www.annualreviews.org/content/journals/10.1146/annurev-anthro-041420-102158',\n",
       " 'https://www.annualreviews.org/content/journals/10.1146/annurev-anthro-041420-013930']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aran_urls = df_ar_anthropology['URL'].tolist()\n",
    "aran_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1d2322a-1c7c-4b01-b3d1-5c33a6a089c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(df, path):\n",
    "    \"\"\"Extracts text from HTML files and saves as text files.\"\"\"\n",
    "\n",
    "    for article_id in df['ID']:\n",
    "        html_file = os.path.join(path, f\"{article_id}.html\")\n",
    "        txt_file = os.path.join(path, f\"{article_id}.txt\")\n",
    "\n",
    "        # Check if the HTML file exists\n",
    "        if not os.path.exists(html_file):\n",
    "            logging.error(f\"Skipping {html_file}: File not found\")\n",
    "            continue\n",
    "\n",
    "        # Read HTML content\n",
    "        with open(html_file, 'r', encoding='utf-8') as file:\n",
    "            soup = BeautifulSoup(file, 'lxml')\n",
    "\n",
    "        # Initialise text variable\n",
    "        text = ''\n",
    "\n",
    "        # Extract the 'Title'\n",
    "        title_tag = soup.find('span', class_='article-title')\n",
    "        if title_tag:\n",
    "            title = ' '.join(title_tag.get_text(' ', strip=True).split())\n",
    "            text += f\"Title: {title}\\n\\n\"\n",
    "\n",
    "        # Extract 'article sections'\n",
    "        for section in soup.find_all('div', class_='articleSection'): # Finds all 'div.articleSection' elements (both top-level and nested).\n",
    "            if not section.find_parent('div', class_='articleSection'): # Keeps only the top-level sections because it filters out nested ones by checking if the 'div.articleSection' has a parent that is also 'div.articleSection'\n",
    "                \n",
    "                # Extract section title\n",
    "                section_title_tag = section.find('div', class_='tl-main-part title')\n",
    "                if section_title_tag:\n",
    "                    section_title = ' '.join(section_title_tag.get_text(' ', strip=True).split())\n",
    "                    text += f\"\\nSection: {section_title}\\n\\n\"\n",
    "\n",
    "                # Extract paragraphs (only from top-level sections)\n",
    "                for paragraph in section.find_all('p'):\n",
    "                    paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                    text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "                # Extract subsections\n",
    "                for subsection in section.find_all('div', recursive=False):\n",
    "                    #label_tag = subsection.find('span', class_='label')\n",
    "                    #if label_tag:\n",
    "                    #    label_text = ' '.join(label_tag.get_text(' ', strip=True).split())\n",
    "                    #    text += f\"\\nSection: {label_text} \"\n",
    "                    #subsection_tag = subsection.find('span', class_='tl-lowest-section')\n",
    "                    #if subsection_tag:\n",
    "                    #    subsection_title = ' '.join(subsection_tag.get_text(' ', strip=True).split())\n",
    "                    #    text += f\"{subsection_title}\\n\\n\"\n",
    "                    for paragraph in subsection.find_all('p'):\n",
    "                        paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                        text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "        # Save text to a text file\n",
    "        with open(txt_file, 'w', encoding='utf-8', newline='\\n') as file:\n",
    "            file.write(text)\n",
    "\n",
    "        logging.info(f\"Saved text for {article_id} to {txt_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46bc6143-6fb8-488b-adc9-702f49e067cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text(df_ar_anthropology, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c370476d-ff6d-4792-8b59-44110bfd7036",
   "metadata": {},
   "source": [
    "### [Journal of Human Evolution](https://www.sciencedirect.com/journal/journal-of-human-evolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd720e6-1c5e-41fb-9cfe-e444ce84b0d8",
   "metadata": {},
   "source": [
    "#### Create output subdirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a269fb7-9abb-41bb-b6d1-244838ec3b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: cl_st2_ph31_eyamrog\\jhue\n"
     ]
    }
   ],
   "source": [
    "# 'Journal of Human Evolution'\n",
    "id = 'jhue'\n",
    "path = os.path.join(output_directory, id)\n",
    "create_directory(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82aab5c9-e497-40f2-b207-4d788c4ce4a6",
   "metadata": {},
   "source": [
    "#### Import the data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b308c701-f7c2-46d0-beb8-79d0fb8136fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_journal_human_evolution_open_access = pd.read_json(f\"{input_directory}/journal_human_evolution_open_access.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8db31cdb-d34a-4f95-bfda-b212bcd3137b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_journal_human_evolution_open_access['Published'] = pd.to_datetime(df_journal_human_evolution_open_access['Published'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6df1f31-4ca7-415c-b0eb-734899f27f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_journal_human_evolution_open_access = df_journal_human_evolution_open_access.loc[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "040ca999-b715-4851-ab3c-6ff262349245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.sciencedirect.com//science/article/pii/S0047248420300294',\n",
       " 'https://www.sciencedirect.com//science/article/pii/S0047248420301123',\n",
       " 'https://www.sciencedirect.com//science/article/pii/S0047248420301135',\n",
       " 'https://www.sciencedirect.com//science/article/pii/S0047248420301305',\n",
       " 'https://www.sciencedirect.com//science/article/pii/S0047248420301615']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jhue_urls = df_journal_human_evolution_open_access['URL'].tolist()\n",
    "jhue_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8b62cae3-a0e6-4fc0-9ba3-3953c65e5a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(df, path):\n",
    "    \"\"\"Extracts text from HTML files and saves as text files.\"\"\"\n",
    "\n",
    "    for article_id in df['ID']:\n",
    "        html_file = os.path.join(path, f\"{article_id}.html\")\n",
    "        txt_file = os.path.join(path, f\"{article_id}.txt\")\n",
    "\n",
    "        # Check if the HTML file exists\n",
    "        if not os.path.exists(html_file):\n",
    "            logging.error(f\"Skipping {html_file}: File not found\")\n",
    "            continue\n",
    "\n",
    "        # Read HTML content\n",
    "        with open(html_file, 'r', encoding='utf-8') as file:\n",
    "            soup = BeautifulSoup(file, 'lxml')\n",
    "\n",
    "        # Initialise text variable\n",
    "        text = ''\n",
    "\n",
    "        # Web Scraping - Begin\n",
    "\n",
    "        # Extract the 'Title'\n",
    "        title = soup.find('h1', id='screen-reader-main-title')\n",
    "        if title:\n",
    "            title_text = ' '.join(title.get_text(' ', strip=True).split())\n",
    "            text += f\"Title: {title_text}\\n\\n\"\n",
    "\n",
    "        # Capture the 'article body'\n",
    "        article_body = soup.find('article')\n",
    "        \n",
    "        # Extract the 'Abstract'\n",
    "        if article_body:\n",
    "            abstract_section = article_body.find('div', id='abstracts')\n",
    "            if abstract_section:\n",
    "                abstract_author = abstract_section.find('div', class_='abstract author')\n",
    "                #abstract_author = abstract_section.find('div', attrs={'class': 'abstract author'})\n",
    "                #abstract_author = abstract_section.find('div', class_=['abstract', 'author']) # Results in match for 'author' or 'author-highlights', failing to reinforce 'author'\n",
    "                if abstract_author:\n",
    "                    h2_title = abstract_author.find('h2')\n",
    "                    if h2_title:\n",
    "                        h2_title_text = ' '.join(h2_title.get_text(' ', strip=True).split())\n",
    "                        text += f\"\\nAbstract: {h2_title_text}\\n\\n\"\n",
    "                    abstract_content = abstract_author.find('div')\n",
    "                    if abstract_content:\n",
    "                        for paragraph in abstract_content.find_all('div', recursive=False):\n",
    "                            paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                            text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "        # Extract the 'body'\n",
    "        if article_body:\n",
    "            body_section = article_body.find('div', id='body')\n",
    "            if body_section:\n",
    "                body_section1 = body_section.find('div')\n",
    "                if body_section1:\n",
    "                    for h2_section in body_section1.find_all('section', recursive=False):\n",
    "                        h2_title = h2_section.find('h2')\n",
    "                        if h2_title:\n",
    "                            h2_title_text = ' '.join(h2_title.get_text(' ', strip=True).split())\n",
    "                            text += f\"\\nSection: {h2_title_text}\\n\\n\"\n",
    "                        for paragraph in h2_section.find_all('div', recursive=False):\n",
    "                            paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                            text += f\"{paragraph_text}\\n\"\n",
    "                        for h3_section in h2_section.find_all('section', recursive=False):\n",
    "                            #h3_title = h3_section.find('h3')\n",
    "                            #if h3_title:\n",
    "                            #    h3_title_text = ' '.join(h3_title.get_text(' ', strip=True).split())\n",
    "                            #    text += f\"\\nSection: {h3_title_text}\\n\\n\"\n",
    "                            for paragraph in h3_section.find_all('div', recursive=False):\n",
    "                                paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                                text += f\"{paragraph_text}\\n\"\n",
    "                            for h4_section in h3_section.find_all('section', recursive=False):\n",
    "                                #h4_title = h4_section.find('h4')\n",
    "                                #if h4_title:\n",
    "                                #    h4_title_text = ' '.join(h4_title.get_text(' ', strip=True).split())\n",
    "                                #    text += f\"\\nSection: {h4_title_text}\\n\\n\"\n",
    "                                for paragraph in h4_section.find_all('div', recursive=False):\n",
    "                                    paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                                    text += f\"{paragraph_text}\\n\"\n",
    "                                for h5_section in h4_section.find_all('section', recursive=False):\n",
    "                                    #h5_title = h5_section.find('h5')\n",
    "                                    #if h5_title:\n",
    "                                    #    h5_title_text = ' '.join(h5_title.get_text(' ', strip=True).split())\n",
    "                                    #    text += f\"\\nSection: {h5_title_text}\\n\\n\"\n",
    "                                    for paragraph in h5_section.find_all('div', recursive=False):\n",
    "                                        paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                                        text += f\"{paragraph_text}\\n\"\n",
    "                for body_section2 in body_section.find_all('section', recursive=False):\n",
    "                    h2_title = body_section2.find('h2')\n",
    "                    if h2_title:\n",
    "                        h2_title_text = ' '.join(h2_title.get_text(' ', strip=True).split())\n",
    "                        text += f\"\\nSection: {h2_title_text}\\n\\n\"\n",
    "                    for paragraph in body_section2.find_all('div', recursive=False):\n",
    "                        paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                        text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "        # Web Scraping - End\n",
    "\n",
    "        # Save text to a text file\n",
    "        with open(txt_file, 'w', encoding='utf-8', newline='\\n') as file:\n",
    "            file.write(text)\n",
    "\n",
    "        logging.info(f\"Saved text for {article_id} to {txt_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "90d3e4cd-d025-441c-a13f-a575d58e1a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text(df_journal_human_evolution_open_access, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aa4ade-f323-4614-ab65-535d3e6f3d1b",
   "metadata": {},
   "source": [
    "## Applied Social Sciences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd479fd-73c9-4ca6-ae2f-f9c9dd07ab28",
   "metadata": {},
   "source": [
    "### [Journal of Applied Social Science](https://journals.sagepub.com/home/jax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb6b35a-a8fc-4315-b640-d4b7856e6006",
   "metadata": {},
   "source": [
    "#### Create output subdirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a3d115-b22e-4e14-8553-67ca8d073dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Journal of Applied Social Science'\n",
    "id = 'jasc'\n",
    "path = os.path.join(output_directory, id)\n",
    "create_directory(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa6d600-ed05-432d-921b-ae1839206143",
   "metadata": {},
   "source": [
    "#### Import the data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36125d36-91ac-43f1-97c4-00285fb8634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_journal_applied_social_science_open_access = pd.read_json(f\"{input_directory}/journal_applied_social_science_open_access.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c8cc6c-479e-49cb-a826-f2be51cae4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_journal_applied_social_science_open_access['Published'] = pd.to_datetime(df_journal_applied_social_science_open_access['Published'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5284a66c-b4f9-4468-92b1-0a6bda19b850",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_journal_applied_social_science_open_access = df_journal_applied_social_science_open_access.loc[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434d167c-e3b7-4d07-aa58-e08a32533053",
   "metadata": {},
   "outputs": [],
   "source": [
    "jasc_urls = df_journal_applied_social_science_open_access['URL'].tolist()\n",
    "jasc_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18de5943-41e4-42f6-b412-5989543c79e1",
   "metadata": {},
   "source": [
    "### [Journal of Social Issues](https://spssi.onlinelibrary.wiley.com/journal/15404560)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f22b0e-fe71-4df7-ab06-b27786aaaa08",
   "metadata": {},
   "source": [
    "#### Create output subdirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4bf949-c090-4124-adb4-319fe2a3f29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Journal of Social Issues'\n",
    "id = 'jsoi'\n",
    "path = os.path.join(output_directory, id)\n",
    "create_directory(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb75010-7601-4ad4-951a-c49822930a63",
   "metadata": {},
   "source": [
    "#### Import the data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e56be63-dcf8-4c64-8606-90b333d9a945",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_journal_social_issues_open_access = pd.read_json(f\"{input_directory}/journal_social_issues_open_access.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134393ea-72a1-4c03-aa7f-afce17ff7306",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_journal_social_issues_open_access['Published'] = pd.to_datetime(df_journal_social_issues_open_access['Published'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b1d6a4-e73e-4d61-8140-a88fcc0b2ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_journal_social_issues_open_access = df_journal_social_issues_open_access.loc[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56d10e0-3285-4278-9ebe-0f6ab171225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsoi_urls = df_journal_social_issues_open_access['URL'].tolist()\n",
    "jsoi_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4687f3a-482a-449d-a7d2-72679247ef45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(df, path):\n",
    "    \"\"\"Extracts text from HTML files and saves as text files.\"\"\"\n",
    "\n",
    "    for article_id in df['ID']:\n",
    "        html_file = os.path.join(path, f\"{article_id}.html\")\n",
    "        txt_file = os.path.join(path, f\"{article_id}.txt\")\n",
    "\n",
    "        # Check if the HTML file exists\n",
    "        if not os.path.exists(html_file):\n",
    "            logging.error(f\"Skipping {html_file}: File not found\")\n",
    "            continue\n",
    "\n",
    "        # Read HTML content\n",
    "        with open(html_file, 'r', encoding='utf-8') as file:\n",
    "            soup = BeautifulSoup(file, 'lxml')\n",
    "\n",
    "        # Initialise text variable\n",
    "        text = ''\n",
    "\n",
    "        # Web Scraping - Begin\n",
    "\n",
    "        # Extract the 'Title'\n",
    "        title = soup.find('h1', class_='citation__title')\n",
    "        if title:\n",
    "            title_text = ' '.join(title.get_text(' ', strip=True).split())\n",
    "            text += f\"Title: {title_text}\\n\\n\"\n",
    "\n",
    "        # Capture the 'article body'\n",
    "        article_body = soup.find('div', class_='article__body')\n",
    "        \n",
    "        # Extract the 'Abstract'\n",
    "        if article_body:\n",
    "            abstract_section = article_body.find('section', class_='article-section__abstract')\n",
    "            if abstract_section:\n",
    "                h2_title = abstract_section.find('h2')\n",
    "                if h2_title:\n",
    "                    h2_title_text = ' '.join(h2_title.get_text(' ', strip=True).split())\n",
    "                    text += f\"\\nAbstract: {h2_title_text}\\n\\n\"\n",
    "                abstract_content = abstract_section.find('div', class_='article-section__content en main')\n",
    "                if abstract_content:\n",
    "                    for paragraph in abstract_content.find_all('p', recursive=False):\n",
    "                        paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                        text += f\"{paragraph_text}\\n\"\n",
    "                    for section in abstract_content.find_all('section', recursive=False):\n",
    "                        h3_title = section.find('h3')\n",
    "                        if h3_title:\n",
    "                            h3_title_text = ' '.join(h3_title.get_text(' ', strip=True).split())\n",
    "                            text += f\"\\nSection: {h3_title_text}\\n\\n\"\n",
    "                        for paragraph in section.find_all('p', recursive=False):\n",
    "                            paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                            text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "        # Extract the 'body'\n",
    "        if article_body:\n",
    "            body_section = article_body.find('section', class_='article-section article-section__full')\n",
    "            if body_section:\n",
    "                for h2_section in body_section.find_all('section', class_='article-section__content', recursive=False):\n",
    "                    h2_title = h2_section.find('h2')\n",
    "                    if h2_title:\n",
    "                        h2_title_text = ' '.join(h2_title.get_text(' ', strip=True).split())\n",
    "                        text += f\"\\nSection: {h2_title_text}\\n\\n\"\n",
    "                    for paragraph in h2_section.find_all('p', recursive=False):\n",
    "                        paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                        text += f\"{paragraph_text}\\n\"\n",
    "                    for h3_section in h2_section.find_all('section', recursive=False):\n",
    "                        #h3_title = h3_section.find('h3')\n",
    "                        #if h3_title:\n",
    "                        #    h3_title_text = ' '.join(h3_title.get_text(' ', strip=True).split())\n",
    "                        #    text += f\"\\nSection: {h3_title_text}\\n\\n\"\n",
    "                        for paragraph in h3_section.find_all('p', recursive=False):\n",
    "                            paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                            text += f\"{paragraph_text}\\n\"\n",
    "                        for h4_section in h3_section.find_all('section', recursive=False):\n",
    "                            #h4_title = h4_section.find('h4')\n",
    "                            #if h4_title:\n",
    "                            #    h4_title_text = ' '.join(h4_title.get_text(' ', strip=True).split())\n",
    "                            #    text += f\"\\nSection: {h4_title_text}\\n\\n\"\n",
    "                            for paragraph in h4_section.find_all('p', recursive=False):\n",
    "                                paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                                text += f\"{paragraph_text}\\n\"\n",
    "                            for h5_section in h4_section.find_all('section', recursive=False):\n",
    "                                #h5_title = h5_section.find('h5')\n",
    "                                #if h5_title:\n",
    "                                #    h5_title_text = ' '.join(h5_title.get_text(' ', strip=True).split())\n",
    "                                #    text += f\"\\nSection: {h5_title_text}\\n\\n\"\n",
    "                                for paragraph in h5_section.find_all('p', recursive=False):\n",
    "                                    paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                                    text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "        # Extract the 'Acknowledgements'\n",
    "        if body_section:\n",
    "            for h2_section in body_section.find_all('div', class_='article-section__content', recursive=False):\n",
    "                h2_title = h2_section.find('h2')\n",
    "                if h2_title:\n",
    "                    h2_title_text = ' '.join(h2_title.get_text(' ', strip=True).split())\n",
    "                    text += f\"\\nSection: {h2_title_text}\\n\\n\"\n",
    "                for paragraph in h2_section.find_all('p', recursive=False):\n",
    "                    paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                    text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "        # Web Scraping - End\n",
    "\n",
    "        # Save text to a text file\n",
    "        with open(txt_file, 'w', encoding='utf-8', newline='\\n') as file:\n",
    "            file.write(text)\n",
    "\n",
    "        logging.info(f\"Saved text for {article_id} to {txt_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2c8446-17a8-4f7a-977e-5602edd08e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text(df_journal_social_issues_open_access, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c58a660-738e-4042-ae4b-3ca8efc965d1",
   "metadata": {},
   "source": [
    "### [Social Science & Medicine](https://www.sciencedirect.com/journal/social-science-and-medicine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d7380c-e1cf-469d-bcd5-1e75f0a39cf2",
   "metadata": {},
   "source": [
    "#### Create output subdirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d395fb1-7042-4a3a-ac74-18c9821bc5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Social Science & Medicine'\n",
    "id = 'socm'\n",
    "path = os.path.join(output_directory, id)\n",
    "create_directory(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97348f8b-959f-4fe6-b6e0-33dc3959c742",
   "metadata": {},
   "source": [
    "#### Import the data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07c506d-f793-434d-bb0c-b1df690735cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_social_science_medicine_open_access = pd.read_json(f\"{input_directory}/social_science_medicine_open_access.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58ee27d-ba9b-494a-bb19-eaa37bc59c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_social_science_medicine_open_access['Published'] = pd.to_datetime(df_social_science_medicine_open_access['Published'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bd8103-501b-4edd-8063-dcb8cb51cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_social_science_medicine_open_access = df_social_science_medicine_open_access.loc[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead7b9e3-331c-4f12-b75b-7f85ffb945df",
   "metadata": {},
   "outputs": [],
   "source": [
    "socm_urls = df_social_science_medicine_open_access['URL'].tolist()\n",
    "socm_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10419f0d-9db9-4bc2-9520-89192d9be03b",
   "metadata": {},
   "source": [
    "## Linguistics, literature and arts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff024cb5-3472-4a81-906e-d561e9cfdbcc",
   "metadata": {},
   "source": [
    "### [Applied Corpus Linguistics](https://www.sciencedirect.com/journal/applied-corpus-linguistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88c80cd-6876-4612-8fc3-ebb320f705c4",
   "metadata": {},
   "source": [
    "#### Create output subdirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1da3a5b-a6c8-4201-b1e3-99e826db2099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Applied Corpus Linguistics'\n",
    "id = 'apcl'\n",
    "path = os.path.join(output_directory, id)\n",
    "create_directory(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10ccdec-1da6-48ba-b246-b8ad6fa77c9d",
   "metadata": {},
   "source": [
    "#### Import the data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775450ac-f2fa-4738-a513-e6be8f69f166",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applied_corpus_linguistics_open_access = pd.read_json(f\"{input_directory}/applied_corpus_linguistics_open_access.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2961f6-aebb-4d43-9649-cf8c024daa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applied_corpus_linguistics_open_access['Published'] = pd.to_datetime(df_applied_corpus_linguistics_open_access['Published'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d4398e-6951-4453-814c-195ccd587a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applied_corpus_linguistics_open_access = df_applied_corpus_linguistics_open_access.loc[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7dfe17-918c-4855-9e5b-26472097f947",
   "metadata": {},
   "outputs": [],
   "source": [
    "apcl_urls = df_applied_corpus_linguistics_open_access['URL'].tolist()\n",
    "apcl_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d30cfb-e8e2-4a2e-aa15-7c9a532ca37c",
   "metadata": {},
   "source": [
    "### [Journal of English Linguistics](https://journals.sagepub.com/home/eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794c5c3c-cdbe-4d56-b4ff-c9736ffd8394",
   "metadata": {},
   "source": [
    "#### Create output subdirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694430bf-f9b7-4816-aef2-95076f392183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Journal of English Linguistics'\n",
    "id = 'jenl'\n",
    "path = os.path.join(output_directory, id)\n",
    "create_directory(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29faebef-90a2-4ff1-a484-a722e1c90cb3",
   "metadata": {},
   "source": [
    "#### Import the data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4adc78a-4aca-4b63-842e-4427180fe8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_journal_english_linguistics_open_access = pd.read_json(f\"{input_directory}/journal_english_linguistics_open_access.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476b4a62-f899-4811-85c3-6b9941a43a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_journal_english_linguistics_open_access['Published'] = pd.to_datetime(df_journal_english_linguistics_open_access['Published'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3acb18c-b9e0-40bf-9b03-79ab6745bd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_journal_english_linguistics_open_access = df_journal_english_linguistics_open_access.loc[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4456ebee-6486-48ad-911e-d060e536a5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "jenl_urls = df_journal_english_linguistics_open_access['URL'].tolist()\n",
    "jenl_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa064e6-116d-4ccd-9346-6d5f2899c4eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
