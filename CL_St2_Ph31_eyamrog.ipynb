{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "810d9f10-ec19-4b09-8f90-e983e460b319",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://laelgelcpublic.s3.sa-east-1.amazonaws.com/lael_50_years_narrow_white.png.no_years.400px_96dpi.png\" width=\"300\" alt=\"LAEL 50 years logo\">\n",
    "<h3>APPLIED LINGUISTICS GRADUATE PROGRAMME (LAEL)</h3>\n",
    "</center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c2c96-2fc3-4a1a-995b-c388036a2a15",
   "metadata": {},
   "source": [
    "# Corpus Linguistics - Study 2 - Phase 3.1 - eyamrog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b2a221-54af-4c1c-a7fd-c80cda278dcc",
   "metadata": {},
   "source": [
    "The aim of this phase is to develop solutions to scrape text from each journal's article HTML page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a685d8b0-7715-45a6-9489-2d3db9b346c8",
   "metadata": {},
   "source": [
    "## Required Python packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b526a82b-22a6-4d28-afa2-eb6e7bcca4fb",
   "metadata": {},
   "source": [
    "- beautifulsoup4\n",
    "- lxml\n",
    "- pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa922755-c4d6-4008-9aad-d35e33b18ed7",
   "metadata": {},
   "source": [
    "## Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "182dbbab-65ee-4695-a418-3e9d9d8599e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3ad412-2346-43d3-8607-08705487f1b2",
   "metadata": {},
   "source": [
    "## Define input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7657874-dc0a-4a7b-ae4b-bb3ca3fd7fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = 'cl_st2_ph2_eyamrog'\n",
    "output_directory = 'cl_st2_ph31_eyamrog'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32b4a24-8a37-4302-9c3e-790aa6dee914",
   "metadata": {},
   "source": [
    "## Create output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dce68e3-44e7-4a7e-bc25-73c21ca6fbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory already exists.\n"
     ]
    }
   ],
   "source": [
    "# Check if the output directory already exists. If it does, do nothing. If it doesn't exist, create it.\n",
    "if os.path.exists(output_directory):\n",
    "    print('Output directory already exists.')\n",
    "else:\n",
    "    try:\n",
    "        os.makedirs(output_directory)\n",
    "        print('Output directory successfully created.')\n",
    "    except OSError as e:\n",
    "        print('Failed to create the directory:', e)\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e32a21-1ea8-4563-aa6f-ee00cceeb3f6",
   "metadata": {},
   "source": [
    "### Create output subdirectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02ec67dc-d7bd-4dc3-85e3-611c633ef03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory(path):\n",
    "    \"\"\"Creates a subdirectory if it doesn't exist.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        try:\n",
    "            os.makedirs(path)\n",
    "            print(f\"Successfully created the directory: {path}\")\n",
    "        except OSError as e:\n",
    "            print(f\"Failed to create the {path} directory: {e}\")\n",
    "            sys.exit(1)\n",
    "    else:\n",
    "        print(f\"Directory already exists: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cace8b39-99d7-4407-8a1e-768144c6d575",
   "metadata": {},
   "source": [
    "## Set up logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8340bd82-f602-48a1-b8c3-61e52e94f04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_filename = f\"{output_directory}/{output_directory}.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24b5f2ab-69bd-49f5-97d3-19c08d5a997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    filename=log_filename\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bae7cc4-88c6-474d-bf2f-810ec7375c7a",
   "metadata": {},
   "source": [
    "## Health Sciences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18c6376-2357-4a21-b421-a159999c0ed5",
   "metadata": {},
   "source": [
    "### [Nature Medicine](https://www.nature.com/nm/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1cfa35-9e76-4a21-8815-f28fbee0dfb7",
   "metadata": {},
   "source": [
    "#### Create output subdirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3d1275c-086c-4065-9978-68f6da4bba84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: cl_st2_ph31_eyamrog\\natm\n"
     ]
    }
   ],
   "source": [
    "# 'Nature Medicine'\n",
    "id = 'natm'\n",
    "path = os.path.join(output_directory, id)\n",
    "create_directory(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546fd353-4b6a-4cbe-937a-69fdadaf30ca",
   "metadata": {},
   "source": [
    "#### Import the data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "414b65be-8370-4fe0-965f-bbe74abe50d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nature_medicine_open_access = pd.read_json(f\"{input_directory}/nature_medicine_open_access.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c455386c-ff89-49c7-aadf-bba48f5f8121",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nature_medicine_open_access['Published'] = pd.to_datetime(df_nature_medicine_open_access['Published'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "162a394d-c6a1-4871-8fb7-4f15a436c6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nature_medicine_open_access = df_nature_medicine_open_access.loc[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea098561-be32-41c3-a73d-639c98875412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.nature.com/articles/s41591-022-02075-9',\n",
       " 'https://www.nature.com/articles/s41591-022-02109-2',\n",
       " 'https://www.nature.com/articles/s41591-022-02049-x',\n",
       " 'https://www.nature.com/articles/s41591-022-02051-3',\n",
       " 'https://www.nature.com/articles/s41591-022-02046-0']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "natm_urls = df_nature_medicine_open_access['URL'].tolist()\n",
    "natm_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ef8552a-9189-42b2-b5b4-a562618e774c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(df, path):\n",
    "    \"\"\"Extracts text from HTML files and saves as text files.\"\"\"\n",
    "\n",
    "    for article_id in df['ID']:\n",
    "        html_file = os.path.join(path, f\"{article_id}.html\")\n",
    "        txt_file = os.path.join(path, f\"{article_id}.txt\")\n",
    "\n",
    "        # Check if the HTML file exists\n",
    "        if not os.path.exists(html_file):\n",
    "            logging.error(f\"Skipping {html_file}: File not found\")\n",
    "            continue\n",
    "\n",
    "        # Read HTML content\n",
    "        with open(html_file, 'r', encoding='utf-8') as file:\n",
    "            soup = BeautifulSoup(file, 'lxml')\n",
    "\n",
    "        # Initialise text variable\n",
    "        text = ''\n",
    "\n",
    "        # Web Scraping - Begin\n",
    "\n",
    "        # Extract the 'Title'\n",
    "        title = soup.find('h1', class_='c-article-title')\n",
    "        if title:\n",
    "            title_text = ' '.join(title.get_text(' ', strip=True).split())\n",
    "            text += f\"Title: {title_text}\\n\\n\"\n",
    "\n",
    "        # Extract the 'Abstract'\n",
    "        abstract_section = soup.find('div', id='Abs1-section')\n",
    "        if abstract_section:\n",
    "            abstract_h2_title = abstract_section.find('h2', class_='c-article-section__title')\n",
    "            if abstract_h2_title:\n",
    "                abstract_h2_title_text = ' '.join(abstract_h2_title.get_text(' ', strip=True).split())\n",
    "                text += f\"\\nAbstract: {abstract_h2_title_text}\\n\\n\"\n",
    "            abstract_content = abstract_section.find('div', class_='c-article-section__content')\n",
    "            if abstract_content:\n",
    "                for paragraph in abstract_content.find_all('p', recursive=False):\n",
    "                    # Remove <sup> elements containing references\n",
    "                    for sup_tag in paragraph.find_all('sup'):\n",
    "                        sup_tag.decompose()\n",
    "                    # Extract the paragraph text\n",
    "                    paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                    text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "        # Extract the 'main content'\n",
    "        main_content = soup.find('div', class_='main-content')\n",
    "        if main_content:\n",
    "            for main_content_section in main_content.find_all('section', recursive=False):\n",
    "                # Extract sections\n",
    "                for section in main_content_section.find_all('div', class_='c-article-section', recursive=False):\n",
    "                    # Extract section title\n",
    "                    section_h2_title = section.find('h2')\n",
    "                    if section_h2_title:\n",
    "                        section_h2_title_text = ' '.join(section_h2_title.get_text(' ', strip=True).split())\n",
    "                        text += f\"\\nSection: {section_h2_title_text}\\n\\n\"\n",
    "                    # Extract section content\n",
    "                    section_content = section.find('div', class_='c-article-section__content')\n",
    "                    if section_content:\n",
    "                        #for content in section_content.find_all(['h3', 'h4', 'h5', 'p'], recursive=False):\n",
    "                        for content in section_content.find_all(['p'], recursive=False):\n",
    "                            # Remove <sup> elements containing references\n",
    "                            for sup_tag in content.find_all('sup'):\n",
    "                                sup_tag.decompose()\n",
    "                            # Extract the content text\n",
    "                            content_text = ' '.join(content.get_text(' ', strip=True).split())\n",
    "                            text += f\"{content_text}\\n\"\n",
    "\n",
    "        # Extract the 'u-mt-32'\n",
    "        u_mt_32 = soup.find('div', class_='u-mt-32')\n",
    "        if u_mt_32:\n",
    "            data_availability_section = u_mt_32.find('section', attrs={'data-title': 'Data availability'})\n",
    "            if data_availability_section:\n",
    "                for section in data_availability_section.find_all('div', class_='c-article-section', recursive=False):\n",
    "                    # Extract section title\n",
    "                    section_h2_title = section.find('h2')\n",
    "                    if section_h2_title:\n",
    "                        section_h2_title_text = ' '.join(section_h2_title.get_text(' ', strip=True).split())\n",
    "                        text += f\"\\nSection: {section_h2_title_text}\\n\\n\"\n",
    "                    # Extract section content\n",
    "                    section_content = section.find('div', class_='c-article-section__content')\n",
    "                    if section_content:\n",
    "                        for content in section_content.find_all('p', recursive=False):\n",
    "                            # Remove <sup> elements containing references\n",
    "                            for sup_tag in content.find_all('sup'):\n",
    "                                sup_tag.decompose()\n",
    "                            # Extract the content text\n",
    "                            content_text = ' '.join(content.get_text(' ', strip=True).split())\n",
    "                            text += f\"{content_text}\\n\"\n",
    "\n",
    "            acknowledgements_section = u_mt_32.find('section', attrs={'data-title': 'Acknowledgements'})\n",
    "            if acknowledgements_section:\n",
    "                for section in acknowledgements_section.find_all('div', class_='c-article-section', recursive=False):\n",
    "                    # Extract section title\n",
    "                    section_h2_title = section.find('h2')\n",
    "                    if section_h2_title:\n",
    "                        section_h2_title_text = ' '.join(section_h2_title.get_text(' ', strip=True).split())\n",
    "                        text += f\"\\nSection: {section_h2_title_text}\\n\\n\"\n",
    "                    # Extract section content\n",
    "                    section_content = section.find('div', class_='c-article-section__content')\n",
    "                    if section_content:\n",
    "                        for content in section_content.find_all('p', recursive=False):\n",
    "                            # Remove <sup> elements containing references\n",
    "                            for sup_tag in content.find_all('sup'):\n",
    "                                sup_tag.decompose()\n",
    "                            # Extract the content text\n",
    "                            content_text = ' '.join(content.get_text(' ', strip=True).split())\n",
    "                            text += f\"{content_text}\\n\"\n",
    "\n",
    "        # Web Scraping - End\n",
    "\n",
    "        # Save text to a text file\n",
    "        with open(txt_file, 'w', encoding='utf-8', newline='\\n') as file:\n",
    "            file.write(text)\n",
    "\n",
    "        logging.info(f\"Saved text for {article_id} to {txt_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23bfcc33-b3ef-4b22-8a45-00870d1ddfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text(df_nature_medicine_open_access, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e7cd58-2f2b-4d00-812e-17907943a65a",
   "metadata": {},
   "source": [
    "### [Annual Review of Public Health](https://www.annualreviews.org/content/journals/publhealth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fc5039-bb6d-4316-87af-b2592b894f55",
   "metadata": {},
   "source": [
    "#### Create output subdirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f54b2768-6e7a-4b45-a958-6aeba669836f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: cl_st2_ph31_eyamrog\\arph\n"
     ]
    }
   ],
   "source": [
    "# 'Annual Review of Public Health'\n",
    "id = 'arph'\n",
    "path = os.path.join(output_directory, id)\n",
    "create_directory(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ab4b87-ceb6-430f-ab20-6c360e7ab344",
   "metadata": {},
   "source": [
    "#### Import the data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eaaf49fe-13bb-47c4-9f44-458bea3f8eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar_public_health = pd.read_json(f\"{input_directory}/ar_public_health.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c072f94-3cf8-4a13-9101-aa28863dd0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar_public_health['Published'] = pd.to_datetime(df_ar_public_health['Published'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1abe9697-6d79-40b0-9c84-4528549ff067",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar_public_health = df_ar_public_health.loc[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f727d9c0-5a01-4120-909c-8ec14ddddbf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.annualreviews.org/content/journals/10.1146/annurev-publhealth-121019-053834',\n",
       " 'https://www.annualreviews.org/content/journals/10.1146/annurev-publhealth-051920-114020',\n",
       " 'https://www.annualreviews.org/content/journals/10.1146/annurev-publhealth-012420-105104',\n",
       " 'https://www.annualreviews.org/content/journals/10.1146/annurev-publhealth-051920-110928',\n",
       " 'https://www.annualreviews.org/content/journals/10.1146/annurev-publhealth-060220-042648']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arph_urls = df_ar_public_health['URL'].tolist()\n",
    "arph_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09d70901-70a7-4cfe-b291-55136d2256a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(df, path):\n",
    "    \"\"\"Extracts text from HTML files and saves as text files.\"\"\"\n",
    "\n",
    "    for article_id in df['ID']:\n",
    "        html_file = os.path.join(path, f\"{article_id}.html\")\n",
    "        txt_file = os.path.join(path, f\"{article_id}.txt\")\n",
    "\n",
    "        # Check if the HTML file exists\n",
    "        if not os.path.exists(html_file):\n",
    "            logging.error(f\"Skipping {html_file}: File not found\")\n",
    "            continue\n",
    "\n",
    "        # Read HTML content\n",
    "        with open(html_file, 'r', encoding='utf-8') as file:\n",
    "            soup = BeautifulSoup(file, 'lxml')\n",
    "\n",
    "        # Initialise text variable\n",
    "        text = ''\n",
    "\n",
    "        # Extract the 'Title'\n",
    "        title_tag = soup.find('span', class_='article-title')\n",
    "        if title_tag:\n",
    "            title = ' '.join(title_tag.get_text(' ', strip=True).split())\n",
    "            text += f\"Title: {title}\\n\\n\"\n",
    "\n",
    "        # Extract 'article sections'\n",
    "        for section in soup.find_all('div', class_='articleSection'): # Finds all 'div.articleSection' elements (both top-level and nested).\n",
    "            if not section.find_parent('div', class_='articleSection'): # Keeps only the top-level sections because it filters out nested ones by checking if the 'div.articleSection' has a parent that is also 'div.articleSection'\n",
    "                \n",
    "                # Extract section title\n",
    "                section_title_tag = section.find('div', class_='tl-main-part title')\n",
    "                if section_title_tag:\n",
    "                    section_title = ' '.join(section_title_tag.get_text(' ', strip=True).split())\n",
    "                    text += f\"\\nSection: {section_title}\\n\\n\"\n",
    "\n",
    "                # Extract paragraphs (only from top-level sections)\n",
    "                for paragraph in section.find_all('p'):\n",
    "                    paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                    text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "                # Extract subsections\n",
    "                for subsection in section.find_all('div', recursive=False):\n",
    "                    #label_tag = subsection.find('span', class_='label')\n",
    "                    #if label_tag:\n",
    "                    #    label_text = ' '.join(label_tag.get_text(' ', strip=True).split())\n",
    "                    #    text += f\"\\nSection: {label_text} \"\n",
    "                    #subsection_tag = subsection.find('span', class_='tl-lowest-section')\n",
    "                    #if subsection_tag:\n",
    "                    #    subsection_title = ' '.join(subsection_tag.get_text(' ', strip=True).split())\n",
    "                    #    text += f\"{subsection_title}\\n\\n\"\n",
    "                    for paragraph in subsection.find_all('p'):\n",
    "                        paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                        text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "        # Save text to a text file\n",
    "        with open(txt_file, 'w', encoding='utf-8', newline='\\n') as file:\n",
    "            file.write(text)\n",
    "\n",
    "        logging.info(f\"Saved text for {article_id} to {txt_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7272b3ab-2ad5-42f8-bb2e-41851c444a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text(df_ar_public_health, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f5f473-3883-4b0e-914e-58419006c98f",
   "metadata": {},
   "source": [
    "### [Lancet Public Health](https://www.thelancet.com/journals/lanpub/home)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ab6f0f-e159-4db4-a8ae-ca8a741041c4",
   "metadata": {},
   "source": [
    "#### Create output subdirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd3779ce-408d-4f0b-b191-85b2628727aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: cl_st2_ph31_eyamrog\\laph\n"
     ]
    }
   ],
   "source": [
    "# 'Lancet Public Health'\n",
    "id = 'laph'\n",
    "path = os.path.join(output_directory, id)\n",
    "create_directory(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df40946-c5ca-416b-afe4-c9006e54295b",
   "metadata": {},
   "source": [
    "#### Import the data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce33fb27-f04b-4436-b98b-2455158768dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lancet_public_health_open_access = pd.read_json(f\"{input_directory}/lancet_public_health_open_access.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "939146c7-58a0-4797-b477-ac8daee1130d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lancet_public_health_open_access['Published'] = pd.to_datetime(df_lancet_public_health_open_access['Published'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7ea8d2b-0047-4402-9b73-6888f6c647ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lancet_public_health_open_access = df_lancet_public_health_open_access.loc[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8cef33c-4647-4103-a676-9ff391d397d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.thelancet.com/journals/lanpub/article/PIIS2468-2667(19)30188-4/fulltext',\n",
       " 'https://www.thelancet.com/journals/lanpub/article/PIIS2468-2667(19)30219-1/fulltext',\n",
       " 'https://www.thelancet.com/journals/lanpub/article/PIIS2468-2667(19)30226-9/fulltext',\n",
       " 'https://www.thelancet.com/journals/lanpub/article/PIIS2468-2667(19)30231-2/fulltext',\n",
       " 'https://www.thelancet.com/journals/lanpub/article/PIIS2468-2667(19)30230-0/fulltext']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laph_urls = df_lancet_public_health_open_access['URL'].tolist()\n",
    "laph_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3fd2c75d-c2ce-43cc-848f-84d466688fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(df, path):\n",
    "    \"\"\"Extracts text from HTML files and saves as text files.\"\"\"\n",
    "\n",
    "    for article_id in df['ID']:\n",
    "        html_file = os.path.join(path, f\"{article_id}.html\")\n",
    "        txt_file = os.path.join(path, f\"{article_id}.txt\")\n",
    "\n",
    "        # Check if the HTML file exists\n",
    "        if not os.path.exists(html_file):\n",
    "            logging.error(f\"Skipping {html_file}: File not found\")\n",
    "            continue\n",
    "\n",
    "        # Read HTML content\n",
    "        with open(html_file, 'r', encoding='utf-8') as file:\n",
    "            soup = BeautifulSoup(file, 'lxml')\n",
    "\n",
    "        # Initialise text variable\n",
    "        text = ''\n",
    "\n",
    "        # Extract the 'Title'\n",
    "        title_tag = soup.find('h1', property='name')\n",
    "        if title_tag:\n",
    "            title = ' '.join(title_tag.get_text(' ', strip=True).split())\n",
    "            text += f\"Title: {title}\\n\\n\"\n",
    "\n",
    "        # Extract the 'Abstract'\n",
    "        abstract_section = soup.find('section', property='abstract')\n",
    "        if abstract_section:\n",
    "            abstract_tag = abstract_section.find('h2', property='name')\n",
    "            if abstract_tag:\n",
    "                abstract = ' '.join(abstract_tag.get_text(' ', strip=True).split())\n",
    "                text += f\"\\nAbstract: {abstract}\\n\\n\"\n",
    "\n",
    "            for section_h3 in abstract_section.find_all('section', recursive=False):\n",
    "                section_h3_title_tag = section_h3.find('h3')\n",
    "                if section_h3_title_tag:\n",
    "                    section_h3_title = ' '.join(section_h3_title_tag.get_text(' ', strip=True).split())\n",
    "                    text += f\"\\nAbstract_Section: {section_h3_title}\\n\\n\"\n",
    "\n",
    "                # Extract paragraphs within each section\n",
    "                paragraphs = section_h3.find_all('div', role='paragraph', recursive=False)\n",
    "                for paragraph in paragraphs:\n",
    "                    # Remove reference citations embedded in <span> tags\n",
    "                    for ref_tag in paragraph.find_all('span', class_='dropBlock reference-citations'):\n",
    "                        ref_tag.decompose()\n",
    "\n",
    "                    # Extract the paragraph text\n",
    "                    paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                    text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "        # Extract the 'article body'\n",
    "        body_section = soup.find('section', property='articleBody')\n",
    "        if body_section:\n",
    "            body_section_core_container = body_section.find('div', class_='core-container')\n",
    "            if body_section_core_container:\n",
    "                # Extract sectioned content\n",
    "                for section_h2 in body_section_core_container.find_all('section', recursive=False):\n",
    "                    section_text = ''  # Reset for each section\n",
    "\n",
    "                    # Extract section title (h2)\n",
    "                    section_h2_title_tag = section_h2.find('h2')\n",
    "                    if section_h2_title_tag:\n",
    "                        section_h2_title = ' '.join(section_h2_title_tag.get_text(' ', strip=True).split())\n",
    "                        section_text += f\"\\nSection: {section_h2_title}\\n\\n\"\n",
    "                    \n",
    "                    # Extract h2 paragraphs, if there are any\n",
    "                    paragraphs = section_h2.find_all('div', role='paragraph', recursive=False)\n",
    "                    for paragraph in paragraphs:\n",
    "                        # Remove nested paragraphs in the paragraph to drop the paragraphs in the 'Research in context' box\n",
    "                        for nested_paragraph in paragraph.find_all('div', role='paragraph'):\n",
    "                            nested_paragraph.decompose()\n",
    "                        # Remove reference citations embedded in <span> tags\n",
    "                        for ref_tag in paragraph.find_all('span', class_='dropBlock reference-citations'):\n",
    "                            ref_tag.decompose()\n",
    "\n",
    "                        # Extract the paragraph text\n",
    "                        paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                        section_text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "                    for section_h3 in section_h2.find_all('section'):\n",
    "                        ## Extract subsection title (h3)\n",
    "                        #section_h3_title_tag = section_h3.find('h3')\n",
    "                        #if section_h3_title_tag:\n",
    "                        #    section_h3_title = ' '.join(section_h3_title_tag.get_text(' ', strip=True).split())\n",
    "                        #    section_text += f\"\\nSection: {section_h3_title}\\n\\n\"\n",
    "\n",
    "                        # Extract h3 paragraphs\n",
    "                        paragraphs = section_h3.find_all('div', role='paragraph', recursive=False)\n",
    "                        for paragraph in paragraphs:\n",
    "                            # Remove reference citations embedded in <span> tags\n",
    "                            for ref_tag in paragraph.find_all('span', class_='dropBlock reference-citations'):\n",
    "                                ref_tag.decompose()\n",
    "\n",
    "                            # Extract the paragraph text\n",
    "                            paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                            section_text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "                        for section_h4 in section_h3.find_all('section'):\n",
    "                            ## Extract subsection title (h4)\n",
    "                            #section_h4_title_tag = section_h4.find('h4')\n",
    "                            #if section_h4_title_tag:\n",
    "                            #    section_h4_title = ' '.join(section_h4_title_tag.get_text(' ', strip=True).split())\n",
    "                            #    section_text += f\"\\nSection: {section_h4_title}\\n\\n\"\n",
    "\n",
    "                            # Extract h4 paragraphs\n",
    "                            paragraphs = section_h4.find_all('div', role='paragraph', recursive=False)\n",
    "                            for paragraph in paragraphs:\n",
    "                                # Remove reference citations embedded in <span> tags\n",
    "                                for ref_tag in paragraph.find_all('span', class_='dropBlock reference-citations'):\n",
    "                                    ref_tag.decompose()\n",
    "\n",
    "                                # Extract the paragraph text\n",
    "                                paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                                section_text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "                            for section_h5 in section_h4.find_all('section'):\n",
    "                                ## Extract subsection title (h5)\n",
    "                                #section_h5_title_tag = section_h5.find('h5')\n",
    "                                #if section_h5_title_tag:\n",
    "                                #    section_h5_title = ' '.join(section_h5_title_tag.get_text(' ', strip=True).split())\n",
    "                                #    section_text += f\"\\nSection: {section_h5_title}\\n\\n\"\n",
    "\n",
    "                                # Extract h5 paragraphs\n",
    "                                paragraphs = section_h5.find_all('div', role='paragraph', recursive=False)\n",
    "                                for paragraph in paragraphs:\n",
    "                                    # Remove reference citations embedded in <span> tags\n",
    "                                    for ref_tag in paragraph.find_all('span', class_='dropBlock reference-citations'):\n",
    "                                        ref_tag.decompose()\n",
    "\n",
    "                                    # Extract the paragraph text\n",
    "                                    paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                                    section_text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "                    text += section_text  # Append structured section text\n",
    "\n",
    "        # Save text to a text file\n",
    "        with open(txt_file, 'w', encoding='utf-8', newline='\\n') as file:\n",
    "            file.write(text)\n",
    "\n",
    "        logging.info(f\"Saved text for {article_id} to {txt_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4520ca4a-f4de-4da1-8eac-018257746589",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text(df_lancet_public_health_open_access, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f194ad6f-39a0-4d59-be27-eb2319a43367",
   "metadata": {},
   "source": [
    "### [New England Journal of Medicine](https://www.nejm.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53333b81-e126-48f3-ab64-50f726a4d36e",
   "metadata": {},
   "source": [
    "#### Create output subdirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5591cc98-dc8a-468f-bcc6-cdad8921d872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: cl_st2_ph31_eyamrog\\nejm\n"
     ]
    }
   ],
   "source": [
    "# 'New England Journal of Medicine'\n",
    "id = 'nejm'\n",
    "path = os.path.join(output_directory, id)\n",
    "create_directory(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7de4569-b600-442d-b154-c9e01944fb98",
   "metadata": {},
   "source": [
    "#### Import the data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ee3cefb-8c5f-46b1-abce-dff7e0d78875",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_england_journal_of_medicine_open_access = pd.read_json(f\"{input_directory}/new_england_journal_of_medicine_open_access.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a4d264d-ef90-4f01-b817-98a9b4650485",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_england_journal_of_medicine_open_access['Published'] = pd.to_datetime(df_new_england_journal_of_medicine_open_access['Published'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "16d8f249-0057-47f3-b207-271509c2e458",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_england_journal_of_medicine_open_access = df_new_england_journal_of_medicine_open_access.loc[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "67b24162-e1aa-49d9-82b0-9b5b83c875e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.nejm.org/doi/full/10.1056/NEJMoa1910355',\n",
       " 'https://www.nejm.org/doi/full/10.1056/NEJMoa1817591',\n",
       " 'https://www.nejm.org/doi/full/10.1056/NEJMoa1908490',\n",
       " 'https://www.nejm.org/doi/full/10.1056/NEJMoa1913662',\n",
       " 'https://www.nejm.org/doi/full/10.1056/NEJMsa1901383']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nejm_urls = df_new_england_journal_of_medicine_open_access['URL'].tolist()\n",
    "nejm_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f0a8de94-c657-4c33-85d4-d8c5add9d2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(df, path):\n",
    "    \"\"\"Extracts text from HTML files and saves as text files.\"\"\"\n",
    "\n",
    "    for article_id in df['ID']:\n",
    "        html_file = os.path.join(path, f\"{article_id}.html\")\n",
    "        txt_file = os.path.join(path, f\"{article_id}.txt\")\n",
    "\n",
    "        # Check if the HTML file exists\n",
    "        if not os.path.exists(html_file):\n",
    "            logging.error(f\"Skipping {html_file}: File not found\")\n",
    "            continue\n",
    "\n",
    "        # Read HTML content\n",
    "        with open(html_file, 'r', encoding='utf-8') as file:\n",
    "            soup = BeautifulSoup(file, 'lxml')\n",
    "\n",
    "        # Initialise text variable\n",
    "        text = ''\n",
    "\n",
    "        # Extract the 'Title'\n",
    "        title_tag = soup.find('h1', property='name')\n",
    "        if title_tag:\n",
    "            title = ' '.join(title_tag.get_text(' ', strip=True).split())\n",
    "            text += f\"Title: {title}\\n\\n\"\n",
    "\n",
    "        # Extract the 'Abstract'\n",
    "        abstract_section = soup.find('section', property='abstract')\n",
    "        if abstract_section:\n",
    "            abstract_tag = abstract_section.find('h2', property='name')\n",
    "            if abstract_tag:\n",
    "                abstract = ' '.join(abstract_tag.get_text(' ', strip=True).split())\n",
    "                text += f\"\\nAbstract: {abstract}\\n\\n\"\n",
    "\n",
    "            for section_h3 in abstract_section.find_all('section', recursive=False):\n",
    "                section_h3_title_tag = section_h3.find('h3')\n",
    "                if section_h3_title_tag:\n",
    "                    section_h3_title = ' '.join(section_h3_title_tag.get_text(' ', strip=True).split())\n",
    "                    text += f\"\\nAbstract_Section: {section_h3_title}\\n\\n\"\n",
    "\n",
    "                # Extract paragraphs within each section\n",
    "                paragraphs = section_h3.find_all('div', role='paragraph', recursive=False)\n",
    "                for paragraph in paragraphs:\n",
    "                    # Remove reference citations embedded in <span> tags\n",
    "                    for sup_tag in paragraph.find_all('sup'):\n",
    "                        sup_tag.decompose()\n",
    "\n",
    "                    # Extract the paragraph text\n",
    "                    paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                    text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "        # Extract the article body\n",
    "        body_section = soup.find('section', property='articleBody')\n",
    "        if body_section:\n",
    "            body_section_core_container = body_section.find('div', class_='core-container')\n",
    "            if body_section_core_container:\n",
    "                # Extract the initial paragraphs that precede the first section (introduction)\n",
    "                text += f\"\\nSection: Introduction\\n\\n\" # Insert the 'Introduction' title\n",
    "                paragraphs = body_section_core_container.find_all('div', role='paragraph', recursive=False) # Prevents nested extraction\n",
    "                for paragraph in paragraphs:\n",
    "                    # Remove reference citations embedded in <span> tags\n",
    "                    for sup_tag in paragraph.find_all('sup'):\n",
    "                        sup_tag.decompose()\n",
    "\n",
    "                    # Extract the paragraph text\n",
    "                    paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                    text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "                # Extract sectioned content\n",
    "                for section_h2 in body_section_core_container.find_all('section', recursive=False):\n",
    "                    section_text = ''  # Reset for each section\n",
    "\n",
    "                    # Extract section title (h2)\n",
    "                    section_h2_title_tag = section_h2.find('h2')\n",
    "                    if section_h2_title_tag:\n",
    "                        section_h2_title = ' '.join(section_h2_title_tag.get_text(' ', strip=True).split())\n",
    "                        section_text += f\"\\nSection: {section_h2_title}\\n\\n\"\n",
    "                    \n",
    "                    # Extract h2 paragraphs, if there are any\n",
    "                    paragraphs = section_h2.find_all('div', role='paragraph', recursive=False)\n",
    "                    for paragraph in paragraphs:\n",
    "                        # Remove reference citations embedded in <span> tags\n",
    "                        for sup_tag in paragraph.find_all('sup'):\n",
    "                            sup_tag.decompose()\n",
    "\n",
    "                        # Extract the paragraph text\n",
    "                        paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                        section_text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "                    for section_h3 in section_h2.find_all('section', recursive=False):\n",
    "                        ## Extract subsection title (h3)\n",
    "                        #section_h3_title_tag = section_h3.find('h3')\n",
    "                        #if section_h3_title_tag:\n",
    "                        #    section_h3_title = ' '.join(section_h3_title_tag.get_text(' ', strip=True).split())\n",
    "                        #    section_text += f\"\\nSection: {section_h3_title}\\n\\n\"\n",
    "\n",
    "                        # Extract h3 paragraphs\n",
    "                        paragraphs = section_h3.find_all('div', role='paragraph', recursive=False)\n",
    "                        for paragraph in paragraphs:\n",
    "                            # Remove reference citations embedded in <span> tags\n",
    "                            for sup_tag in paragraph.find_all('sup'):\n",
    "                                sup_tag.decompose()\n",
    "\n",
    "                            # Extract the paragraph text\n",
    "                            paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                            section_text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "                        for section_h4 in section_h3.find_all('section', recursive=False):\n",
    "                            ## Extract subsection title (h4)\n",
    "                            #section_h4_title_tag = section_h4.find('h4')\n",
    "                            #if section_h4_title_tag:\n",
    "                            #    section_h4_title = ' '.join(section_h4_title_tag.get_text(' ', strip=True).split())\n",
    "                            #    section_text += f\"\\nSection: {section_h4_title}\\n\\n\"\n",
    "\n",
    "                            # Extract h4 paragraphs\n",
    "                            paragraphs = section_h4.find_all('div', role='paragraph', recursive=False)\n",
    "                            for paragraph in paragraphs:\n",
    "                                # Remove reference citations embedded in <span> tags\n",
    "                                for sup_tag in paragraph.find_all('sup'):\n",
    "                                    sup_tag.decompose()\n",
    "\n",
    "                                # Extract the paragraph text\n",
    "                                paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                                section_text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "                            for section_h5 in section_h4.find_all('section', recursive=False):\n",
    "                                ## Extract subsection title (h5)\n",
    "                                #section_h5_title_tag = section_h5.find('h5')\n",
    "                                #if section_h5_title_tag:\n",
    "                                #    section_h5_title = ' '.join(section_h5_title_tag.get_text(' ', strip=True).split())\n",
    "                                #    section_text += f\"\\nSection: {section_h5_title}\\n\\n\"\n",
    "\n",
    "                                # Extract h5 paragraphs\n",
    "                                paragraphs = section_h5.find_all('div', role='paragraph', recursive=False)\n",
    "                                for paragraph in paragraphs:\n",
    "                                    # Remove reference citations embedded in <span> tags\n",
    "                                    for sup_tag in paragraph.find_all('sup'):\n",
    "                                        sup_tag.decompose()\n",
    "\n",
    "                                    # Extract the paragraph text\n",
    "                                    paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                                    section_text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "                    text += section_text  # Append structured section text\n",
    "\n",
    "        # Save text to a text file\n",
    "        with open(txt_file, 'w', encoding='utf-8', newline='\\n') as file:\n",
    "            file.write(text)\n",
    "\n",
    "        logging.info(f\"Saved text for {article_id} to {txt_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ee9f001-0f5e-4e77-bb93-664ea5a2041d",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text(df_new_england_journal_of_medicine_open_access, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ae6b83-4479-4ca7-9411-ba35aac9ff98",
   "metadata": {},
   "source": [
    "## Biological Sciences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cbac84-dc51-49cf-852b-82bb2182c7b0",
   "metadata": {},
   "source": [
    "### [Cell](https://www.cell.com/cell/home)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789d47dd-ee83-45be-885e-d11d11286aa2",
   "metadata": {},
   "source": [
    "#### Create output subdirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "206c47cc-9b19-4cd6-9544-2b3c25230251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: cl_st2_ph31_eyamrog\\cell\n"
     ]
    }
   ],
   "source": [
    "# 'Cell'\n",
    "id = 'cell'\n",
    "path = os.path.join(output_directory, id)\n",
    "create_directory(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5060d505-54cd-4d32-b01a-1efbff2defa6",
   "metadata": {},
   "source": [
    "#### Import the data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "42f79848-8e38-43c8-b9b5-13c778d6230f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cell_open_access = pd.read_json(f\"{input_directory}/cell_open_access.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3b909b3e-b42d-45cd-85cf-6db3ce9a43b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cell_open_access['Published'] = pd.to_datetime(df_cell_open_access['Published'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fbab9d47-2a3d-4b4a-b995-9e4a64906597",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cell_open_access = df_cell_open_access.loc[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5b863e99-b157-4627-9ebd-c8abeaec1445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.cell.com/cell/fulltext/S0092-8674(19)31270-X',\n",
       " 'https://www.cell.com/cell/fulltext/S0092-8674(19)31378-9',\n",
       " 'https://www.cell.com/cell/fulltext/S0092-8674(19)31328-5',\n",
       " 'https://www.cell.com/cell/fulltext/S0092-8674(19)31283-8',\n",
       " 'https://www.cell.com/cell/fulltext/S0092-8674(19)31317-0']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_urls = df_cell_open_access['URL'].tolist()\n",
    "cell_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "37e0a945-4cc5-497d-ac11-47d9eab09792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(df, path):\n",
    "    \"\"\"Extracts text from HTML files and saves as text files.\"\"\"\n",
    "\n",
    "    for article_id in df['ID']:\n",
    "        html_file = os.path.join(path, f\"{article_id}.html\")\n",
    "        txt_file = os.path.join(path, f\"{article_id}.txt\")\n",
    "\n",
    "        # Check if the HTML file exists\n",
    "        if not os.path.exists(html_file):\n",
    "            logging.error(f\"Skipping {html_file}: File not found\")\n",
    "            continue\n",
    "\n",
    "        # Read HTML content\n",
    "        with open(html_file, 'r', encoding='utf-8') as file:\n",
    "            soup = BeautifulSoup(file, 'lxml')\n",
    "\n",
    "        # Initialise text variable\n",
    "        text = ''\n",
    "\n",
    "        # Web Scraping - Begin\n",
    "\n",
    "        # Extract the 'Title'\n",
    "        title = soup.find('h1', property='name')\n",
    "        if title:\n",
    "            title_text = ' '.join(title.get_text(' ', strip=True).split())\n",
    "            text += f\"Title: {title_text}\\n\\n\"\n",
    "\n",
    "        # Extract the 'Abstract'\n",
    "        abstract_section = soup.find('div', id='abstracts')\n",
    "        if abstract_section:\n",
    "            author_abstract_section = abstract_section.find('section', id='author-abstract')\n",
    "            if author_abstract_section:\n",
    "                author_abstract_h2_title = author_abstract_section.find('h2', property='name')\n",
    "                if author_abstract_h2_title:\n",
    "                    author_abstract_h2_title_text = ' '.join(author_abstract_h2_title.get_text(' ', strip=True).split())\n",
    "                    text += f\"\\nAbstract: {author_abstract_h2_title_text}\\n\\n\"\n",
    "                for paragraph in author_abstract_section.find_all('div', role='paragraph', recursive=False):\n",
    "                    paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                    text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "        # Extract the 'body'\n",
    "        body_section = soup.find('section', id='bodymatter')\n",
    "        if body_section:\n",
    "            body_core_container = body_section.find('div', class_='core-container')\n",
    "            if body_core_container:\n",
    "                # Extract sections\n",
    "                for section_h2 in body_core_container.find_all('section', recursive=False):\n",
    "                    # Extract section title\n",
    "                    section_h2_title = section_h2.find('h2')\n",
    "                    if section_h2_title:\n",
    "                        section_h2_title_text = ' '.join(section_h2_title.get_text(' ', strip=True).split())\n",
    "                        text += f\"\\nSection: {section_h2_title_text}\\n\\n\"\n",
    "                    # Extract section paragraphs\n",
    "                    for paragraph in section_h2.find_all('div', role='paragraph', recursive=False):\n",
    "                        paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                        text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "                    # Extract subsections\n",
    "                    for section_h3 in section_h2.find_all('section', recursive=False):\n",
    "                        ## Extract subsection title\n",
    "                        #section_h3_title = section_h3.find('h3')\n",
    "                        #if section_h3_title:\n",
    "                        #    section_h3_title_text = ' '.join(section_h3_title.get_text(' ', strip=True).split())\n",
    "                        #    text += f\"\\nSubsection: {section_h3_title_text}\\n\\n\"\n",
    "                        # Extract subsection paragraphs\n",
    "                        for paragraph in section_h3.find_all('div', role='paragraph', recursive=False):\n",
    "                            paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                            text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "                        # Extract subsubsections\n",
    "                        for section_h4 in section_h3.find_all('section', recursive=False):\n",
    "                            ## Extract subsubsection title\n",
    "                            #section_h4_title = section_h4.find('h4')\n",
    "                            #if section_h4_title:\n",
    "                            #    section_h4_title_text = ' '.join(section_h4_title.get_text(' ', strip=True).split())\n",
    "                            #    text += f\"\\nSubsubsection: {section_h4_title_text}\\n\\n\"\n",
    "                            # Extract subsubsection paragraphs\n",
    "                            for paragraph in section_h4.find_all('div', role='paragraph', recursive=False):\n",
    "                                paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                                text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "                            # Extract subsubsubsections\n",
    "                            for section_h5 in section_h4.find_all('section', recursive=False):\n",
    "                                ## Extract subsubsubsection title\n",
    "                                #section_h5_title = section_h5.find('h5')\n",
    "                                #if section_h5_title:\n",
    "                                #    section_h5_title_text = ' '.join(section_h5_title.get_text(' ', strip=True).split())\n",
    "                                #    text += f\"\\nSubsubsubsection: {section_h5_title_text}\\n\\n\"\n",
    "                                # Extract subsubsubsection paragraphs\n",
    "                                for paragraph in section_h5.find_all('div', role='paragraph', recursive=False):\n",
    "                                    paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                                    text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "        # Web Scraping - End\n",
    "\n",
    "        # Save text to a text file\n",
    "        with open(txt_file, 'w', encoding='utf-8', newline='\\n') as file:\n",
    "            file.write(text)\n",
    "\n",
    "        logging.info(f\"Saved text for {article_id} to {txt_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "92759545-f01c-4d95-b11f-5332a59945d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text(df_cell_open_access, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370b99dc-b1d0-41b4-b543-b4d0ed244b56",
   "metadata": {},
   "source": [
    "### [American Journal of Human Biology](https://onlinelibrary.wiley.com/journal/15206300?msockid=0525cb73d9a76a060b80df20d87e6b4b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de4e155-1193-48d3-af9e-3963d57fd40d",
   "metadata": {},
   "source": [
    "#### Create output subdirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d264e64b-efd0-4c4c-b92c-0d70818a3da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: cl_st2_ph31_eyamrog\\ajhb\n"
     ]
    }
   ],
   "source": [
    "# 'American Journal of Human Biology'\n",
    "id = 'ajhb'\n",
    "path = os.path.join(output_directory, id)\n",
    "create_directory(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d4a177-82d8-4d56-a7f5-5e3387ef9c01",
   "metadata": {},
   "source": [
    "#### Import the data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1ba78c8e-6e20-4bf9-aba7-c228663f2abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_american_journal_human_biology_open_access = pd.read_json(f\"{input_directory}/american_journal_human_biology_open_access.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f7735dfe-bb52-4849-9395-318240553e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_american_journal_human_biology_open_access['Published'] = pd.to_datetime(df_american_journal_human_biology_open_access['Published'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4d30bd6e-8732-4e3a-be21-066810a99911",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_american_journal_human_biology_open_access = df_american_journal_human_biology_open_access.loc[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6be0feb6-3c47-477e-9aa3-2f66619c6dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://onlinelibrary.wiley.com/doi/10.1002/ajhb.23389',\n",
       " 'https://onlinelibrary.wiley.com/doi/10.1002/ajhb.23350',\n",
       " 'https://onlinelibrary.wiley.com/doi/10.1002/ajhb.23340',\n",
       " 'https://onlinelibrary.wiley.com/doi/10.1002/ajhb.23339',\n",
       " 'https://onlinelibrary.wiley.com/doi/10.1002/ajhb.23407']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ajhb_urls = df_american_journal_human_biology_open_access['URL'].tolist()\n",
    "ajhb_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c0deb5bb-25ca-4efb-b6d6-c657a309b4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(df, path):\n",
    "    \"\"\"Extracts text from HTML files and saves as text files.\"\"\"\n",
    "\n",
    "    for article_id in df['ID']:\n",
    "        html_file = os.path.join(path, f\"{article_id}.html\")\n",
    "        txt_file = os.path.join(path, f\"{article_id}.txt\")\n",
    "\n",
    "        # Check if the HTML file exists\n",
    "        if not os.path.exists(html_file):\n",
    "            logging.error(f\"Skipping {html_file}: File not found\")\n",
    "            continue\n",
    "\n",
    "        # Read HTML content\n",
    "        with open(html_file, 'r', encoding='utf-8') as file:\n",
    "            soup = BeautifulSoup(file, 'lxml')\n",
    "\n",
    "        # Initialise text variable\n",
    "        text = ''\n",
    "\n",
    "        # Web Scraping - Begin\n",
    "\n",
    "        # Extract the 'Title'\n",
    "        title = soup.find('h1', class_='citation__title')\n",
    "        if title:\n",
    "            title_text = ' '.join(title.get_text(' ', strip=True).split())\n",
    "            text += f\"Title: {title_text}\\n\\n\"\n",
    "\n",
    "        # Capture the 'article body'\n",
    "        article_body = soup.find('div', class_='article__body')\n",
    "        \n",
    "        # Extract the 'Abstract'\n",
    "        if article_body:\n",
    "            abstract_section = article_body.find('section', class_='article-section__abstract')\n",
    "            if abstract_section:\n",
    "                h2_title = abstract_section.find('h2')\n",
    "                if h2_title:\n",
    "                    h2_title_text = ' '.join(h2_title.get_text(' ', strip=True).split())\n",
    "                    text += f\"\\nAbstract: {h2_title_text}\\n\\n\"\n",
    "                abstract_content = abstract_section.find('div', class_='article-section__content en main')\n",
    "                if abstract_content:\n",
    "                    for paragraph in abstract_content.find_all('p', recursive=False):\n",
    "                        paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                        text += f\"{paragraph_text}\\n\"\n",
    "                    for section in abstract_content.find_all('section', recursive=False):\n",
    "                        h3_title = section.find('h3')\n",
    "                        if h3_title:\n",
    "                            h3_title_text = ' '.join(h3_title.get_text(' ', strip=True).split())\n",
    "                            text += f\"\\nAbstract_Section: {h3_title_text}\\n\\n\"\n",
    "                        for paragraph in section.find_all('p', recursive=False):\n",
    "                            paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                            text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "        # Extract the 'body'\n",
    "        if article_body:\n",
    "            body_section = article_body.find('section', class_='article-section article-section__full')\n",
    "            if body_section:\n",
    "                for h2_section in body_section.find_all('section', class_='article-section__content', recursive=False):\n",
    "                    h2_title = h2_section.find('h2')\n",
    "                    if h2_title:\n",
    "                        h2_title_text = ' '.join(h2_title.get_text(' ', strip=True).split())\n",
    "                        text += f\"\\nSection: {h2_title_text}\\n\\n\"\n",
    "                    for paragraph in h2_section.find_all('p', recursive=False):\n",
    "                        paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                        text += f\"{paragraph_text}\\n\"\n",
    "                    for h3_section in h2_section.find_all('section', recursive=False):\n",
    "                        #h3_title = h3_section.find('h3')\n",
    "                        #if h3_title:\n",
    "                        #    h3_title_text = ' '.join(h3_title.get_text(' ', strip=True).split())\n",
    "                        #    text += f\"\\nSection: {h3_title_text}\\n\\n\"\n",
    "                        for paragraph in h3_section.find_all('p', recursive=False):\n",
    "                            paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                            text += f\"{paragraph_text}\\n\"\n",
    "                        for h4_section in h3_section.find_all('section', recursive=False):\n",
    "                            #h4_title = h4_section.find('h4')\n",
    "                            #if h4_title:\n",
    "                            #    h4_title_text = ' '.join(h4_title.get_text(' ', strip=True).split())\n",
    "                            #    text += f\"\\nSection: {h4_title_text}\\n\\n\"\n",
    "                            for paragraph in h4_section.find_all('p', recursive=False):\n",
    "                                paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                                text += f\"{paragraph_text}\\n\"\n",
    "                            for h5_section in h4_section.find_all('section', recursive=False):\n",
    "                                #h5_title = h5_section.find('h5')\n",
    "                                #if h5_title:\n",
    "                                #    h5_title_text = ' '.join(h5_title.get_text(' ', strip=True).split())\n",
    "                                #    text += f\"\\nSection: {h5_title_text}\\n\\n\"\n",
    "                                for paragraph in h5_section.find_all('p', recursive=False):\n",
    "                                    paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                                    text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "        # Extract the 'Acknowledgements'\n",
    "        if body_section:\n",
    "            for h2_section in body_section.find_all('div', class_='article-section__content', recursive=False):\n",
    "                h2_title = h2_section.find('h2')\n",
    "                if h2_title:\n",
    "                    h2_title_text = ' '.join(h2_title.get_text(' ', strip=True).split())\n",
    "                    text += f\"\\nSection: {h2_title_text}\\n\\n\"\n",
    "                for paragraph in h2_section.find_all('p', recursive=False):\n",
    "                    paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                    text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "        # Web Scraping - End\n",
    "\n",
    "        # Save text to a text file\n",
    "        with open(txt_file, 'w', encoding='utf-8', newline='\\n') as file:\n",
    "            file.write(text)\n",
    "\n",
    "        logging.info(f\"Saved text for {article_id} to {txt_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c461d9d4-4fcf-4b7d-9deb-5daf68836c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text(df_american_journal_human_biology_open_access, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cc94d3-6310-4294-8ae4-83b0404f4187",
   "metadata": {},
   "source": [
    "## Human Sciences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5d6440-84a0-4644-8a3a-af917dcb3e8a",
   "metadata": {},
   "source": [
    "### [Annual Review of Anthropology](https://www.annualreviews.org/content/journals/anthro)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64a35c9-9690-4d2d-b0a3-8eb1255444d1",
   "metadata": {},
   "source": [
    "#### Create output subdirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d59df1e4-aebe-466e-8630-693ce28e42e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: cl_st2_ph31_eyamrog\\aran\n"
     ]
    }
   ],
   "source": [
    "# 'Annual Review of Anthropology'\n",
    "id = 'aran'\n",
    "path = os.path.join(output_directory, id)\n",
    "create_directory(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d204cc-d743-4d14-a04c-a3548aa87622",
   "metadata": {},
   "source": [
    "#### Import the data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3ee6c2c9-3860-443c-934e-db2e5ed2491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar_anthropology = pd.read_json(f\"{input_directory}/ar_anthropology.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2185a6ec-fff0-4da4-bd77-75059ff47428",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar_anthropology['Published'] = pd.to_datetime(df_ar_anthropology['Published'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "17909147-b309-4380-b737-e8718beed147",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar_anthropology = df_ar_anthropology.loc[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ba121fd1-8a05-4488-b2c3-ef81cb197916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.annualreviews.org/content/journals/10.1146/annurev-an-51-082222-100001',\n",
       " 'https://www.annualreviews.org/content/journals/10.1146/annurev-anthro-070120-111609',\n",
       " 'https://www.annualreviews.org/content/journals/10.1146/annurev-anthro-041420-112543',\n",
       " 'https://www.annualreviews.org/content/journals/10.1146/annurev-anthro-041420-102158',\n",
       " 'https://www.annualreviews.org/content/journals/10.1146/annurev-anthro-041420-013930']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aran_urls = df_ar_anthropology['URL'].tolist()\n",
    "aran_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a1d2322a-1c7c-4b01-b3d1-5c33a6a089c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(df, path):\n",
    "    \"\"\"Extracts text from HTML files and saves as text files.\"\"\"\n",
    "\n",
    "    for article_id in df['ID']:\n",
    "        html_file = os.path.join(path, f\"{article_id}.html\")\n",
    "        txt_file = os.path.join(path, f\"{article_id}.txt\")\n",
    "\n",
    "        # Check if the HTML file exists\n",
    "        if not os.path.exists(html_file):\n",
    "            logging.error(f\"Skipping {html_file}: File not found\")\n",
    "            continue\n",
    "\n",
    "        # Read HTML content\n",
    "        with open(html_file, 'r', encoding='utf-8') as file:\n",
    "            soup = BeautifulSoup(file, 'lxml')\n",
    "\n",
    "        # Initialise text variable\n",
    "        text = ''\n",
    "\n",
    "        # Extract the 'Title'\n",
    "        title_tag = soup.find('span', class_='article-title')\n",
    "        if title_tag:\n",
    "            title = ' '.join(title_tag.get_text(' ', strip=True).split())\n",
    "            text += f\"Title: {title}\\n\\n\"\n",
    "\n",
    "        # Extract 'article sections'\n",
    "        for section in soup.find_all('div', class_='articleSection'): # Finds all 'div.articleSection' elements (both top-level and nested).\n",
    "            if not section.find_parent('div', class_='articleSection'): # Keeps only the top-level sections because it filters out nested ones by checking if the 'div.articleSection' has a parent that is also 'div.articleSection'\n",
    "                \n",
    "                # Extract section title\n",
    "                section_title_tag = section.find('div', class_='tl-main-part title')\n",
    "                if section_title_tag:\n",
    "                    section_title = ' '.join(section_title_tag.get_text(' ', strip=True).split())\n",
    "                    text += f\"\\nSection: {section_title}\\n\\n\"\n",
    "\n",
    "                # Extract paragraphs (only from top-level sections)\n",
    "                for paragraph in section.find_all('p'):\n",
    "                    paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                    text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "                # Extract subsections\n",
    "                for subsection in section.find_all('div', recursive=False):\n",
    "                    #label_tag = subsection.find('span', class_='label')\n",
    "                    #if label_tag:\n",
    "                    #    label_text = ' '.join(label_tag.get_text(' ', strip=True).split())\n",
    "                    #    text += f\"\\nSection: {label_text} \"\n",
    "                    #subsection_tag = subsection.find('span', class_='tl-lowest-section')\n",
    "                    #if subsection_tag:\n",
    "                    #    subsection_title = ' '.join(subsection_tag.get_text(' ', strip=True).split())\n",
    "                    #    text += f\"{subsection_title}\\n\\n\"\n",
    "                    for paragraph in subsection.find_all('p'):\n",
    "                        paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                        text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "        # Save text to a text file\n",
    "        with open(txt_file, 'w', encoding='utf-8', newline='\\n') as file:\n",
    "            file.write(text)\n",
    "\n",
    "        logging.info(f\"Saved text for {article_id} to {txt_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "46bc6143-6fb8-488b-adc9-702f49e067cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text(df_ar_anthropology, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c370476d-ff6d-4792-8b59-44110bfd7036",
   "metadata": {},
   "source": [
    "### [Journal of Human Evolution](https://www.sciencedirect.com/journal/journal-of-human-evolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd720e6-1c5e-41fb-9cfe-e444ce84b0d8",
   "metadata": {},
   "source": [
    "#### Create output subdirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5a269fb7-9abb-41bb-b6d1-244838ec3b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: cl_st2_ph31_eyamrog\\jhue\n"
     ]
    }
   ],
   "source": [
    "# 'Journal of Human Evolution'\n",
    "id = 'jhue'\n",
    "path = os.path.join(output_directory, id)\n",
    "create_directory(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82aab5c9-e497-40f2-b207-4d788c4ce4a6",
   "metadata": {},
   "source": [
    "#### Import the data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b308c701-f7c2-46d0-beb8-79d0fb8136fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_journal_human_evolution_open_access = pd.read_json(f\"{input_directory}/journal_human_evolution_open_access.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8db31cdb-d34a-4f95-bfda-b212bcd3137b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_journal_human_evolution_open_access['Published'] = pd.to_datetime(df_journal_human_evolution_open_access['Published'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c6df1f31-4ca7-415c-b0eb-734899f27f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_journal_human_evolution_open_access = df_journal_human_evolution_open_access.loc[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "040ca999-b715-4851-ab3c-6ff262349245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.sciencedirect.com//science/article/pii/S0047248420300294',\n",
       " 'https://www.sciencedirect.com//science/article/pii/S0047248420301123',\n",
       " 'https://www.sciencedirect.com//science/article/pii/S0047248420301135',\n",
       " 'https://www.sciencedirect.com//science/article/pii/S0047248420301305',\n",
       " 'https://www.sciencedirect.com//science/article/pii/S0047248420301615']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jhue_urls = df_journal_human_evolution_open_access['URL'].tolist()\n",
    "jhue_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8b62cae3-a0e6-4fc0-9ba3-3953c65e5a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(df, path):\n",
    "    \"\"\"Extracts text from HTML files and saves as text files.\"\"\"\n",
    "\n",
    "    for article_id in df['ID']:\n",
    "        html_file = os.path.join(path, f\"{article_id}.html\")\n",
    "        txt_file = os.path.join(path, f\"{article_id}.txt\")\n",
    "\n",
    "        # Check if the HTML file exists\n",
    "        if not os.path.exists(html_file):\n",
    "            logging.error(f\"Skipping {html_file}: File not found\")\n",
    "            continue\n",
    "\n",
    "        # Read HTML content\n",
    "        with open(html_file, 'r', encoding='utf-8') as file:\n",
    "            soup = BeautifulSoup(file, 'lxml')\n",
    "\n",
    "        # Initialise text variable\n",
    "        text = ''\n",
    "\n",
    "        # Web Scraping - Begin\n",
    "\n",
    "        # Extract the 'Title'\n",
    "        title = soup.find('h1', id='screen-reader-main-title')\n",
    "        if title:\n",
    "            title_text = ' '.join(title.get_text(' ', strip=True).split())\n",
    "            text += f\"Title: {title_text}\\n\\n\"\n",
    "\n",
    "        # Capture the 'article body'\n",
    "        article_body = soup.find('article')\n",
    "        \n",
    "        # Extract the 'Abstract'\n",
    "        if article_body:\n",
    "            abstract_section = article_body.find('div', id='abstracts')\n",
    "            if abstract_section:\n",
    "                abstract_author = abstract_section.find('div', class_='abstract author')\n",
    "                #abstract_author = abstract_section.find('div', attrs={'class': 'abstract author'})\n",
    "                #abstract_author = abstract_section.find('div', class_=['abstract', 'author']) # Results in match for 'author' or 'author-highlights', failing to reinforce 'author'\n",
    "                if abstract_author:\n",
    "                    h2_title = abstract_author.find('h2')\n",
    "                    if h2_title:\n",
    "                        h2_title_text = ' '.join(h2_title.get_text(' ', strip=True).split())\n",
    "                        text += f\"\\nAbstract: {h2_title_text}\\n\\n\"\n",
    "                    abstract_content = abstract_author.find('div')\n",
    "                    if abstract_content:\n",
    "                        for paragraph in abstract_content.find_all('div', recursive=False):\n",
    "                            paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                            text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "        # Extract the 'body'\n",
    "        if article_body:\n",
    "            body_section = article_body.find('div', id='body')\n",
    "            if body_section:\n",
    "                body_section1 = body_section.find('div')\n",
    "                if body_section1:\n",
    "                    for h2_section in body_section1.find_all('section', recursive=False):\n",
    "                        h2_title = h2_section.find('h2')\n",
    "                        if h2_title:\n",
    "                            h2_title_text = ' '.join(h2_title.get_text(' ', strip=True).split())\n",
    "                            text += f\"\\nSection: {h2_title_text}\\n\\n\"\n",
    "                        for paragraph in h2_section.find_all('div', recursive=False):\n",
    "                            paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                            text += f\"{paragraph_text}\\n\"\n",
    "                        for h3_section in h2_section.find_all('section', recursive=False):\n",
    "                            #h3_title = h3_section.find('h3')\n",
    "                            #if h3_title:\n",
    "                            #    h3_title_text = ' '.join(h3_title.get_text(' ', strip=True).split())\n",
    "                            #    text += f\"\\nSection: {h3_title_text}\\n\\n\"\n",
    "                            for paragraph in h3_section.find_all('div', recursive=False):\n",
    "                                paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                                text += f\"{paragraph_text}\\n\"\n",
    "                            for h4_section in h3_section.find_all('section', recursive=False):\n",
    "                                #h4_title = h4_section.find('h4')\n",
    "                                #if h4_title:\n",
    "                                #    h4_title_text = ' '.join(h4_title.get_text(' ', strip=True).split())\n",
    "                                #    text += f\"\\nSection: {h4_title_text}\\n\\n\"\n",
    "                                for paragraph in h4_section.find_all('div', recursive=False):\n",
    "                                    paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                                    text += f\"{paragraph_text}\\n\"\n",
    "                                for h5_section in h4_section.find_all('section', recursive=False):\n",
    "                                    #h5_title = h5_section.find('h5')\n",
    "                                    #if h5_title:\n",
    "                                    #    h5_title_text = ' '.join(h5_title.get_text(' ', strip=True).split())\n",
    "                                    #    text += f\"\\nSection: {h5_title_text}\\n\\n\"\n",
    "                                    for paragraph in h5_section.find_all('div', recursive=False):\n",
    "                                        paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                                        text += f\"{paragraph_text}\\n\"\n",
    "                for body_section2 in body_section.find_all('section', recursive=False):\n",
    "                    h2_title = body_section2.find('h2')\n",
    "                    if h2_title:\n",
    "                        h2_title_text = ' '.join(h2_title.get_text(' ', strip=True).split())\n",
    "                        text += f\"\\nSection: {h2_title_text}\\n\\n\"\n",
    "                    for paragraph in body_section2.find_all('div', recursive=False):\n",
    "                        paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                        text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "        # Web Scraping - End\n",
    "\n",
    "        # Save text to a text file\n",
    "        with open(txt_file, 'w', encoding='utf-8', newline='\\n') as file:\n",
    "            file.write(text)\n",
    "\n",
    "        logging.info(f\"Saved text for {article_id} to {txt_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "90d3e4cd-d025-441c-a13f-a575d58e1a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text(df_journal_human_evolution_open_access, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aa4ade-f323-4614-ab65-535d3e6f3d1b",
   "metadata": {},
   "source": [
    "## Applied Social Sciences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd479fd-73c9-4ca6-ae2f-f9c9dd07ab28",
   "metadata": {},
   "source": [
    "### [Journal of Applied Social Science](https://journals.sagepub.com/home/jax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb6b35a-a8fc-4315-b640-d4b7856e6006",
   "metadata": {},
   "source": [
    "#### Create output subdirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "26a3d115-b22e-4e14-8553-67ca8d073dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: cl_st2_ph31_eyamrog\\jasc\n"
     ]
    }
   ],
   "source": [
    "# 'Journal of Applied Social Science'\n",
    "id = 'jasc'\n",
    "path = os.path.join(output_directory, id)\n",
    "create_directory(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa6d600-ed05-432d-921b-ae1839206143",
   "metadata": {},
   "source": [
    "#### Import the data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "36125d36-91ac-43f1-97c4-00285fb8634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_journal_applied_social_science_open_access = pd.read_json(f\"{input_directory}/journal_applied_social_science_open_access.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "47c8cc6c-479e-49cb-a826-f2be51cae4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_journal_applied_social_science_open_access['Published'] = pd.to_datetime(df_journal_applied_social_science_open_access['Published'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5284a66c-b4f9-4468-92b1-0a6bda19b850",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_journal_applied_social_science_open_access = df_journal_applied_social_science_open_access.loc[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "434d167c-e3b7-4d07-aa58-e08a32533053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://journals.sagepub.com/doi/abs/10.1177/1936724420980374',\n",
       " 'https://journals.sagepub.com/doi/abs/10.1177/19367244211003471',\n",
       " 'https://journals.sagepub.com/doi/abs/10.1177/1936724421998275',\n",
       " 'https://journals.sagepub.com/doi/abs/10.1177/19367244211000709',\n",
       " 'https://journals.sagepub.com/doi/abs/10.1177/19367244211000271']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jasc_urls = df_journal_applied_social_science_open_access['URL'].tolist()\n",
    "jasc_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "878d682f-24d4-426f-827f-7e144ca5bbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(df, path):\n",
    "    \"\"\"Extracts text from HTML files and saves as text files.\"\"\"\n",
    "\n",
    "    for article_id in df['ID']:\n",
    "        html_file = os.path.join(path, f\"{article_id}.html\")\n",
    "        txt_file = os.path.join(path, f\"{article_id}.txt\")\n",
    "\n",
    "        # Check if the HTML file exists\n",
    "        if not os.path.exists(html_file):\n",
    "            logging.error(f\"Skipping {html_file}: File not found\")\n",
    "            continue\n",
    "\n",
    "        # Read HTML content\n",
    "        with open(html_file, 'r', encoding='utf-8') as file:\n",
    "            soup = BeautifulSoup(file, 'lxml')\n",
    "\n",
    "        # Initialise text variable\n",
    "        text = ''\n",
    "\n",
    "        # Web Scraping - Begin\n",
    "\n",
    "        # Extract the 'Title'\n",
    "        title = soup.find('h1', property='name')\n",
    "        if title:\n",
    "            title_text = ' '.join(title.get_text(' ', strip=True).split())\n",
    "            text += f\"Title: {title_text}\\n\\n\"\n",
    "\n",
    "        # Extract the 'Abstract'\n",
    "        abstract_section = soup.find('div', id='abstracts')\n",
    "        if abstract_section:\n",
    "            author_abstract_section = abstract_section.find('section', id='abstract')\n",
    "            if author_abstract_section:\n",
    "                author_abstract_h2_title = author_abstract_section.find('h2', property='name')\n",
    "                if author_abstract_h2_title:\n",
    "                    author_abstract_h2_title_text = ' '.join(author_abstract_h2_title.get_text(' ', strip=True).split())\n",
    "                    text += f\"\\nAbstract: {author_abstract_h2_title_text}\\n\\n\"\n",
    "                for paragraph in author_abstract_section.find_all('div', role='paragraph', recursive=False):\n",
    "                    paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                    text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "        # Extract the 'body'\n",
    "        body_section = soup.find('section', id='bodymatter')\n",
    "        if body_section:\n",
    "            body_core_container = body_section.find('div', class_='core-container')\n",
    "            if body_core_container:\n",
    "                # Extract the initial paragraphs that precede the first section (initial paragraphs)\n",
    "                text += f\"\\nSection: Initial Paragraphs\\n\\n\" # Insert the 'Initial Paragraphs' title\n",
    "                for paragraph in body_core_container.find_all('div', role='paragraph', recursive=False):\n",
    "                    paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                    text += f\"{paragraph_text}\\n\"\n",
    "                # Extract sections\n",
    "                for section_h2 in body_core_container.find_all('section', recursive=False):\n",
    "                    # Extract section title\n",
    "                    section_h2_title = section_h2.find('h2')\n",
    "                    if section_h2_title:\n",
    "                        section_h2_title_text = ' '.join(section_h2_title.get_text(' ', strip=True).split())\n",
    "                        text += f\"\\nSection: {section_h2_title_text}\\n\\n\"\n",
    "                    # Extract section paragraphs\n",
    "                    for paragraph in section_h2.find_all('div', role='paragraph', recursive=False):\n",
    "                        paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                        text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "                    # Extract subsections\n",
    "                    for section_h3 in section_h2.find_all('section', recursive=False):\n",
    "                        ## Extract subsection title\n",
    "                        #section_h3_title = section_h3.find('h3')\n",
    "                        #if section_h3_title:\n",
    "                        #    section_h3_title_text = ' '.join(section_h3_title.get_text(' ', strip=True).split())\n",
    "                        #    text += f\"\\nSubsection: {section_h3_title_text}\\n\\n\"\n",
    "                        # Extract subsection paragraphs\n",
    "                        for paragraph in section_h3.find_all('div', role='paragraph', recursive=False):\n",
    "                            paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                            text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "                        # Extract subsubsections\n",
    "                        for section_h4 in section_h3.find_all('section', recursive=False):\n",
    "                            ## Extract subsubsection title\n",
    "                            #section_h4_title = section_h4.find('h4')\n",
    "                            #if section_h4_title:\n",
    "                            #    section_h4_title_text = ' '.join(section_h4_title.get_text(' ', strip=True).split())\n",
    "                            #    text += f\"\\nSubsubsection: {section_h4_title_text}\\n\\n\"\n",
    "                            # Extract subsubsection paragraphs\n",
    "                            for paragraph in section_h4.find_all('div', role='paragraph', recursive=False):\n",
    "                                paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                                text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "                            # Extract subsubsubsections\n",
    "                            for section_h5 in section_h4.find_all('section', recursive=False):\n",
    "                                ## Extract subsubsubsection title\n",
    "                                #section_h5_title = section_h5.find('h5')\n",
    "                                #if section_h5_title:\n",
    "                                #    section_h5_title_text = ' '.join(section_h5_title.get_text(' ', strip=True).split())\n",
    "                                #    text += f\"\\nSubsubsubsection: {section_h5_title_text}\\n\\n\"\n",
    "                                # Extract subsubsubsection paragraphs\n",
    "                                for paragraph in section_h5.find_all('div', role='paragraph', recursive=False):\n",
    "                                    paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                                    text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "        # Web Scraping - End\n",
    "\n",
    "        # Save text to a text file\n",
    "        with open(txt_file, 'w', encoding='utf-8', newline='\\n') as file:\n",
    "            file.write(text)\n",
    "\n",
    "        logging.info(f\"Saved text for {article_id} to {txt_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9af0d180-576b-48cf-81b7-fba89db12a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text(df_journal_applied_social_science_open_access, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18de5943-41e4-42f6-b412-5989543c79e1",
   "metadata": {},
   "source": [
    "### [Journal of Social Issues](https://spssi.onlinelibrary.wiley.com/journal/15404560)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f22b0e-fe71-4df7-ab06-b27786aaaa08",
   "metadata": {},
   "source": [
    "#### Create output subdirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0a4bf949-c090-4124-adb4-319fe2a3f29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: cl_st2_ph31_eyamrog\\jsoi\n"
     ]
    }
   ],
   "source": [
    "# 'Journal of Social Issues'\n",
    "id = 'jsoi'\n",
    "path = os.path.join(output_directory, id)\n",
    "create_directory(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb75010-7601-4ad4-951a-c49822930a63",
   "metadata": {},
   "source": [
    "#### Import the data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7e56be63-dcf8-4c64-8606-90b333d9a945",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_journal_social_issues_open_access = pd.read_json(f\"{input_directory}/journal_social_issues_open_access.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "134393ea-72a1-4c03-aa7f-afce17ff7306",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_journal_social_issues_open_access['Published'] = pd.to_datetime(df_journal_social_issues_open_access['Published'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d4b1d6a4-e73e-4d61-8140-a88fcc0b2ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_journal_social_issues_open_access = df_journal_social_issues_open_access.loc[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d56d10e0-3285-4278-9ebe-0f6ab171225d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://spssi.onlinelibrary.wiley.com/doi/10.1111/josi.12376',\n",
       " 'https://spssi.onlinelibrary.wiley.com/doi/10.1111/josi.12369',\n",
       " 'https://spssi.onlinelibrary.wiley.com/doi/10.1111/josi.12360',\n",
       " 'https://spssi.onlinelibrary.wiley.com/doi/10.1111/josi.12398',\n",
       " 'https://spssi.onlinelibrary.wiley.com/doi/10.1111/josi.12399']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsoi_urls = df_journal_social_issues_open_access['URL'].tolist()\n",
    "jsoi_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f4687f3a-482a-449d-a7d2-72679247ef45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(df, path):\n",
    "    \"\"\"Extracts text from HTML files and saves as text files.\"\"\"\n",
    "\n",
    "    for article_id in df['ID']:\n",
    "        html_file = os.path.join(path, f\"{article_id}.html\")\n",
    "        txt_file = os.path.join(path, f\"{article_id}.txt\")\n",
    "\n",
    "        # Check if the HTML file exists\n",
    "        if not os.path.exists(html_file):\n",
    "            logging.error(f\"Skipping {html_file}: File not found\")\n",
    "            continue\n",
    "\n",
    "        # Read HTML content\n",
    "        with open(html_file, 'r', encoding='utf-8') as file:\n",
    "            soup = BeautifulSoup(file, 'lxml')\n",
    "\n",
    "        # Initialise text variable\n",
    "        text = ''\n",
    "\n",
    "        # Web Scraping - Begin\n",
    "\n",
    "        # Extract the 'Title'\n",
    "        title = soup.find('h1', class_='citation__title')\n",
    "        if title:\n",
    "            title_text = ' '.join(title.get_text(' ', strip=True).split())\n",
    "            text += f\"Title: {title_text}\\n\\n\"\n",
    "\n",
    "        # Capture the 'article body'\n",
    "        article_body = soup.find('div', class_='article__body')\n",
    "        \n",
    "        # Extract the 'Abstract'\n",
    "        if article_body:\n",
    "            abstract_section = article_body.find('section', class_='article-section__abstract')\n",
    "            if abstract_section:\n",
    "                h2_title = abstract_section.find('h2')\n",
    "                if h2_title:\n",
    "                    h2_title_text = ' '.join(h2_title.get_text(' ', strip=True).split())\n",
    "                    text += f\"\\nAbstract: {h2_title_text}\\n\\n\"\n",
    "                abstract_content = abstract_section.find('div', class_='article-section__content en main')\n",
    "                if abstract_content:\n",
    "                    for paragraph in abstract_content.find_all('p', recursive=False):\n",
    "                        paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                        text += f\"{paragraph_text}\\n\"\n",
    "                    for section in abstract_content.find_all('section', recursive=False):\n",
    "                        h3_title = section.find('h3')\n",
    "                        if h3_title:\n",
    "                            h3_title_text = ' '.join(h3_title.get_text(' ', strip=True).split())\n",
    "                            text += f\"\\nAbstract_Section: {h3_title_text}\\n\\n\"\n",
    "                        for paragraph in section.find_all('p', recursive=False):\n",
    "                            paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                            text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "        # Extract the 'body'\n",
    "        if article_body:\n",
    "            body_section = article_body.find('section', class_='article-section article-section__full')\n",
    "            if body_section:\n",
    "                for h2_section in body_section.find_all('section', class_='article-section__content', recursive=False):\n",
    "                    h2_title = h2_section.find('h2')\n",
    "                    if h2_title:\n",
    "                        h2_title_text = ' '.join(h2_title.get_text(' ', strip=True).split())\n",
    "                        text += f\"\\nSection: {h2_title_text}\\n\\n\"\n",
    "                    for paragraph in h2_section.find_all('p', recursive=False):\n",
    "                        paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                        text += f\"{paragraph_text}\\n\"\n",
    "                    for h3_section in h2_section.find_all('section', recursive=False):\n",
    "                        #h3_title = h3_section.find('h3')\n",
    "                        #if h3_title:\n",
    "                        #    h3_title_text = ' '.join(h3_title.get_text(' ', strip=True).split())\n",
    "                        #    text += f\"\\nSection: {h3_title_text}\\n\\n\"\n",
    "                        for paragraph in h3_section.find_all('p', recursive=False):\n",
    "                            paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                            text += f\"{paragraph_text}\\n\"\n",
    "                        for h4_section in h3_section.find_all('section', recursive=False):\n",
    "                            #h4_title = h4_section.find('h4')\n",
    "                            #if h4_title:\n",
    "                            #    h4_title_text = ' '.join(h4_title.get_text(' ', strip=True).split())\n",
    "                            #    text += f\"\\nSection: {h4_title_text}\\n\\n\"\n",
    "                            for paragraph in h4_section.find_all('p', recursive=False):\n",
    "                                paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                                text += f\"{paragraph_text}\\n\"\n",
    "                            for h5_section in h4_section.find_all('section', recursive=False):\n",
    "                                #h5_title = h5_section.find('h5')\n",
    "                                #if h5_title:\n",
    "                                #    h5_title_text = ' '.join(h5_title.get_text(' ', strip=True).split())\n",
    "                                #    text += f\"\\nSection: {h5_title_text}\\n\\n\"\n",
    "                                for paragraph in h5_section.find_all('p', recursive=False):\n",
    "                                    paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                                    text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "        # Extract the 'Acknowledgements'\n",
    "        if body_section:\n",
    "            for h2_section in body_section.find_all('div', class_='article-section__content', recursive=False):\n",
    "                h2_title = h2_section.find('h2')\n",
    "                if h2_title:\n",
    "                    h2_title_text = ' '.join(h2_title.get_text(' ', strip=True).split())\n",
    "                    text += f\"\\nSection: {h2_title_text}\\n\\n\"\n",
    "                for paragraph in h2_section.find_all('p', recursive=False):\n",
    "                    paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                    text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "        # Web Scraping - End\n",
    "\n",
    "        # Save text to a text file\n",
    "        with open(txt_file, 'w', encoding='utf-8', newline='\\n') as file:\n",
    "            file.write(text)\n",
    "\n",
    "        logging.info(f\"Saved text for {article_id} to {txt_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0b2c8446-17a8-4f7a-977e-5602edd08e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text(df_journal_social_issues_open_access, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c58a660-738e-4042-ae4b-3ca8efc965d1",
   "metadata": {},
   "source": [
    "### [Social Science & Medicine](https://www.sciencedirect.com/journal/social-science-and-medicine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d7380c-e1cf-469d-bcd5-1e75f0a39cf2",
   "metadata": {},
   "source": [
    "#### Create output subdirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8d395fb1-7042-4a3a-ac74-18c9821bc5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: cl_st2_ph31_eyamrog\\socm\n"
     ]
    }
   ],
   "source": [
    "# 'Social Science & Medicine'\n",
    "id = 'socm'\n",
    "path = os.path.join(output_directory, id)\n",
    "create_directory(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97348f8b-959f-4fe6-b6e0-33dc3959c742",
   "metadata": {},
   "source": [
    "#### Import the data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e07c506d-f793-434d-bb0c-b1df690735cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_social_science_medicine_open_access = pd.read_json(f\"{input_directory}/social_science_medicine_open_access.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b58ee27d-ba9b-494a-bb19-eaa37bc59c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_social_science_medicine_open_access['Published'] = pd.to_datetime(df_social_science_medicine_open_access['Published'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "46bd8103-501b-4edd-8063-dcb8cb51cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_social_science_medicine_open_access = df_social_science_medicine_open_access.loc[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ead7b9e3-331c-4f12-b75b-7f85ffb945df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.sciencedirect.com//science/article/pii/S0277953619305933',\n",
       " 'https://www.sciencedirect.com//science/article/pii/S0277953619306288',\n",
       " 'https://www.sciencedirect.com//science/article/pii/S0277953619306379',\n",
       " 'https://www.sciencedirect.com//science/article/pii/S0277953619306434',\n",
       " 'https://www.sciencedirect.com//science/article/pii/S0277953619306628']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "socm_urls = df_social_science_medicine_open_access['URL'].tolist()\n",
    "socm_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "06bda20b-7145-4081-982d-0671372e783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(df, path):\n",
    "    \"\"\"Extracts text from HTML files and saves as text files.\"\"\"\n",
    "\n",
    "    for article_id in df['ID']:\n",
    "        html_file = os.path.join(path, f\"{article_id}.html\")\n",
    "        txt_file = os.path.join(path, f\"{article_id}.txt\")\n",
    "\n",
    "        # Check if the HTML file exists\n",
    "        if not os.path.exists(html_file):\n",
    "            logging.error(f\"Skipping {html_file}: File not found\")\n",
    "            continue\n",
    "\n",
    "        # Read HTML content\n",
    "        with open(html_file, 'r', encoding='utf-8') as file:\n",
    "            soup = BeautifulSoup(file, 'lxml')\n",
    "\n",
    "        # Initialise text variable\n",
    "        text = ''\n",
    "\n",
    "        # Web Scraping - Begin\n",
    "\n",
    "        # Extract the 'Title'\n",
    "        title = soup.find('h1', id='screen-reader-main-title')\n",
    "        if title:\n",
    "            title_text = ' '.join(title.get_text(' ', strip=True).split())\n",
    "            text += f\"Title: {title_text}\\n\\n\"\n",
    "\n",
    "        # Capture the 'article body'\n",
    "        article_body = soup.find('article')\n",
    "        \n",
    "        # Extract the 'Abstract'\n",
    "        if article_body:\n",
    "            abstract_section = article_body.find('div', id='abstracts')\n",
    "            if abstract_section:\n",
    "                abstract_author = abstract_section.find('div', class_='abstract author')\n",
    "                #abstract_author = abstract_section.find('div', attrs={'class': 'abstract author'})\n",
    "                #abstract_author = abstract_section.find('div', class_=['abstract', 'author']) # Results in match for 'author' or 'author-highlights', failing to reinforce 'author'\n",
    "                if abstract_author:\n",
    "                    h2_title = abstract_author.find('h2')\n",
    "                    if h2_title:\n",
    "                        h2_title_text = ' '.join(h2_title.get_text(' ', strip=True).split())\n",
    "                        text += f\"\\nAbstract: {h2_title_text}\\n\\n\"\n",
    "                    for abstract_content in abstract_author.find_all('div'):\n",
    "                        h3_title = abstract_content.find('h3')\n",
    "                        if h3_title:\n",
    "                            h3_title_text = ' '.join(h3_title.get_text(' ', strip=True).split())\n",
    "                            text += f\"\\nAbstract_Section: {h3_title_text}\\n\\n\"\n",
    "                        for paragraph in abstract_content.find_all('div', recursive=False):\n",
    "                            paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                            text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "        # Extract the 'body'\n",
    "        if article_body:\n",
    "            body_section = article_body.find('div', id='body')\n",
    "            if body_section:\n",
    "                body_section1 = body_section.find('div')\n",
    "                if body_section1:\n",
    "                    for h2_section in body_section1.find_all('section', recursive=False):\n",
    "                        h2_title = h2_section.find('h2')\n",
    "                        if h2_title:\n",
    "                            h2_title_text = ' '.join(h2_title.get_text(' ', strip=True).split())\n",
    "                            text += f\"\\nSection: {h2_title_text}\\n\\n\"\n",
    "                        for paragraph in h2_section.find_all('div', recursive=False):\n",
    "                            paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                            text += f\"{paragraph_text}\\n\"\n",
    "                        for h3_section in h2_section.find_all('section', recursive=False):\n",
    "                            #h3_title = h3_section.find('h3')\n",
    "                            #if h3_title:\n",
    "                            #    h3_title_text = ' '.join(h3_title.get_text(' ', strip=True).split())\n",
    "                            #    text += f\"\\nSection: {h3_title_text}\\n\\n\"\n",
    "                            for paragraph in h3_section.find_all('div', recursive=False):\n",
    "                                paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                                text += f\"{paragraph_text}\\n\"\n",
    "                            for h4_section in h3_section.find_all('section', recursive=False):\n",
    "                                #h4_title = h4_section.find('h4')\n",
    "                                #if h4_title:\n",
    "                                #    h4_title_text = ' '.join(h4_title.get_text(' ', strip=True).split())\n",
    "                                #    text += f\"\\nSection: {h4_title_text}\\n\\n\"\n",
    "                                for paragraph in h4_section.find_all('div', recursive=False):\n",
    "                                    paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                                    text += f\"{paragraph_text}\\n\"\n",
    "                                for h5_section in h4_section.find_all('section', recursive=False):\n",
    "                                    #h5_title = h5_section.find('h5')\n",
    "                                    #if h5_title:\n",
    "                                    #    h5_title_text = ' '.join(h5_title.get_text(' ', strip=True).split())\n",
    "                                    #    text += f\"\\nSection: {h5_title_text}\\n\\n\"\n",
    "                                    for paragraph in h5_section.find_all('div', recursive=False):\n",
    "                                        paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                                        text += f\"{paragraph_text}\\n\"\n",
    "                for body_section2 in body_section.find_all('section', recursive=False):\n",
    "                    h2_title = body_section2.find('h2')\n",
    "                    if h2_title:\n",
    "                        h2_title_text = ' '.join(h2_title.get_text(' ', strip=True).split())\n",
    "                        text += f\"\\nSection: {h2_title_text}\\n\\n\"\n",
    "                    for paragraph in body_section2.find_all('div', recursive=False):\n",
    "                        paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                        text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "        # Web Scraping - End\n",
    "\n",
    "        # Save text to a text file\n",
    "        with open(txt_file, 'w', encoding='utf-8', newline='\\n') as file:\n",
    "            file.write(text)\n",
    "\n",
    "        logging.info(f\"Saved text for {article_id} to {txt_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e8f55633-2d00-4aa4-83c8-bd9653f32c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text(df_social_science_medicine_open_access, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10419f0d-9db9-4bc2-9520-89192d9be03b",
   "metadata": {},
   "source": [
    "## Linguistics, literature and arts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff024cb5-3472-4a81-906e-d561e9cfdbcc",
   "metadata": {},
   "source": [
    "### [Applied Corpus Linguistics](https://www.sciencedirect.com/journal/applied-corpus-linguistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88c80cd-6876-4612-8fc3-ebb320f705c4",
   "metadata": {},
   "source": [
    "#### Create output subdirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c1da3a5b-a6c8-4201-b1e3-99e826db2099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: cl_st2_ph31_eyamrog\\apcl\n"
     ]
    }
   ],
   "source": [
    "# 'Applied Corpus Linguistics'\n",
    "id = 'apcl'\n",
    "path = os.path.join(output_directory, id)\n",
    "create_directory(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10ccdec-1da6-48ba-b246-b8ad6fa77c9d",
   "metadata": {},
   "source": [
    "#### Import the data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "775450ac-f2fa-4738-a513-e6be8f69f166",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applied_corpus_linguistics_open_access = pd.read_json(f\"{input_directory}/applied_corpus_linguistics_open_access.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7a2961f6-aebb-4d43-9649-cf8c024daa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applied_corpus_linguistics_open_access['Published'] = pd.to_datetime(df_applied_corpus_linguistics_open_access['Published'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e2d4398e-6951-4453-814c-195ccd587a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_applied_corpus_linguistics_open_access = df_applied_corpus_linguistics_open_access.loc[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4d7dfe17-918c-4855-9e5b-26472097f947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.sciencedirect.com//science/article/pii/S2666799121000010',\n",
       " 'https://www.sciencedirect.com//science/article/pii/S2666799121000083',\n",
       " 'https://www.sciencedirect.com//science/article/pii/S2666799121000101',\n",
       " 'https://www.sciencedirect.com//science/article/pii/S2666799121000113',\n",
       " 'https://www.sciencedirect.com//science/article/pii/S266679912200003X']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apcl_urls = df_applied_corpus_linguistics_open_access['URL'].tolist()\n",
    "apcl_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "56a9d9e4-deb0-4f12-9122-9b7ff8940635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(df, path):\n",
    "    \"\"\"Extracts text from HTML files and saves as text files.\"\"\"\n",
    "\n",
    "    for article_id in df['ID']:\n",
    "        html_file = os.path.join(path, f\"{article_id}.html\")\n",
    "        txt_file = os.path.join(path, f\"{article_id}.txt\")\n",
    "\n",
    "        # Check if the HTML file exists\n",
    "        if not os.path.exists(html_file):\n",
    "            logging.error(f\"Skipping {html_file}: File not found\")\n",
    "            continue\n",
    "\n",
    "        # Read HTML content\n",
    "        with open(html_file, 'r', encoding='utf-8') as file:\n",
    "            soup = BeautifulSoup(file, 'lxml')\n",
    "\n",
    "        # Initialise text variable\n",
    "        text = ''\n",
    "\n",
    "        # Web Scraping - Begin\n",
    "\n",
    "        # Extract the 'Title'\n",
    "        title = soup.find('h1', id='screen-reader-main-title')\n",
    "        if title:\n",
    "            title_text = ' '.join(title.get_text(' ', strip=True).split())\n",
    "            text += f\"Title: {title_text}\\n\\n\"\n",
    "\n",
    "        # Capture the 'article body'\n",
    "        article_body = soup.find('article')\n",
    "        \n",
    "        # Extract the 'Abstract'\n",
    "        if article_body:\n",
    "            abstract_section = article_body.find('div', id='abstracts')\n",
    "            if abstract_section:\n",
    "                abstract_author = abstract_section.find('div', class_='abstract author')\n",
    "                #abstract_author = abstract_section.find('div', attrs={'class': 'abstract author'})\n",
    "                #abstract_author = abstract_section.find('div', class_=['abstract', 'author']) # Results in match for 'author' or 'author-highlights', failing to reinforce 'author'\n",
    "                if abstract_author:\n",
    "                    h2_title = abstract_author.find('h2')\n",
    "                    if h2_title:\n",
    "                        h2_title_text = ' '.join(h2_title.get_text(' ', strip=True).split())\n",
    "                        text += f\"\\nAbstract: {h2_title_text}\\n\\n\"\n",
    "                    abstract_content = abstract_author.find('div')\n",
    "                    if abstract_content:\n",
    "                        for paragraph in abstract_content.find_all('div', recursive=False):\n",
    "                            paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                            text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "        # Extract the 'body'\n",
    "        if article_body:\n",
    "            body_section = article_body.find('div', id='body')\n",
    "            if body_section:\n",
    "                body_section1 = body_section.find('div')\n",
    "                if body_section1:\n",
    "                    for h2_section in body_section1.find_all('section', recursive=False):\n",
    "                        h2_title = h2_section.find('h2')\n",
    "                        if h2_title:\n",
    "                            h2_title_text = ' '.join(h2_title.get_text(' ', strip=True).split())\n",
    "                            text += f\"\\nSection: {h2_title_text}\\n\\n\"\n",
    "                        for paragraph in h2_section.find_all('div', recursive=False):\n",
    "                            paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                            text += f\"{paragraph_text}\\n\"\n",
    "                        for h3_section in h2_section.find_all('section', recursive=False):\n",
    "                            #h3_title = h3_section.find('h3')\n",
    "                            #if h3_title:\n",
    "                            #    h3_title_text = ' '.join(h3_title.get_text(' ', strip=True).split())\n",
    "                            #    text += f\"\\nSection: {h3_title_text}\\n\\n\"\n",
    "                            for paragraph in h3_section.find_all('div', recursive=False):\n",
    "                                paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                                text += f\"{paragraph_text}\\n\"\n",
    "                            for h4_section in h3_section.find_all('section', recursive=False):\n",
    "                                #h4_title = h4_section.find('h4')\n",
    "                                #if h4_title:\n",
    "                                #    h4_title_text = ' '.join(h4_title.get_text(' ', strip=True).split())\n",
    "                                #    text += f\"\\nSection: {h4_title_text}\\n\\n\"\n",
    "                                for paragraph in h4_section.find_all('div', recursive=False):\n",
    "                                    paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                                    text += f\"{paragraph_text}\\n\"\n",
    "                                for h5_section in h4_section.find_all('section', recursive=False):\n",
    "                                    #h5_title = h5_section.find('h5')\n",
    "                                    #if h5_title:\n",
    "                                    #    h5_title_text = ' '.join(h5_title.get_text(' ', strip=True).split())\n",
    "                                    #    text += f\"\\nSection: {h5_title_text}\\n\\n\"\n",
    "                                    for paragraph in h5_section.find_all('div', recursive=False):\n",
    "                                        paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                                        text += f\"{paragraph_text}\\n\"\n",
    "                for body_section2 in body_section.find_all('section', recursive=False):\n",
    "                    h2_title = body_section2.find('h2')\n",
    "                    if h2_title:\n",
    "                        h2_title_text = ' '.join(h2_title.get_text(' ', strip=True).split())\n",
    "                        text += f\"\\nSection: {h2_title_text}\\n\\n\"\n",
    "                    for paragraph in body_section2.find_all('div', recursive=False):\n",
    "                        paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                        text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "        # Web Scraping - End\n",
    "\n",
    "        # Save text to a text file\n",
    "        with open(txt_file, 'w', encoding='utf-8', newline='\\n') as file:\n",
    "            file.write(text)\n",
    "\n",
    "        logging.info(f\"Saved text for {article_id} to {txt_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "352c72f6-9b55-4ee3-8534-b8ff57cb0751",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text(df_applied_corpus_linguistics_open_access, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d30cfb-e8e2-4a2e-aa15-7c9a532ca37c",
   "metadata": {},
   "source": [
    "### [Journal of English Linguistics](https://journals.sagepub.com/home/eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794c5c3c-cdbe-4d56-b4ff-c9736ffd8394",
   "metadata": {},
   "source": [
    "#### Create output subdirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "694430bf-f9b7-4816-aef2-95076f392183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: cl_st2_ph31_eyamrog\\jenl\n"
     ]
    }
   ],
   "source": [
    "# 'Journal of English Linguistics'\n",
    "id = 'jenl'\n",
    "path = os.path.join(output_directory, id)\n",
    "create_directory(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29faebef-90a2-4ff1-a484-a722e1c90cb3",
   "metadata": {},
   "source": [
    "#### Import the data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d4adc78a-4aca-4b63-842e-4427180fe8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_journal_english_linguistics_open_access = pd.read_json(f\"{input_directory}/journal_english_linguistics_open_access.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "476b4a62-f899-4811-85c3-6b9941a43a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_journal_english_linguistics_open_access['Published'] = pd.to_datetime(df_journal_english_linguistics_open_access['Published'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d3acb18c-b9e0-40bf-9b03-79ab6745bd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_journal_english_linguistics_open_access = df_journal_english_linguistics_open_access.loc[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4456ebee-6486-48ad-911e-d060e536a5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://journals.sagepub.com/doi/abs/10.1177/0075424220911067',\n",
       " 'https://journals.sagepub.com/doi/abs/10.1177/0075424220938949',\n",
       " 'https://journals.sagepub.com/doi/abs/10.1177/0075424220945008',\n",
       " 'https://journals.sagepub.com/doi/abs/10.1177/0075424220982063',\n",
       " 'https://journals.sagepub.com/doi/abs/10.1177/0075424220982649']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jenl_urls = df_journal_english_linguistics_open_access['URL'].tolist()\n",
    "jenl_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a2f154cb-f91d-4fa5-8ce1-1eabc3c1ef45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(df, path):\n",
    "    \"\"\"Extracts text from HTML files and saves as text files.\"\"\"\n",
    "\n",
    "    for article_id in df['ID']:\n",
    "        html_file = os.path.join(path, f\"{article_id}.html\")\n",
    "        txt_file = os.path.join(path, f\"{article_id}.txt\")\n",
    "\n",
    "        # Check if the HTML file exists\n",
    "        if not os.path.exists(html_file):\n",
    "            logging.error(f\"Skipping {html_file}: File not found\")\n",
    "            continue\n",
    "\n",
    "        # Read HTML content\n",
    "        with open(html_file, 'r', encoding='utf-8') as file:\n",
    "            soup = BeautifulSoup(file, 'lxml')\n",
    "\n",
    "        # Initialise text variable\n",
    "        text = ''\n",
    "\n",
    "        # Web Scraping - Begin\n",
    "\n",
    "        # Extract the 'Title'\n",
    "        title = soup.find('h1', property='name')\n",
    "        if title:\n",
    "            title_text = ' '.join(title.get_text(' ', strip=True).split())\n",
    "            text += f\"Title: {title_text}\\n\\n\"\n",
    "\n",
    "        # Extract the 'Abstract'\n",
    "        abstract_section = soup.find('div', id='abstracts')\n",
    "        if abstract_section:\n",
    "            author_abstract_section = abstract_section.find('section', id='abstract')\n",
    "            if author_abstract_section:\n",
    "                author_abstract_h2_title = author_abstract_section.find('h2', property='name')\n",
    "                if author_abstract_h2_title:\n",
    "                    author_abstract_h2_title_text = ' '.join(author_abstract_h2_title.get_text(' ', strip=True).split())\n",
    "                    text += f\"\\nAbstract: {author_abstract_h2_title_text}\\n\\n\"\n",
    "                for paragraph in author_abstract_section.find_all('div', role='paragraph', recursive=False):\n",
    "                    paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                    text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "        # Extract the 'body'\n",
    "        body_section = soup.find('section', id='bodymatter')\n",
    "        if body_section:\n",
    "            body_core_container = body_section.find('div', class_='core-container')\n",
    "            if body_core_container:\n",
    "                ## Extract the initial paragraphs that precede the first section (initial paragraphs)\n",
    "                #text += f\"\\nSection: Initial Paragraphs\\n\\n\" # Insert the 'Initial Paragraphs' title\n",
    "                #for paragraph in body_core_container.find_all('div', role='paragraph', recursive=False):\n",
    "                #    paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                #    text += f\"{paragraph_text}\\n\"\n",
    "                # Extract sections\n",
    "                for section_h2 in body_core_container.find_all('section', recursive=False):\n",
    "                    # Extract section title\n",
    "                    section_h2_title = section_h2.find('h2')\n",
    "                    if section_h2_title:\n",
    "                        section_h2_title_text = ' '.join(section_h2_title.get_text(' ', strip=True).split())\n",
    "                        text += f\"\\nSection: {section_h2_title_text}\\n\\n\"\n",
    "                    # Extract section paragraphs\n",
    "                    for paragraph in section_h2.find_all('div', role='paragraph', recursive=False):\n",
    "                        paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                        text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "                    # Extract subsections\n",
    "                    for section_h3 in section_h2.find_all('section', recursive=False):\n",
    "                        ## Extract subsection title\n",
    "                        #section_h3_title = section_h3.find('h3')\n",
    "                        #if section_h3_title:\n",
    "                        #    section_h3_title_text = ' '.join(section_h3_title.get_text(' ', strip=True).split())\n",
    "                        #    text += f\"\\nSubsection: {section_h3_title_text}\\n\\n\"\n",
    "                        # Extract subsection paragraphs\n",
    "                        for paragraph in section_h3.find_all('div', role='paragraph', recursive=False):\n",
    "                            paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                            text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "                        # Extract subsubsections\n",
    "                        for section_h4 in section_h3.find_all('section', recursive=False):\n",
    "                            ## Extract subsubsection title\n",
    "                            #section_h4_title = section_h4.find('h4')\n",
    "                            #if section_h4_title:\n",
    "                            #    section_h4_title_text = ' '.join(section_h4_title.get_text(' ', strip=True).split())\n",
    "                            #    text += f\"\\nSubsubsection: {section_h4_title_text}\\n\\n\"\n",
    "                            # Extract subsubsection paragraphs\n",
    "                            for paragraph in section_h4.find_all('div', role='paragraph', recursive=False):\n",
    "                                paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                                text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "                            # Extract subsubsubsections\n",
    "                            for section_h5 in section_h4.find_all('section', recursive=False):\n",
    "                                ## Extract subsubsubsection title\n",
    "                                #section_h5_title = section_h5.find('h5')\n",
    "                                #if section_h5_title:\n",
    "                                #    section_h5_title_text = ' '.join(section_h5_title.get_text(' ', strip=True).split())\n",
    "                                #    text += f\"\\nSubsubsubsection: {section_h5_title_text}\\n\\n\"\n",
    "                                # Extract subsubsubsection paragraphs\n",
    "                                for paragraph in section_h5.find_all('div', role='paragraph', recursive=False):\n",
    "                                    paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                                    text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "        # Web Scraping - End\n",
    "\n",
    "        # Save text to a text file\n",
    "        with open(txt_file, 'w', encoding='utf-8', newline='\\n') as file:\n",
    "            file.write(text)\n",
    "\n",
    "        logging.info(f\"Saved text for {article_id} to {txt_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7c306306-179d-4261-b9cb-478301e8ccb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text(df_journal_english_linguistics_open_access, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7338ed4-3a76-414b-98cf-6f62632115ba",
   "metadata": {},
   "source": [
    "### [Corpora](https://www.euppublishing.com/journal/cor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b85f718-069b-4fed-8a0e-ac82043c6cb1",
   "metadata": {},
   "source": [
    "#### Create output subdirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b622371f-1c24-4ad4-8a3d-10b5705afb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: cl_st2_ph31_eyamrog\\corp\n"
     ]
    }
   ],
   "source": [
    "# 'Corpora'\n",
    "id = 'corp'\n",
    "path = os.path.join(output_directory, id)\n",
    "create_directory(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951a2955-def7-409d-98c0-612762530b45",
   "metadata": {},
   "source": [
    "#### Import the data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48c352d1-3345-4e86-9a0c-79342e7ec4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpora_open_access = pd.read_json(f\"{input_directory}/corpora_open_access.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75b3a578-efcd-44de-9f2a-b539aebb5869",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpora_open_access['Published'] = pd.to_datetime(df_corpora_open_access['Published'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2895710-674f-4d07-b43a-ea31af9ff7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpora_open_access = df_corpora_open_access.loc[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5d4e99a-becc-4b18-b2bb-eef1161b368f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.euppublishing.com/doi/full/10.3366/cor.2020.0184',\n",
       " 'https://www.euppublishing.com/doi/full/10.3366/cor.2021.0226',\n",
       " 'https://www.euppublishing.com/doi/full/10.3366/cor.2022.0244',\n",
       " 'https://www.euppublishing.com/doi/full/10.3366/cor.2022.0245',\n",
       " 'https://www.euppublishing.com/doi/full/10.3366/cor.2022.0246']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp_urls = df_corpora_open_access['URL'].tolist()\n",
    "corp_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3a4571c-7744-4b22-902e-090e7be5376a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(df, path):\n",
    "    \"\"\"Extracts text from HTML files and saves as text files.\"\"\"\n",
    "\n",
    "    for article_id in df['ID']:\n",
    "        html_file = os.path.join(path, f\"{article_id}.html\")\n",
    "        txt_file = os.path.join(path, f\"{article_id}.txt\")\n",
    "\n",
    "        # Check if the HTML file exists\n",
    "        if not os.path.exists(html_file):\n",
    "            logging.error(f\"Skipping {html_file}: File not found\")\n",
    "            continue\n",
    "\n",
    "        # Read HTML content\n",
    "        with open(html_file, 'r', encoding='utf-8') as file:\n",
    "            soup = BeautifulSoup(file, 'lxml')\n",
    "\n",
    "        # Initialise text variable\n",
    "        text = ''\n",
    "\n",
    "        # Web Scraping - Begin\n",
    "\n",
    "        # Capture the 'article body'\n",
    "        article_body = soup.find('article')\n",
    "\n",
    "        # Extract the 'Title'\n",
    "        if article_body:\n",
    "            title = article_body.find('h1', property='name')\n",
    "            if title:\n",
    "                title_text = ' '.join(title.get_text(' ', strip=True).split())\n",
    "                text += f\"Title: {title_text}\\n\\n\"\n",
    "\n",
    "        # Extract the 'Abstract'\n",
    "        if article_body:\n",
    "            abstract_section = article_body.find('section', id='abstract')\n",
    "            if abstract_section:\n",
    "                h2_title = abstract_section.find('h2')\n",
    "                if h2_title:\n",
    "                    h2_title_text = ' '.join(h2_title.get_text(' ', strip=True).split())\n",
    "                    text += f\"\\nAbstract: {h2_title_text}\\n\\n\"\n",
    "                for paragraph in abstract_section.find_all('div', role='paragraph', recursive=False):\n",
    "                    paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                    text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "        # Extract the 'body'\n",
    "        if article_body:\n",
    "            body_section = article_body.find('section', id='bodymatter').find('div', class_='core-container')\n",
    "            if body_section:\n",
    "                for h2_section in body_section.find_all('section', recursive=False):\n",
    "                    h2_title = h2_section.find('h2')\n",
    "                    if h2_title:\n",
    "                        h2_title_text = ' '.join(h2_title.get_text(' ', strip=True).split())\n",
    "                        text += f\"\\nSection: {h2_title_text}\\n\\n\"\n",
    "                    for paragraph in h2_section.find_all('div', role='paragraph', recursive=False):\n",
    "                        paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                        text += f\"{paragraph_text}\\n\"\n",
    "                    for h3_section in h2_section.find_all('section', recursive=False):\n",
    "                        #h3_title = h3_section.find('h3')\n",
    "                        #if h3_title:\n",
    "                        #    h3_title_text = ' '.join(h3_title.get_text(' ', strip=True).split())\n",
    "                        #    text += f\"\\nSection: {h3_title_text}\\n\\n\"\n",
    "                        for paragraph in h3_section.find_all('div', role='paragraph', recursive=False):\n",
    "                            paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                            text += f\"{paragraph_text}\\n\"\n",
    "                        for h4_section in h3_section.find_all('section', recursive=False):\n",
    "                            #h4_title = h4_section.find('h4')\n",
    "                            #if h4_title:\n",
    "                            #    h4_title_text = ' '.join(h4_title.get_text(' ', strip=True).split())\n",
    "                            #    text += f\"\\nSection: {h4_title_text}\\n\\n\"\n",
    "                            for paragraph in h4_section.find_all('div', role='paragraph', recursive=False):\n",
    "                                paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                                text += f\"{paragraph_text}\\n\"\n",
    "                            for h5_section in h4_section.find_all('section', recursive=False):\n",
    "                                #h5_title = h5_section.find('h5')\n",
    "                                #if h5_title:\n",
    "                                #    h5_title_text = ' '.join(h5_title.get_text(' ', strip=True).split())\n",
    "                                #    text += f\"\\nSection: {h5_title_text}\\n\\n\"\n",
    "                                for paragraph in h5_section.find_all('div', role='paragraph', recursive=False):\n",
    "                                    paragraph_text = ' '.join(paragraph.get_text(' ', strip=True).split())\n",
    "                                    text += f\"{paragraph_text}\\n\"\n",
    "\n",
    "        # Web Scraping - End\n",
    "\n",
    "        # Save text to a text file\n",
    "        with open(txt_file, 'w', encoding='utf-8', newline='\\n') as file:\n",
    "            file.write(text)\n",
    "\n",
    "        logging.info(f\"Saved text for {article_id} to {txt_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9d3d84e-17c8-46b0-bdee-6cbcf002f14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text(df_corpora_open_access, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e92d0ea-7464-4d68-8e36-d9acb4dcf7d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
