Methods to Address Confounding and Other Biases in Meta-Analyses: Review and Recommendations

Copyright © 2022 by Annual Reviews. This work is licensed under a Creative Commons Attribution 4.0 International License , which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. See credit lines of images or other third-party material in this article for license information

ABSTRACT
Meta-analyses contribute critically to cumulative science, but they can produce misleading conclusions if their constituent primary studies are biased, for example by unmeasured confounding in nonrandomized studies. We provide practical guidance on how meta-analysts can address confounding and other biases that affect studies’ internal validity, focusing primarily on sensitivity analyses that help quantify how biased the meta-analysis estimates might be. We review a number of sensitivity analysis methods to do so, especially recent developments that are straightforward to implement and interpret and that use somewhat less stringent statistical assumptions than do earlier methods. We give recommendations for how these newer methods could be applied in practice and illustrate using a previously published meta-analysis. Sensitivity analyses can provide informative quantitative summaries of evidence strength, and we suggest reporting them routinely in meta-analyses of potentially biased studies. This recommendation in no way diminishes the importance of defining study eligibility criteria that reduce bias and of characterizing studies’ risks of bias qualitatively.
1. INTRODUCTION
Meta-analyses contribute critically to cumulative science ( 4 , 13 ), but they can produce biased estimates and misleading conclusions if their constituent primary studies are themselves biased. Nonrandomized studies may be particularly prone to unmeasured confounding, misclassification, selection bias, and other biases. We provide practical guidance on how meta-analysts can address these biases that affect studies’ internal validity. We first briefly cover approaches for defining study eligibility criteria that reduce bias (Section 2) and for qualitatively assessing studies’ risks of bias (Section 3). We then address our primary focus, namely methods for quantitatively assessing how sensitive meta-analysis results may be to residual bias that cannot be eliminated by limiting study eligibility (Section 4). This last topic has received relatively less attention both in the methodological literature on meta-analysis and in empirical meta-analyses ( 10 ), in part because early methods were reasonably criticized for invoking strong statistical assumptions and requiring extensive statistical expertise to implement and interpret ( 18 ). However, as we discuss, several recently developed methods have made progress on these fronts—although they do still have limitations—and we therefore advocate for routinely using both qualitative and quantitative methods to assess risks of bias in individual studies and in the meta-analysis as a whole. We illustrate the use of selected quantitative sensitivity analyses in an applied example (Section 5). We focus primarily on meta-analyses whose research questions concern causation and in which the most critical bias is unmeasured confounding, although we comment on other biases throughout, especially in Section 4.3. We do not address publication bias and similar selection processes and so refer to biases affecting studies’ internal validity simply as “bias.”
2. DEFINING ELIGIBILITY CRITERIA THAT REDUCE BIAS
We recommend that attention to reducing bias in a meta-analysis begin as early as the protocol design stage, during which the eligibility criteria for studies’ inclusion in the meta-analysis and in primary versus secondary analyses can be crafted to reduce bias. These eligibility criteria, along with the rest of the meta-analysis protocol, should be preregistered formally, with any post hoc deviations disclosed in the final manuscript ( 18 , 37 ).
First, the meta-analyst must decide whether to include nonrandomized studies (NRS) at all and, if so, whether to include only certain types of NRS. If an initial scoping review identifies a number of relevant, well-conducted randomized studies (RS) on the topic of interest, limiting eligibility to RS may provide the least biased results and still permit reasonable statistical precision. However, there may be very few (or no) relevant RS, for example because it is not feasible or ethical to randomize the exposure. Alternatively, in some contexts, RS may be available but may be subject to limitations that NRS help mitigate. For example, if the available RS use less externally generalizable samples or shorter follow-up periods than do NRS, then including NRS in the meta-analysis may better address the research question ( 18 ). When including NRS in a meta-analysis, we recommend that eligibility nevertheless be restricted to study designs that provide reasonably credible evidence, given the specific biases that are relevant to a given scientific topic ( 18 ); however, when few well-designed NRS are available, deciding how stringent to be can create challenging trade-offs between bias and precision.
Regarding confounding specifically, NRS are generally least susceptible to bias when they use longitudinal designs with the exposure measured before the outcome and when they control for confounders measured at baseline, ideally including baseline measures of the exposure and outcome themselves. 1 On the other hand, cross-sectional studies that measure the exposure, outcome, and any adjusted covariates contemporaneously are usually quite prone to confounding because the temporal ordering of the variables is unclear. That is, the direction(s) of causation between the exposure and outcome often cannot be established and the adjusted covariates may not permit adequate control of confounding ( 54 , 59 ). For this reason, we typically recommend that cross-sectional studies be excluded altogether in meta-analyses whose research questions concern causation, except if the exposure clearly precedes the outcome despite their contemporaneous measurement (e.g., if the exposure is fixed at birth or the outcome is mortality). 2 The sidebar titled A Hierarchy of Nonrandomized Designs for Controlling Confounding provides a more detailed, but approximate, ranking of NRS design features ordered by the level of robustness to confounding that they typically provide.
A HIERARCHY OF NONRANDOMIZED DESIGNS FOR CONTROLLING CONFOUNDING
Nonrandomized designs in ascending order of robustness to confounding:

Sidebar adapted with permission from Reference 54 .
If the meta-analyst is concerned about biases other than confounding, risk-of-bias tools for NRS can provide guidance on which design features could be used as inclusion criteria ( 48 ). When reviewing articles for inclusion, these design features should preferably be assessed not on the basis of study authors’ own labels for study designs (e.g., “longitudinal study”), which are defined inconsistently, but rather on the basis of studies’ actual design features, such as those in the sidebar titled A Hierarchy of Nonrandomized Designs for Controlling Confounding ( 18 ). Methods to conduct literature searches for NRS are discussed elsewhere ( 18 ).
In meta-analyses that include both RS and NRS or that include NRS whose designs provide substantially different levels of robustness to confounding, we recommend prespecifying in the meta-analysis protocol which designs will be included in primary and in secondary analyses ( 18 ). In general, we recommend analyzing RS and NRS separately, at least in secondary analyses ( 18 ). Regarding confounding, if the meta-analyst anticipates that the literature contains very few (if any) RS, primary analyses could be conducted using any available RS plus longitudinal NRS that measure the exposure before the outcome and that control for baseline confounders and the baseline outcome; secondary analyses could then stratify by randomization status using subset analyses or meta-regression methods ( 16 , 18 , 35 , 39 , 49 ). Similar secondary analyses could be conducted using risk-of-bias ratings, described in Section 3. Additionally, NRS often report estimates that adjust for different sets of confounders, and, ideally, the meta-analysis protocol would also prespecify which of these estimates will be extracted. Overall, we recommend that the estimate that adjusts for the largest number of pre-exposure confounders be extracted for primary analyses, but when unadjusted estimates are also available, these could also be extracted for secondary analyses.
3. QUALITATIVE METHODS FOR ASSESSING RISKS OF BIAS
We recommend that meta-analyses of NRS conduct detailed risk-of-bias assessments on each study ( 18 ). The ROBINS-I (Risk Of Bias In Non-randomized Studies – of Intervention) tool provides particularly well-informed guidance on the design features that most contribute to risks of confounding and other biases ( 48 ); guidance on its use and reporting are provided elsewhere ( 18 ). We suggest that meta-analyses of NRS report on risks of bias in at least three ways: ( a ) for the meta-analysis as a whole, the number and percent of studies occupying each level of the ranking given in the sidebar titled A Hierarchy of Nonrandomized Designs for Controlling Confounding; ( b ) for each study, its summary and domain-specific risk-of-bias ratings assessed using ROBINS-I; and ( c ) for each study, the list of pre-exposure confounders that were adjusted in the estimate that was extracted for primary meta-analyses.
These methods for detailing each study's risks of bias and design features are integral for meta-analyses of NRS, but it can be challenging to intuit how these individual characteristics contribute to the aggregate bias in the meta-analysis results. A common method to do so, and the one required in Cochrane Collaboration reviews, is the GRADE (Grading of Recommendations, Assessment, Development, and Evaluation) approach ( 18 , 43 ). In this approach, the meta-analyst first heuristically gauges the “proportion of information” ( 18 ) in the meta-analysis that is contributed by studies at low versus high risks of various types of bias ( 43 ). Using this heuristic assessment, the meta-analyst can choose to downgrade the overall certainty rating of the meta-analysis results from the default “high certainty” to “moderate,” “low,” or “very low.” At the meta-analyst's discretion, the certainty rating could be upgraded again if the pooled estimate is large (GRADE suggests the criterion of risk ratio >2 or <0.5), if there is evidence of a dose–response relationship, or if the biases are thought to have attenuated rather than inflated estimates ( 43 ).
GRADE and other qualitative approaches to assessing aggregate risks of bias are useful but have limitations. Intuiting how much information each study contributes to the meta-analysis is difficult when studies’ standard errors and estimates differ, and considerable subjectivity is involved in deciding how to downgrade or upgrade the overall certainty rating, as the GRADE Working Group discusses ( 43 ). Additionally, the GRADE approach ultimately provides a four-tiered qualitative rating of the overall certainty of the results, rather than a quantitative summary of how numerical estimates might have been affected by bias. For these reasons, we encourage supplementing these qualitative methods with quantitative methods for assessing the sensitivity of meta-analysis results to bias (Section 4).
4. QUANTITATIVE METHODS FOR ASSESSING SENSITIVITY TO UNMEASURED CONFOUNDING AND OTHER BIASES
Sensitivity analyses are quantitative methods that characterize how numerical estimates might be affected by bias. We classify sensitivity analyses into two-stage and one-stage methods. Two-stage methods first adjust each study's point estimate and potentially also its variance, and then they meta-analyze these bias-corrected estimates. In contrast, one-stage methods correct the meta-analysis holistically by specifying the distribution of bias across studies, rather than in each study individually. Below, we primarily discuss conceptually distinct methods that could be used in the context of unmeasured confounding (although many of these methods also accommodate other biases; Section 4.3) and are reasonably straightforward to implement in practice without extensive customization. Supplemental Tables 1 a –1 b provide additional details on the methods.
Two-stage methods begin by adjusting each individual study using any of four broad approaches. First, some methods use subjective elicitation, in which expert reviewers subjectively specify a numerical value for the severity of bias in each study as well as their own uncertainty in making each judgment ( 50 ). Each study's estimate is then corrected using the specified bias, and its variance estimate is inflated to accommodate subjective uncertainty ( 50 ).
Second, external adjustment methods adjust each study using information from an external study, which itself may or may not be included in the meta-analysis. For example, Greenland & O'Rourke ( 12 ) proposed adjusting each of the meta-analyzed NRS using information from a comparably designed external study that reports both an estimate that is thought to be fully adjusted for confounding (and so is unbiased) and a partially adjusted estimate that is subject to the same amount of confounding bias as the meta-analyzed estimate to be adjusted.
Third, methods based on multiple imputation are related to external adjustment but apply in the special context in which the meta-analyst has access to individual participant data for all studies. Under the assumptions that at least some studies are fully adjusted and that, in the remaining partially adjusted studies, confounder data are missing at random, individual participants’ confounder values are imputed on the basis of relationships among the observed variables, using multilevel models to account for heterogeneity across studies in the joint distribution of the observed variables ( 2 , 19 , 40 ). Each partially adjusted study can then be adjusted on the basis of these imputed confounder values using any standard method for measured confounding, such as regression adjustment or propensity score methods.
Fourth, some methods use analytical bias formulas to adjust each study, given hypothetical sensitivity parameters regarding the severity and distribution of unmeasured confounder(s). For example, the method developed by Goto et al. ( 11 ) assumes that each study has a single, categorical unmeasured confounder; under this assumption, each study's estimate can be corrected using four sensitivity parameters that characterize the confounder's prevalences among the unexposed and exposed subjects as well as the confounder's strengths of association with the exposure and with the outcome ( 1 ). 3
After obtaining bias-corrected estimates for each study using one of the above methods, two-stage methods then proceed to meta-analyze the bias-corrected estimates. Some methods simply conduct a standard meta-analysis in this second stage, without directly modeling additional uncertainty introduced by unmeasured confounding, because they essentially treat any sensitivity parameters used to obtain the corrected estimates as hypothetical fixed values ( 11 ). Other methods inflate the variances of the corrected estimates to reflect statistical error associated with the external data ( 12 ) or subjective uncertainty associated with subjective elicitation ( 50 ). In multiple imputation methods, increases in uncertainty due to unmeasured confounding are naturally captured by the between-imputation variance, which contributes to the final, pooled variance estimate ( 41 ). A conceptually unique partial identification approach first bounds each study's causal effect using bounds on the possible values of the outcome variable and then bounds the pooled estimate by taking the intersection of all the studies’ bounds ( 27 ). This approach is unusual because it provides an interval rather than a point estimate, assumes there is no effect heterogeneity across studies, and may often yield no interval at all in meta-analyses of more than a few studies.
The key advantage of two-stage methods is that they allow case-by-case adjustment of each study based on extensive information regarding its magnitude of bias. When such information is available, is accurate, and fulfills any necessary statistical assumptions ( Supplemental Table 1 a ), these methods can allow for accurate and precise bias correction. With sufficiently detailed data from fully adjusted studies, some methods have the important advantage of directly and objectively correcting inference for uncertainty introduced by unmeasured confounding ( 2 , 12 , 19 , 40 ). In addition, there is a rich literature on methods to handle confounding and other biases in individual studies (reviewed in References 22 and 62 ), and in principle, two-stage methods could use any of these existing methods in the first stage.
However, the reliance of two-stage methods on extensive information about each study is also a disadvantage because this information may often be unattainable for any given study, let alone when meta-analyzing many existing studies. For example, external adjustment and multiple imputation methods require extensive data from fully adjusted studies, and if these “fully adjusted” studies in fact still have residual confounding, the methods may not adjust adequately. Two-stage methods using analytical formulas require fairly detailed information and assumptions about the unmeasured confounder(s)—for example, that there is a single, categorical unmeasured confounder with known prevalences ( 11 ). Of the two-stage methods, those using subjective elicitation require perhaps the fewest “inputs” but at the cost of relying critically on expert reviewers’ ability to numerically estimate the severity of confounding bias in each study ( 50 ).
Multiple imputation methods can be implemented using well-established R packages ( 19 , 51 ). To our knowledge, no software is available for the other methods, but all would be straightforward to implement in any command language for statistical analysis (e.g., R or SAS) by coding a few lines of analytical bias formulas.
One-stage methods occupy two broad categories: bias correction methods and E-value analog methods ( Supplemental Table 1 b ).
These methods specify a hypothetical distribution of bias across studies and then obtain a bias-corrected pooled estimate or distribution of effects. McCandless ( 36 ) proposed a Bayesian approach in which log- and logit-normal hyperpriors are specified on the distribution across studies of three sensitivity parameters: ( a ) the unmeasured confounder's strength of association with the outcome, conditional on the exposure and on any measured confounders; ( b ) the confounder's prevalence among the unexposed group; and ( c ) the confounder's prevalence among the exposed group. Assuming that, across studies, the sensitivity parameters are independent of one another and of studies’ causal population effects, McCandless ( 36 ) then obtained a bias-corrected likelihood and posterior for the meta-analysis by arithmetically correcting studies’ estimates using these three sensitivity parameters. It is critical to note that the bias formula they used to do so assumes that each study has a single, binary unmeasured confounder that does not interact with the exposure and that is independent of any measured confounders, conditional on the exposure ( 25 ). The latter assumption is highly problematic because it is in fact always violated when the measured and unmeasured confounders affect the exposure ( 17 , 53 ); thus, while we do not recommend applying this method as is, the general Bayesian approach could be adapted to use other bias formulas that obviate this assumption. Unlike two-stage methods that require the meta-analyst to specify sensitivity parameters for each study individually, the Bayesian framework requires the meta-analyst to specify only the means and variances of the sensitivity parameters’ hyperpriors across studies.
Another method considers confounding bias that is additive on the scale on which studies’ estimates are meta-analyzed (e.g., the log-risk ratio scale) and that is assumed to be distributed normally across studies, again independently of studies’ causal population effects ( 33 ). This method characterizes evidence strength in the meta-analysis in terms of the proportion ( ) of causal population effects that are meaningfully strong, defined as effects above a threshold ( q ) that the meta-analyst has chosen to represent a meaningfully strong causal effect in the scientific context [e.g., risk ratio (RR) = 1.1 or some other threshold]. 4 (For meta-analyses with pooled estimates in the apparently preventive direction, meaningfully strong causal effects could be defined as those below a threshold, such as RR = 0.90). In addition, the meta-analyst can estimate the proportion of effects below a second, possibly symmetric, threshold in the opposite direction from the pooled estimate. These proportion metrics were recently introduced in the general context of random-effects meta-analysis to better convey evidence strength across heterogeneous effects than would be conveyed by the pooled estimate alone ( 31 ). 5
To bias-correct the proportion of meaningfully strong causal effects, Mathur & VanderWeele ( 33 ) assumed that the additive bias is log-normal across studies. The meta-analyst would specify as sensitivity parameters the mean and variance across studies of these biases. Mathur & VanderWeele ( 33 ) discuss methods to choose these parameters; for example, the variance of the biases could be calculated by first specifying the proportion of the confounded heterogeneity estimate ( ) that is in fact due to heterogeneous bias. The metric can then be estimated using simple arithmetic expressions involving these sensitivity parameters along with estimates from the confounded meta-analysis ( 33 ). Comparable nonparametric methods can estimate without making the usual assumption in meta-analysis that the causal population effects are normal ( 32 ) or independent ( 35 ), and they provide inference that performs better in small meta-analyses or in those with extreme true proportions. These methods specify a single fixed value for the bias in all studies (“homogeneous bias”). In some cases, assuming homogeneous bias yields a conservative estimate: For example, if the bias-corrected mean estimate is greater than the threshold q , then estimating under the assumption of homogeneous bias will typically be an underestimate (representing greater sensitivity to unmeasured confounding) if in fact the bias is heterogeneous. In the Supplemental Material , we detail the conditions under which the nonparametric estimate is conservative, and we provide simple alternative expressions that are conservative under other conditions (e.g., when the bias-corrected mean estimate is less than q ).
As described above, bias correction methods specify the severity of bias across studies to obtain a corrected pooled estimate. Conversely, E-value analog methods characterize the severity of bias that would be required, hypothetically, to shift the pooled estimate to the null or to otherwise explain away the results of the meta-analysis. These methods are thus similar to the E-value, a recently introduced sensitivity analysis for unmeasured confounding in individual studies that does not require assumptions on the nature of unmeasured confounder(s) ( 7 , 55 ). This standard E-value represents the minimum strength of association, on the RR scale, that unmeasured confounder(s) would need to have with both the exposure and the outcome, conditional on any measured covariates, to fully explain away the observed exposure–outcome association in an individual study ( 7 , 55 ). When the confounded estimate in an individual study, , is apparently causative ( ), the E-value is

As a simple E-value analog for a meta-analysis, Equation 1 could be applied directly to the pooled estimate transformed to the RR scale ( 33 ). This E-value analog represents the average strengths of association across studies, on the RR scale, that unmeasured confounder(s) would need to have with studies’ exposures and outcomes in order to shift the pooled estimate to the null. Additionally, one can consider the severity of confounding that would be required to shift the confidence interval for the pooled estimate to include the null; to do so, in the above expression would simply be replaced with the confidence interval limit closer to the null ( 33 , 55 ). These metrics do not make assumptions on the distribution of the bias in the confounded population effects, although, again, the bias must be independent of studies’ standard errors ( Supplemental Material ).
Like most sensitivity analyses for meta-analyses, this simple E-value analog is limited to characterizing evidence strength only in terms of the pooled estimate and its confidence interval. Other E-value analogs instead characterize evidence strength in terms of the aforementioned proportion of meaningfully strong causal effects, ( 33 ). For example, Mathur & VanderWeele ( 33 ) proposed a metric, , that represents the minimum average strengths of association on the RR scale that unmeasured confounder(s) would need to have with both the exposure and the outcome in order to reduce to less than some value r (e.g., 0.15) the proportion of studies with causal population effects stronger than q . The rationale for this approach is that, when effects are heterogeneous, one might define explaining away the results of the meta-analysis in terms of substantially reducing the proportion of meaningfully strong effects in this way. Letting and denote the pooled estimate and heterogeneity estimate from the confounded meta-analysis, denote the across-study variance of the log-normal bias, and Φ denote the normal cumulative distribution function, the metric can be estimated as follows for a confounded pooled estimate that is apparently causative ( on the log-RR scale): 6

As for , comparable nonparametric methods ( 32 , 35 ) can allow estimation of in a wider range of settings than is possible using the above parametric methods. The nonparametric methods assume homogeneous bias across studies; however, if the bias is in fact heterogeneous, the nonparametric estimates can sometimes be interpreted as conservative estimates ( Supplemental Material ).
In contrast with two-stage methods that require extensive information and assumptions about the severity of bias in each study, one-stage bias correction methods require specification of only a small number of sensitivity parameters that characterize the severity of bias across all studies. E-value analog methods require yet fewer, if any, sensitivity parameters to be specified because, conversely to bias correction methods, E-value methods solve for the severity of bias that would have to exist in order to explain away the results of the meta-analysis. This approach has the advantage of reducing “researcher degrees of freedom” associated with choosing sensitivity parameters post hoc, which may be especially problematic with two-stage methods ( 44 ) and with qualitative evidence-grading systems ( 43 ). Whereas all two-stage methods require access to at least study-level estimates and variances ( Supplemental Table 1 a ), often precluding analysis by third parties, certain one-stage methods can be conducted using only statistical estimates from the meta-analysis itself, allowing for sensitivity analysis of some published meta-analyses for which study-level data are unavailable ( 33 , 55 ).
However, by eliminating case-by-case specification of bias parameters, one-stage methods typically introduce assumptions about the distribution of bias across studies that are unnecessary for most two-stage methods. Most one-stage methods assume either that sensitivity parameters are homogeneous across studies or that they are log- or logit-normal. Diagnostic plots and tests can sometimes be used to rule out severe violations of these assumptions; for example, the assumptions of Mathur & VanderWeele ( 33 ) imply that the population confounded effects are normal, so standard normality tests for meta-analyses could be used ( 15 , 60 ). Nonparametric methods ( 32 ) can sometimes be calculated and interpreted under weakened assumptions on the bias distribution ( Supplemental Material ). Also, most one-stage methods make the important assumption that the bias in each study is independent of its population causal effect, 7 which could be violated if, for example, study authors who investigate small causal effects tend to adjust for only a few confounders in order to obtain statistically significant results. We give some practical guidance for navigating statistical assumptions in Section 4.4.
McCandless's ( 36 ) method could be implemented by modifying their example R code; as noted in Section 4.2, we consider it critical to use a different bias formula when applying the general Bayesian framework. All other one-stage methods discussed above ( 32 , 33 ) can be implemented using the website http://www.evalue-calculator.com/meta/ or the R package EValue ( 28 ), for which vignettes are available ( 29 ). We provide a step-by-step tutorial for using this website and R package in the Supplemental Material .
Although we have focused on methods that can provide sensitivity analyses at least for unmeasured confounding, some of these methods also naturally accommodate other biases in NRS or RS, such as participant selection, measurement error, missing data, or noncompliance ( Supplemental Tables 1 a –1 b ). For example, in principle, meta-analysts could subjectively elicit any type of bias ( 50 ). Bayesian methods have been proposed for other biases ( 47 , 61 ). Methods to handle psychometric artifacts arising from, for example, range restriction of the outcome or imperfect construct validity are detailed elsewhere ( 42 ).
One-stage sensitivity analyses for unmeasured confounding ( 32 , 33 ) could be readily adapted for certain other biases for which expressions equivalent to the E-value are now available for individual studies [selection bias ( 46 ), differential measurement error ( 57 ), and combinations of these biases with unmeasured confounding ( 45 )]. These E-value equivalents represent the severity of bias, in terms of sensitivity parameters that are specific to the bias under consideration, that would be required to shift the effect of an individual study to the null. To apply these results for a meta-analysis, the meta-analyst could first estimate , which represents multiplicative bias on the RR scale regardless of origin, exactly as described above ( 32 , 33 ), and then could calculate a bias-specific analog to by transforming using the relevant bias-specific E-value expression ( 45 , 46 , 57 ).
We recommend that meta-analyses of NRS routinely report one or more sensitivity analyses for unmeasured confounding and potentially for other biases as relevant to the scientific context and study designs (see the sidebar titled Sensitivity Analysis Recommendations). This recommendation in no way detracts from the importance of also implementing the recommendations in Sections 2 and 3. As noted above, all sensitivity analysis methods make statistical assumptions of varying stringency, and many sensitivity analyses require extensive information characterizing the amount of bias in each study. In addition, many methods are not yet implemented in software.
SENSITIVITY ANALYSIS RECOMMENDATIONS


Given these considerations, one possible practical approach for choosing among the methods is as follows. First, the meta-analyst could calculate and report the following simple E-value analogs: ( a ) the E-value for the pooled estimate and its confidence interval limit closer to the null, which respectively represent the average severity of confounding across studies that would be required to shift the pooled estimate, and to shift its confidence interval, to the null ( 33 , 55 ); and ( b ) a nonparametric estimate of , which represents the severity of homogeneous confounding that would need to be present in each study in order to reduce to less than r the proportion of causal population effects stronger than a chosen threshold, q ( 32 , 33 ). As discussed in Section 4.2 and the Supplemental Material , the metric is perhaps most informative when it is calculated and interpreted under conservative assumptions, rather than under the strict assumption that the bias truly is homogeneous.
We believe that, as a generic starting point, reporting these simple metrics is reasonable because these metrics apply to a fairly broad range of meta-analyses of NRS: ( a ) They do not make assumptions about the nature of unmeasured confounder(s) themselves within studies (e.g., the metrics accommodate multiple confounders, nonbinary confounders, and confounders that interact with the exposure); ( b ) they require no specification of sensitivity parameters; and ( c ) they are straightforward to implement using standard study-level data and available software ( 28 , 29 ). Additionally, these metrics characterize the sensitivity to unmeasured confounding of both the pooled estimate (and its confidence interval) and the proportion of meaningfully strong causal effects; they thus provide a straightforward way to summarize the distribution of causal effects in the meta-analysis in terms of both its mean and its variability. [If the heterogeneity estimate is 0, then the metric would be omitted.] As such, the metrics provide complementary information: Depending on the distribution of population effects, the point estimate may be more or less sensitive to confounding than the percentage of meaningfully strong effects would be.
Given the results of these simple sensitivity analyses, we recommend that the meta-analyst then attempt to assess and report whether it is actually plausible that the meta-analyzed studies are subject to confounding as severe as that indicated by the E-values and by . Examples can be found in existing meta-analyses ( 3 , 9 , 26 ). This assessment would be based on substantive knowledge of the exposures and outcomes under consideration and the risk-of-bias assessments described in Section 2. For example, more unmeasured confounding would be plausible in a meta-analysis of cross-sectional studies than in an otherwise comparable meta-analysis of longitudinal studies that control for an ample set of baseline confounders, including baseline values of the exposure and outcome ( 54 , 59 ). In addition, examining the confounding associations of measured confounders with the exposure and outcome, for example from studies that report both adjusted and unadjusted estimates, can also help inform assessments of the plausible severity of unmeasured confounding. However, even if measured confounders have strong confounding associations, residual unmeasured confounding above and beyond these strong measured confounders may be considerably less severe. Also, because the E-value considers maximum bias, even if unmeasured confounders do in fact have confounding associations similar in magnitude to the E-value, this does not necessarily mean that these confounders could actually explain away the effect; rather, it means only that the evidence is less clear. Last, some empirical studies have more broadly assessed the extent of agreement or disagreement between NRS and RS on the same topic (e.g., those included in the same meta-analysis); we summarize several such results in the Supplemental Material . However, it is critical to note that disagreements between NRS and RS cannot be interpreted as direct estimates of confounding bias, but rather as estimates of the aggregation of confounding bias plus any other systematic differences between study designs. Furthermore, the severity of confounding differs across meta-analyses and scientific topics.
In general, we advise also conducting sensitivity analyses that consider more precise forms of heterogeneous bias across studies. One possible approach is to apply one-stage methods that assume log-normal bias across studies and do not make assumptions about the nature of confounder(s) within each study; as discussed above, these methods also characterize the heterogeneous distribution of population effects ( 33 ). If the meta-analyst is concerned about a specific, single unmeasured confounder with known prevalences, one-stage Bayesian methods that similarly assume log-normal and logit-normal sensitivity parameters across studies could be adapted ( 36 ), again replacing the existing bias formula with one that obviates the problematic conditional independence assumption. If the meta-analyst has access to the specific forms of external data or individual participant data required by two-stage methods ( Supplemental Table 1 a ), then these methods could be applied to obviate distributional assumptions on the bias and to provide potentially more accurate bias-corrected estimates, albeit by introducing different statistical assumptions.
5. APPLIED EXAMPLE
We now illustrate the use and interpretation of selected one-stage sensitivity analyses by applying them to a published meta-analysis. Kodama et al. ( 21 ) meta-analyzed longitudinal studies that assessed the association of lower versus higher maximal aerobic capacity with all-cause mortality ( Supplemental Figure 1 ). Prior to correction for unmeasured confounding, our replication of their meta-analysis using 16 studies yielded a pooled RR of 1.73 [95% confidence interval (1.50, 1.99); ; heterogeneity ]. All studies were longitudinal and adjusted for some, but not all, possible confounders. The aerobic capacity measure was arithmetically adjusted for average age and sex differences, but many studies did not adjust for other possible confounders, such as smoking, body mass index, physical activity, and underlying diseases. Kodama et al. ( 21 ) did not report on whether each study measured confounders at baseline and did not rate studies’ risks of bias.
We first conducted the simple sensitivity analyses described in Section 4.4. (The Supplemental Material provides a step-by-step tutorial on how to conduct all analyses described below.) Computing the E-value for the pooled estimate indicated that unmeasured confounder(s) associated with both lower aerobic capacity and higher all-cause mortality by average RRs of 2.85-fold each could potentially shift the pooled estimate to the null; average confounding associations of 2.36-fold each could potentially shift the confidence interval to the null. To characterize the heterogeneous distribution of causal effects, we considered effects larger than RR = 1.1 to represent meaningfully strong detrimental effects of lower aerobic capacity. We therefore chose because we conducted analyses on the log-RR scale. Prior to correction for unmeasured confounding, we estimated that the percentage of studies with meaningfully strong population effects ( ) was nearly 100%.
We then estimated that to reduce this percentage to 15%, homogeneous unmeasured confounding RRs with both lower aerobic capacity and higher all-cause mortality of [95% confidence interval (2.35, 4.28)] each could suffice, but weaker homogeneous confounding could not ( Supplemental Figure 2 a ) ( 32 ). Although control of confounding in the meta-analyzed studies was quite limited, these sensitivity analyses seem to suggest reasonably robust evidence for effects of aerobic capacity on mortality: It seems somewhat implausible that, above and beyond measured confounding, each study had sufficiently severe unmeasured confounding (e.g., associations of RR = 2.36 to 3.07 with both aerobic capacity and mortality) to shift the pooled estimate or its confidence interval to null or to reduce the percentage of meaningfully strong effects to only 15%.
To supplement these simple sensitivity analyses, we also assessed the sensitivity of these results to unmeasured confounding under the assumption that bias was highly heterogeneous across studies such that it accounted for 80% of the estimated total between-study variance ( Supplemental Figure 2 b ) ( 33 ). We estimated that, to reduce the percentage of meaningfully strong causal effects to 15%, unmeasured confounding RRs with both higher aerobic capacity and lower all-cause mortality of on average [95% confidence interval (1.67, 4.00)] across studies would suffice to explain away the meta-analysis results in this sense, but weaker confounding would not ( 33 ). Again, this severity of unmeasured confounding seems somewhat implausible in these longitudinal studies, and thus the conclusion that there is a considerable percentage of studies with meaningfully large effects seems fairly robust to even substantial degrees of heterogeneous unmeasured confounding. This final analysis assumes that the bias was log-normal across studies; diagnostic plots did not suggest any severe violation of this assumption.
6. CONCLUSION AND RECOMMENDATIONS
We have recommended routinely applying quantitative sensitivity analyses on the grounds that they provide informative, relatively objective quantitative summaries of evidence strength that complement more widespread qualitative approaches (Section 3). Our practical recommendations have been as follows:

Our recommendations to routinely apply quantitative sensitivity analyses may prove controversial. Others have reasonably argued that sensitivity analysis methods require unrealistic statistical assumptions and are difficult for readers with limited statistical training to implement and interpret ( 18 ). However, as we have discussed, more recently developed sensitivity analyses relax some—though certainly not all—important assumptions and are straightforward to implement and interpret. We therefore believe that these methods make progress toward resolving these concerns and that the methods, when reported responsibly, can contribute substantially to characterizing the credibility of a meta-analysis.
To further advance this field, several future directions seem particularly impactful. First, it would be valuable to continue extending quantitative methods, for example to more flexibly model the propagation of uncertainty in sensitivity parameters to meta-analysis results, to characterize evidence strength using metrics that summarize heterogeneous effect distributions rather than only the pooled estimate, and to further accommodate heterogeneous bias, especially bias that is correlated with the causal population effects. Second, because interpreting sensitivity analyses requires assessing the severity of bias that is plausible in the meta-analyzed studies, it would be valuable to continue establishing empirical benchmarks for the actual severity of bias in studies on different topics and of different designs. We have reviewed some such work ( 8 ) in the Supplemental Material , but we particularly encourage further developments that more rigorously parse genuine bias from other systematic differences between study designs ( 5 , 6 ).
Third, making data publicly available for both original research (with appropriate deidentification) and meta-analyses would resolve a critical and largely unnecessary limiting factor on meta-analysts’ ability to handle bias. If individual participant data were routinely available, much more sophisticated quantitative methods with fewer assumptions could be developed. Meta-analysts themselves often do not make even study-level data available publicly or on request ( 34 , 38 ), which largely prevents third parties from conducting sensitivity analyses except sometimes by a single method ( 33 ). Simple policies and incentives by journals can sometimes rapidly improve data availability when ethical ( 14 , 20 ), with many collateral benefits for the credibility and efficiency of both original research and meta-analyses.
We hope that the methods and recommendations discussed in this review, along with the suggested future directions, will help inform a balanced and nuanced view of the credibility of meta-analyses.
disclosure statement
T.J.V. reports receiving personal fees from Flerish and from Flourishing Metrics. M.B.M. is not aware of any affiliations, memberships, funding, or financial holdings that might be perceived as affecting her objectivity.
reproducibility
All data, materials, and code required to reproduce the applied example (Section 5) and the reanalysis of empirical benchmarking data ( Supplemental Material ) are publicly available and documented ( https://osf.io/yd3ws/ ).
acknowledgments
We thank Sebastian Schneeweiss and the other RCT-DUPLICATE authors (Reference 9 ) for providing study-level data for reanalysis (see the Supplemental Material ). This research was supported by National Institutes of Health (NIH) grants R01 LM013866R01 and R01 CA222147; the NIH-funded Biostatistics, Epidemiology and Research Design (BERD) Shared Resource of Stanford University's Clinical and Translational Education and Research (UL1TR003142); the Biostatistics Shared Resource (BSR) of the NIH-funded Stanford Cancer Institute (P30CA124435); and the Quantitative Sciences Unit through the Stanford Diabetes Research Center (P30DK116074).
literature cited
