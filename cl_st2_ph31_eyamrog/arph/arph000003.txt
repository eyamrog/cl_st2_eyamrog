Title: Risks and Opportunities to Ensure Equity in the Application of Big Data Research in Public Health


Section: 

Copyright © 2022 by Annual Reviews. This work is licensed under a Creative Commons Attribution 4.0 International License , which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. See credit lines of images or other third-party material in this article for license information


Section: ABSTRACT

The big data revolution presents an exciting frontier to expand public health research, broadening the scope of research and increasing the precision of answers. Despite these advances, scientists must be vigilant against also advancing potential harms toward marginalized communities. In this review, we provide examples in which big data applications have (unintentionally) perpetuated discriminatory practices, while also highlighting opportunities for big data applications to advance equity in public health. Here, big data is framed in the context of the five Vs (volume, velocity, veracity, variety, and value), and we propose a sixth V, virtuosity, which incorporates equity and justice frameworks. Analytic approaches to improving equity are presented using social computational big data, fairness in machine learning algorithms, medical claims data, and data augmentation as illustrations. Throughout, we emphasize the biasing influence of data absenteeism and positionality and conclude with recommendations for incorporating an equity lens into big data research.

Section: INTRODUCTION

In the context of health, big data is considered “health data from multiple sources at scale” ( 84 ) and is considered “big” when, in fact, it is complex. Big data encompasses data from biospecimens; health records; medical and other imaging; individual, community, or satellite sensors; administrative records; policies, laws, and human rights records; environmental indicators; behavioral data; internet, social media, and mobile phone data; stock market trends; public opinion; and more. A commonality for big data research in public health is that these data are almost always evaluated using observational rather than experimental methodologies.
Since a 2018 review of big data and public health ( 98 ), the risks of misinterpreting big data findings for vulnerable populations that may exacerbate health inequity have become more apparent. 1 Equity-focused critiques suggest that incomplete conceptualizations of big data may solidify biases, which further marginalize vulnerable populations ( 37 ). For example, there are widespread concerns about racial bias in algorithms ( 109 ), a digital divide that includes only data for certain populations ( 84 ), and the absence of big data in health and social systems in several lower-income countries ( 36 ).
There are also incredible opportunities to leverage big data for health equity by bringing together structural determinant data previously not included in epidemiologic research, integrating new forms of data, and bridging policy analysis with health planning—all of which can help direct public health action to ensure more equitable outcomes ( 28 , 78 ) ( Table 1 ). Advocates highlight the importance of big data for the uptake of evidence-based interventions (the purview of implementation science) as well as for “delivery of the right intervention to the right population at the right time, and includ[ing] consideration of social and environmental determinants” ( 78 , p. 245). Research linking health data with new forms of structural determinants data, for example, with measures of racially directed violence or discriminatory housing policies (such as redlining) ( 102 ) can help expand the scope of public health ( 28 ). By de-siloing and amalgamating data from seemingly unconnected sources (i.e., mash-ups), advocates hope to create new actionable knowledge ( 34 ).
Examples of recent uses of big data for selected core public health activities and topics
Abbreviations: COVID-19, coronavirus disease 2019; MLM, multilevel model; MSE, multiple systems estimation; MSM, men who have sex with men; WHO, World Health Organization.

In this review, we ( a ) summarize concerns about big data and health equity, ( b ) present a series of analytic approaches to explore and address inequities using big data, and ( c ) discuss data augmentation methods to embed a health equity lens into big data research. We use the five Vs framing ( 6 ), encompassing volume, velocity, veracity, variety, and value ( Figure 1 ), and add a sixth V, virtuosity, to reconceptualize big data research to explicitly focus on equity (see the sidebar titled The Six Vs).

Figure 1 Click to view



Section: DATA DO NOT MAKE THEMSELVES: RISKS FROM BIG DATA CONCEPTUALIZATION

Many critiques of big data concern the veracity and variety of data and risks to health equity resulting from how data are created. Precision public health, for example, is criticized for relying on data sets that do not reflect underlying structural relationships, missing upstream socioeconomic determinants ( 77 ). For example, early in the coronavirus disease 2019 (COVID-19) pandemic, mobile phone–based mobility data were misinterpreted to suggest that noncompliance with restrictions was widespread in low-income communities (compared with higher-income communities) in the USA ( 20 , 138 ). A fuller exploration demonstrated that the privileges of working from home were not available to essential workers, who had to continue commuting for work ( 20 ). Without a deeper understanding of upstream drivers, there is the risk that big data–based policies will further exacerbate inequities.
THE SIX Vs


The potential to address health equity through big data research rests largely on the inclusiveness and accuracy of data for all types of individuals and communities (veracity and variety). Although numerous national population-based surveys in the USA and other countries have engaged methods to include more diverse samples, declining survey participation rates over time have contributed to challenges in using these national surveys for equity-focused research ( 8 ). Widespread concerns have emerged about how little attention to inclusion and representation exists in big data formation, such that persons who identify as racial/ethnic or gender minorities, are from resource-poor settings with limited cyberinfrastructures to capture data, and are from environments for which providing personal data can result in discrimination and underrepresentation often are not included ( 30 ). One of the most important challenges associated with gaps in data for vulnerable populations can be thought of as data absenteeism ( 84 ), whereby some groups are absent from data. Paramount to any scientific endeavor is the need to understand both how data were collected (or generated) and the gap between the study population (study participant data) and the target population. Barring a true census in which everyone is included, people will be missed; it is incumbent on the scientist to assess whether certain types of people or communities are systematically missed or misrepresented. Equity in who benefits from scientific knowledge and resource allocation fundamentally cannot be achieved if groups are not represented in the scientific studies that generate the knowledge that informs the distribution of resources.
Additionally, data are not reflective of the real world but instead represent protocols designed to collect or produce data, as well as the conscious or unconscious biases of investigators which may systematically miss or undercount certain communities ( 30 ). The US Census, for example, tasked with counting every resident of the USA every 10 years, has systematically undercounted racial/ethnic minorities (and minority language groups) because census workers did not feel comfortable visiting homes in low-income neighborhoods, where racial/ethnic minorities disproportionately live ( 4 ). As a result, these communities do not benefit from resources that are allocated according to population size.
There is a well-recognized digital divide in which many people do not have access to digital tools used to collect data (e.g., smartphones, internet portals). Data absenteeism is particularly relevant to global health because many data do not exist for populations lacking widespread internet use. Assumptions that individuals are the sole owners of cell phones or computer devices and that the data captured reflect these same unique individuals also generate biases in data, as demonstrated in a study of Ebola transmission risk in which multiple individuals shared a phone ( 36 ). Conceptualizing virtuosity in the formation of big data requires asking: Who is included and not included here, and why (variety)? Can we address possible data gaps with supplementation methods or data sets (variety)? How could discrimination, through lack of inclusion, bias interpretation and have a detrimental impact (value)? Are there theories related to marginalization that we can apply to research questions to enable a more thorough understanding of the data inputs and their meaning (veracity)? What assumptions are we making about historical, upstream, or socioecological factors; racialization; or discrimination risk in regard to the questions we are exploring (veracity)? How are we making assumptions about variables in our data and who they speak for, and how can we address them if biases may be present (veracity)?
Data absenteeism has been linked to inaccurate algorithms, such as when limited diversity among individuals included in machine learning–based risk models results in model preferences that unfairly characterize what is normal, deviates from normal, or is excluded altogether ( 9 , 24 , 30 , 109 , 115 ). Big data critiques also refer to uneven power structures embedded in the data's origins (referred to as the positionality of data and their creators), resulting in calls for the democratization of data and data augmentation ( 37 ). Theory can play a role in how one approaches big data and equity ( 39 ), not only as a socioecological theory to help understand different layers of information but also as theories presenting a perspective on the following questions: What shapes inequities, and for whom? Who is in the position of power in data structure, development, and analysis?
Machine learning programs are intended to improve health outcomes, reduce expenditures, and improve service delivery in clinical and public health programs. Machine learning involves developing algorithms and applying them to synthesized data ( 41 ). Recent studies show that discrimination can occur when algorithms are applied in the absence of complete data, in the presence of biased data, and/or when biased assumptions are used to characterize data inputs ( 35 , 41 , 109 , 114 , 129 ). In a highly publicized study of electronic health record algorithms that determined which patients were referred for extra care, Obermeyer et al. ( 109 ) demonstrated that the algorithm excluded Black patients who were equally as sick as White patients. The algorithm developers had inappropriately applied a variable capturing total health care expenditures as a proxy for unmet health needs in establishing risk scores. This example illustrates the inverse prevention law, in which those with greatest need are the least likely to receive resources ( 14 ), because the cost variable did not capture the unmet needs of Black patients (value). In the following sections, we present various approaches used to enmesh equity approaches into big data research to mitigate these challenges.

Section: ANALYTIC APPROACHES TO IMPROVE EQUITY FOR BIG DATA

Social computational big data includes data from technology companies such as Google, Facebook, and Twitter ( 135 ) that are collected and stored from application programming interfaces (APIs) and contain data restrictions ( 122 ). Moment-by-moment oscillations in behavior that are shown on these online platforms leave digital footprints, which can be aggregated to uncover emerging population trends across a wide range of health topics ( 55 , 57 , 58 , 66 , 70 , 71 , 82 , 83 , 108 , 111 , 112 , 117 – 119 , 123 , 132 , 135 , 147 ). Social computational big data is frequently validated against gold standard metrics, such as governmental sources, to corroborate its use for predicting real-world activities ( 11 , 56 , 57 , 63 , 64 , 87 , 124 , 125 , 141 ). However, ethical challenges exist. This section examines the sixth V, virtuosity, in the context of using social computational big data, with a focus on ( a ) data access and availability, ( b ) inclusion and representativeness, ( c ) advantages, ( d ) methods of analyses, and ( e ) future goals for the field as related to equity.
As of 2021, there are an average of 500 million tweets per day on Twitter, as well as 1.88 billion daily active users on Facebook, 500 million users on Instagram, and 3.5 billion searches on Google ( 17 , 38 , 97 ). The massive data volume and velocity provide an archive of human behavior. Data must be captured through the APIs that technology giants provide. However, APIs have access limitations, based on industry controls ( 136 ), which reduce the accessibility for scientists aiming to capture the full volume and velocity of big data on these platforms. Originally, Facebook provided a public API for use in collecting data for research related purposes ( 52 , 85 ), but it is no longer publicly accessible. It is estimated that Twitter's free-streaming API enables the data collection of 1% of all tweets ( 99 – 100 ), significantly less than the full data set of tweets from the Twitter Firehose, a prohibitively expensive service ( 73 ). Consequently, social computational big data is highly volatile, as it is based on the ever-changing nature of the industry and public opinion, such as personal privacy and security demands ( 2 , 69 ). Academic scientists are not the gatekeepers of these data but must work with the industry's positionality as to which and how many data can be retrieved.
Data absenteeism is an important concern in social computational big data research ( 60 , 81 ). Online data sources from industry tech giants such as Facebook, Google, and Twitter do not bridge the digital divide but may in fact widen it; therefore, these sources cannot be considered a replacement for methods that capture the needs of digitally excluded populations, such as individuals who cannot afford access, residents of rural areas with limited connectivity, and resource-constrained nations with political and economic barriers to access ( 134 ). Social computational big data does not undergo population-based sampling and, thus, is not representative of the general population ( 99 ). Social media platforms are skewed toward younger users (age 18–29 years) and users in urban areas ( 32 , 95 ), reducing the generalizability of findings to estimates of population trends ( 32 , 46 , 93 ). Finally, the ability to identify the demographics of online users is a challenge, and even though new computational methods can detect user demographics on social media ( 18 – 19 ), data anonymity and privacy standards put forth by the industry often limit the collection of demographic data from users’ profiles.
Compared with traditional data capturing methods, social computational big data can provide a richer picture of experiences, which has important equity implications for public health research. The origins of these platforms were not intended for research, but recall and social desirability bias may be less present. Internet search queries and discussions on social platforms are organic, making them more authentic, unfiltered, and genuine compared with data collected explicitly for research-related purposes ( 12 , 33 , 42 , 88 ). Lastly, without the barriers of social desirability bias, these data may present more truthful and accurate views about beliefs and behaviors ( 15 , 22 , 54 , 61 , 65 , 89 , 90 , 127 , 128 , 137 , 146 ). For instance, people tend to lie when it comes to racially charged topics ( 131 ). An empirical example showed that racist searches on Google were a robust negative predictor of US President Barack Obama's voting share, while national survey estimates about being racist were not ( 131 ).
A validation method for social computational big data compares results with governmental sources (the gold standard), as in the example of Yelp reviews being validated for capturing foodborne illness through correlating Yelp results with data from CDC (Centers for Disease Control and Prevention) reports of foodborne illness outbreaks ( 107 ). This research led to the development of a supplemental foodborne illness surveillance system combining Twitter and Yelp data for public health departments’ foodborne illness tracking ( 51 ). Such an approach is more sensitive and responsive to real-time signals compared with federal surveys of health conditions and behaviors, which can be limited in data lag and representativeness ( 8 ).
Another method of validation is to establish a relationship between online beliefs/behaviors and offline consequences. An example of this online-to-offline relationship is how online racism can signal the perpetuation of real-world hate crimes ( 59 , 103 ). A strong time-series lag time correlation has been found between anti-Islam-related hashtags on Twitter and anti-Muslim hate crimes ( 101 ), and evidence of a 1-week-earlier lag time between negative sentiment toward Mexicans and Hispanics on Twitter and worse mental health outcomes in this population ( 59 ) suggests a connection between the online and offline worlds. Therefore, data from online sources can be used to reveal subversive feelings and beliefs that traditional epidemiologic methods may not fully appreciate.
Finally, the racialization of disease and the long history of abuse in medical research ( 40 ) have prevented scientists from capturing data on stigmatized and marginalized populations ( 5 , 12 , 43 , 91 ). For instance, a review of government-funded cancer studies found that all racial and ethnic minorities are considerably underrepresented in cancer clinical trials, with fewer than 2% of studies focusing on minority health needs ( 21 , 80 ). Even in studies of environment-related diseases that disproportionally affect minority communities, Black, Brown, and Indigenous populations are less likely than their White counterparts to be represented ( 16 , 80 ). Data from online social media platforms may help reduce this gap in recruitment, as a greater proportion of racial and ethnic minorities use social networks ( 32 , 46 ).
Technically, the machine learning community approaches equity through the concept of fairness. There have recently been substantial efforts to formalize this concept in machine learning algorithms. Three formal definitions have gained recognition: ( a ) anticlassification, ( b ) classification parity, and ( c ) calibration ( 23 ). In the anticlassification approach, protected attributes like race, gender, or age, and their proxies, are not explicitly included in analyses. The idea is that these protected attributes should not be considered in the final model, which by definition would make predictions unequally across all protected groups. Therefore, by excluding them, the model is forced to predict the expected value of the outcome by using variables that are not considered to lead to unfair models. Here, no guarantees about the accuracy of the model for different groups are made. With classification parity, common definitions of model accuracy or performance (e.g., false-negative rate) are forced to be equal across different groups defined by the protected variables. As such, if the model is used to redistribute limited resources (i.e., access to health care), then protected groups are equally considered. Although classification parity guarantees that the model performs the same for all groups, it usually comes at the expense of a minimum common denominator, such that at least one group performs worse than it otherwise would. Calibration requires that, conditional on the risk estimate provided by the model, outcomes be independent of the protected attributes. Although this last definition is always desirable, in practice it falls short of protecting all groups.
The definitions of fairness seem intuitive, to an extent. For instance, it is natural to think that if one is expected to build a fair model, protected variables should not be used to decrease or increase the probability of certain predictions (anticlassification). Equally, if the final model does not have the same accuracy across all groups defined by the protected variables, then one might think that the model is not fair toward that group (classification parity). Finally, it is logical to assume that if a model, on average, represents the true underlying risk of the specific group, then this model is fair (calibration). More importantly, given that a formal definition of fairness is agreed upon, algorithms can be designed to satisfy it ( 3 ). For instance, a recent method constructs decision rules that ensure true-positive rates and false-positive rates are equal among prespecified groups ( 49 ). Equally, techniques that use pre- and postprocessing of the input variables and regularization to lead to parsimonious models have been devised to guarantee parity across different demographic variables ( 44 , 149 ). Unfortunately, no notion of fairness comes for free without affecting model performance for some or all groups ( 96 ).
Consider the concept of anticlassification. Sometimes, the variables left out have highly predictive values and not including them results in a less accurate model for one of the groups. For example, oxygen saturation level measurements have been shown to be less accurate for Black patients than White patients ( 129 ). As such, an algorithm that leaves out protected variables would not correct for the inherent bias of the technique. Similarly, although male patients with breast cancer have higher mortality than female patients (while the latter have substantially higher incidence) ( 142 ), any model trying to predict mortality of breast cancer patients or incidence must consider sex to accurately predict risk levels. Additionally, identifying all proxy variables is extremely difficult, and if they are left in the model, the influence of the protected variables on the outcome can be learned even when those variables are not present. For instance, the difference in intensity between the pixels in computer tomography images is enough for an algorithm to learn that the images come from different hospital systems ( 148 ). If hospital systems see patients with different underlying risks, then the model would use this information without accurately learning the true underlying features that would lead to a correct interpretation of the image.
Expanding on the male breast cancer example, due to the stark differences, any data set built to predict mortality of breast cancer patients will comprise many more female than male patients, and requiring a false-negative rate to be the same across all protected groups (classification parity) will be detrimental for female patients ( 142 ). Also, requiring proper calibration (outcomes need to be independent of the protected attributes conditional on the risk estimate) can be extremely misleading if the outcome is not well explained by the collected variables or a proxy outcome is analyzed. In the Obermeyer et al. ( 109 ) study described above, total health care expenditures were used as a proxy for unmet health needs in establishing risk scores. Requiring calibration of a model built with these data would have been problematic for Black patients because it would reinforce the root problem: the fact that the outcomes collected (expenditures) did not represent the true need for medical attention.
Although debiasing machine learning models have gained a lot of attention in recent years, a means of applying all scenarios has not yet been found, underscoring the difficult nature of this problem. Therefore, depending on the specific problem at hand, scientists should investigate the three currently accepted definitions of fairness and evaluate which is most relevant.
Claims data from the US Centers for Medicare and Medicaid or from private health insurers are rich sources of big data that are used to collect habitually unchanging patient health information and other demographics. Claims data can be considered high in volume, given how many data are collected during health care visits. There are a variety of such data—health conditions, diagnostic codes, billing, and payment—and they are produced at a high velocity, updating frequently given how many health services are provided daily. But what about the veracity or the trustworthiness of the data? Incorporating the sixth V—virtuosity—supports an equity lens through which to interrogate the data to properly understand the patterning of health outcomes uncovered in claims data. In this section, we consider large administrative data sets that reflect routinely collected patient information.
To analyze claims data, scientists use artificial intelligence (AI) to develop machine learning algorithms that learn to detect patterns in the data. These data are being used to examine relationships between health outcomes, for example, mortality among Veterans Health Administration patients and prediction of type 2 diabetes ( 121 , 143 ). However, elucidating health patterns using claims data is challenging, given that the initial purpose for collecting the data is for billing rather than monitoring of patient and population health.
The use of algorithms to detect patterns in claims data presents numerous challenges. First, many patterns detected are correlational and are limited in their ability to explain why certain patterns are uncovered ( 133 , 145 ). Moreover, the validity of methods and algorithm transparency need to be addressed. One of the main biases in algorithms is omitted variable bias, which arises from having limited information about other potentially relevant factors that influence the outcome ( 145 ). The lack of information applies in particular to data points that measure the structural and social determinants of health. For example, in a study on prediction of type 2 diabetes, having limited information about the patient's diet or access to healthy and nutritious food creates omitted variable bias. Another challenge is data absenteeism. As a result of US states not expanding Medicaid, entire swaths of populations were missing within claims records. Given the various challenges of using AI to analyze big (claims) data, a reorientation toward ensuring that veracity and virtuosity are evaluated is paramount. Attention to biases including measurement error, sampling bias, and ascertainment bias can mitigate equity risks from claims data ( 75 ). Equally important, such methods may best be viewed as hypothesis generating, rather than hypothesis testing.
To help explain why patterns are seen in claims data, analysis approaches would benefit from explicitly leveraging the inherent data structures. Clustering of populations within different environments (e.g., counties) is an intrinsic data structure. One statistical technique that supports the use of a health equity lens in big data analysis is the multilevel (hierarchical) model. Multilevel models (MLMs) concern themselves with data that are nested within clusters of higher orders of influence (e.g., years, months, classrooms, countries) ( 53 ). In typical MLM analyses, individual (lower-level) units of analysis are grouped within contextual (higher-level) units. The use of MLMs and other similarly situated analyses has been growing, particularly in research aiming to understand how policy and other structural determinants may shape the outcomes of interest ( 50 , 79 , 92 , 112 – 113 ). For example, a 2011 study by Klawitter ( 79 ) found that gay men who lived in states with antidiscrimination policy protections in employment earned 8% more per year than gay men in states with no antidiscrimination legislation—illustrating the importance of leveraging the clustered structure of the data to explain the patterns.
In health insurance claims, data points (patient data and outcomes) are clustered within geographic regions (e.g., census tracts, counties, states) with diverse policy and structural environments. These diverse geographies have varied policies on, for instance, minimum wage, labor rights, antidiscrimination protections, and poverty alleviation measures. How might the policy and environmental contexts in which the patients live shape the patterning of the health outcomes that AI analyses might find in the claims data? Scholars are beginning to grapple with these questions as applied to big data. A study by Davis et al. ( 29 ) found that 42% of Oregon Medicaid and commercial insurance patients who were eligible for colorectal screenings had a completed screening over a 4-year period. Interestingly, with the use of MLMs, the authors found that the percentage varied by county, such that counties with higher rates of socioeconomic deprivation (e.g., lower high school graduation rates, higher unemployment rates) and lower rates of endoscopy specialists had the lowest colorectal screening rates ( 29 ). Studies that intentionally leverage data structure can enhance equity approaches to claims data analysis and can help ensure that proper algorithm learning is taking place to support an accurate interpretation of the results.

Section: DATA AUGMENTATION TO IMPROVE BIG DATA EQUITY

Much of the appeal of big data comes from its high volume attribute, which carries the aspiration of statistical flexibility to support complex analyses and protect against type I and II errors. High volume, referring to the number of participants, may lead to an aspiration of generalizability, in which identified statistical associations are broadly applicable to other populations. However, a high volume of participants cannot replace the representativeness of participants, and no quantity of data can confer generalizability if representativeness is lacking ( 72 ).
Data augmentation methods such as multiple systems estimation (MSE), also known as capture–recapture, can be used to assess and correct population undercounting, thereby improving representativeness and moving closer to equity (virtuosity). With origins in wildlife biology, MSE is applied in public health to estimate the size of “hidden” and hard-to-reach populations as well as to evaluate the completeness of surveillance systems and identify patterns of differential underreporting ( 48 , 144 ). MSE estimates the total size of a population on the basis of the degree of overlap between two or more incomplete lists (or samples) of that population (i.e., how many unique individuals are observed on multiple lists). The greater the overlap is, the smaller the unobserved population (i.e., the number of people not already observed on any of the lists) will be; conversely, the smaller the overlap is, the greater the unobserved population will be. Several assumptions are necessary for valid estimation, yet the so-called list independence assumption often receives the most attention in public health applications ( 68 ). This assumption states that the lists used for MSE must be statistically independent from one another; being on one list does not increase or decrease one's probability of being on another list. Various design and statistical approaches are available to satisfy or relax this assumption, most notably regression modeling to account for list dependency ( 144 ).
MSE has been applied in countless settings, using various types of lists, to provide context and scope for populations of public health interest ( 10 , 48 , 67 , 104 , 110 , 144 ). For example, Hu et al. ( 67 ) implemented a two-sample MSE of men who have sex with men (MSM) in Mainland China using high-volume data from social networking sites. Simulating mobile phone positions covering the country, the authors recorded profile identifiers from Blued, a popular MSM social networking app, over two 2-week periods 7 months apart, as two independent samples for the MSE study. Each sample included nearly 2.5 million profiles, with an overlap of 1.3 million profiles. They estimated 8,288,536 MSM in Mainland China. Subnational estimates by cities and provinces served as denominators to calculate the burden of HIV and sexually transmitted infections among MSM in different areas.
Incorporating at least three lists provides additional statistical flexibility for MSE. In an example of incorporating high-variety data, Min et al. ( 94 ) linked six data sets covering the spectrum of medical touch points to estimate the prevalence of and trends in opioid use disorder (OUD) in British Columbia, Canada, from 1996 to 2017. Stratified analyses determined that the greatest increase in OUD prevalence was among males aged 12–30 years and 31–44 years, from 2013 to 2017. Stratified analyses, as illustrated in this example, are especially important for the advancement of equity and demonstrate whether subgroups are systematically underrepresented in data (and differentially deprived of resources) or, in the case of human rights abuses, systematically targeted in instances of mass killings ( 45 , 86 ). As noted by Barocas ( 7 , p. 2405), commenting on the need to include stratified estimates of race/ethnicity in applications of MSE, “inaccurate counting is another form of structural racism and leads to widening disparities.”
Lists and data sources are almost always incomplete, and often systematically incomplete. Failing to acknowledge this fact risks perpetuating inequities by continuing to underrepresent vulnerable populations in scientific studies. Populations that are of key interest due to their unique vulnerabilities to health outcomes are often not represented in high-volume data sets. These populations may be missed because they make up only a small proportion of the overall population, or because of the stigma and discrimination that often follows from disclosing their identity.
Beyond MSE applications, record linkage of overlapping and complementary data sets can be used for other data augmentation purposes. Just as administrative records may be linked for MSE, administrative records, as well as federal population-based surveys ( 8 ), are increasingly being linked to generate complex data sets that map individuals’ contacts to institutional touch points. Some of these data sources may come from institutions that explicitly serve specific marginalized populations (e.g., housing services, social welfare programs, health clinics providing services to groups such as female sex workers or transgender women), ensuring that these populations are included (by design) in the creation of the data. Other data sources may come from hospital records, arrest records, and social services (e.g., housing, child welfare, substance use treatment). Each data source collects overlapping information with a different focus and, in aggregate, provides more context through which to view the lived experience of each individual, albeit with the limitations of data positionality described above. In order to not perpetuate inequities and discriminatory practices, scientists must exercise caution in selecting data sets for data augmentation and be vigilant against any biases that may be in place in the construction of those data sets ( 37 ).
Record linkage, which was not an aim when these siloed data systems were created, poses additional challenges. Investigators must think creatively about how to accurately match the same individual across multiple systems on the basis of limited personal identifiers while protecting everyone's privacy. A potential solution may be found in hashing ( 31 ) and biometric scanning (e.g., fingerprint scanners, iris scanners) ( 1 , 126 , 140 ), which are emerging, low-cost methods in which algorithms generate complex alphanumeric identifiers based on identifiable information. Each identifier is unique to a client's record but cannot be reverse-coded to identify the client. Such an algorithm can be applied internally within an institution to preserve client privacy while facilitating the linkage of records across institutional touch points, clinics, and other data sources. However, with regard to biometric scanning, fingerprint scanners may be associated with the criminal justice system. Even though the code generated from a scan cannot be linked back to criminal or immigration databases, this approach to record linkage may not be considered acceptable to certain groups or may be met with skepticism ( 1 , 76 ).

Section: DISCUSSION

Our review examines important equity and justice considerations in the conceptualization and application of big data methods. For users of big data, it is paramount to challenge our assumptions and develop new or improved frameworks to ensure equitable big data research practices. Williams et al. ( 145 , p. 100) state:
In particular, creators of algorithmic systems have three general classes of approaches to prevent discrimination: they can make the data less biased beforehand, build fairness criteria into the algorithm… or alter the application of the rules after the algorithm runs.

However, it would be better to prevent discrimination in all three classes. A strategic way to enshrine virtuosity within big data practice is to foreground the experiences of particular groups that experience marginalization. Incorporating frameworks such as intersectionality theory (which recognizes that when social identity categories intersect, reflecting interlocking systems of privilege and oppression, they may result in unique and intensified forms of discrimination) can improve the way we approach uses of big data in public health ( 25 ). At a minimum, this requires practitioners of big data to recognize and challenge their positionality to the data themselves and their interpretation. For example, the use of big data sources in a health equity framework requires that practitioners address issues such as racism, sexism, homophobia, transphobia, and classism. The explicit application of theoretical and analytic frameworks that contend with the structural and contextual factors that shape our lives is paramount in research using big data.
To improve equity in big data research, we offer several suggestions for researchers to critically incorporate the sixth V, virtuosity. First, include social epidemiologists in research and prioritize social epidemiology training beyond programs in epidemiology and other public health disciplines. Scientists who study social epidemiology have deeper knowledge of the structural and systemic forces that have generated the distribution of advantages and disadvantages in society ( 74 ). Second, increase the level of diversity in researchers across disciplines pursuing big data and equity. Discriminatory biases can be prevented through the addition of a wide range of perspectives, which can reduce the likelihood of generating biases based on singular viewpoints ( 47 ). Third, generate partnerships between industry and academia ( 124 ). Big tech should work with social epidemiologists to generate more ethical and virtuous research. Fourth, federal and state policies are needed to safeguard against biased and discriminatory production of big data. Fifth, it is important for us as scientists to evaluate our own biases and understand that we do not have the breadth of experience to know what is fully needed to improve equity.
Coupling these recommendations with a focus on training, schools of public health, departments of epidemiology and biostatistics, and other data science training programs must have diverse representation among students, staff, and faculty. These programs should emphasize core competencies in sampling theory and design, participatory engagement, and working with data and collaborators spanning diverse disciplines (e.g., social welfare, criminal justice), as well as an understanding of the role of historical and structural racism and inequality. When teaching focuses on biases, especially as they relate to study design and analysis, instructors should include a historical lens highlighting structural inequalities and interrogating its influence on data, data absenteeism, and the positionality of data. We also encourage the discussion of methodological approaches to reduce the impact of such biases. As an example, the Department of Epidemiology and Biostatistics at the University of California, San Francisco, in partnership with the Bakar Computational Health Sciences Institute and the Center for Health and Community, has developed a predoctoral training program that integrates the theoretical frameworks and methodological tools of behavioral and social scientists with the rapidly evolving technical repertoire of computational health scientists. Graduates of the DaTABASE (Data Science Training to Advance Behavioral and Social Science Expertise for Health Research) program (National Institutes of Health/National Institute on Minority Health and Health Disparities grant T32MD015070) are trained to apply analytic tools to novel data sets for research on behavioral and social processes underlying health disparities. Finally, public health practitioners must honor data augmentation and community involvement in it and, as such, must listen to and acknowledge the lived experience of often absent communities and ensure that they are at the forefront of the design and development of equitable research ( 15 , 22 , 61 , 89 , 127 , 139 , 146 ).

Section: disclosure statement

P.W. consulted for McKinsey & Company on modeling COVID-19 transmission from March 2020 to September 2020.

Section: acknowledgments

Research reported in this review was supported by National Institutes of Health (NIH)/National Institute of Allergy and Infection Diseases Career Development Award 5K01MH119910-02 (to P.W.); the National Institute of Biomedical Imaging and Bioengineering of the NIH under award K08EB026500 (to G.V.); NIH grants R01DK115492, 7N91020C00039, and R56AR063705 (to Y.H.); and PRISE (Partnerships for Research in Implementation Science for Equity) at the University of California, San Francisco (to M.H.).

Section: literature cited

